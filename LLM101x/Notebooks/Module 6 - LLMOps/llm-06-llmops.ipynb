{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53482,"databundleVersionId":6201832,"sourceType":"competition"},{"sourceId":54662,"databundleVersionId":6169864,"sourceType":"competition"}],"dockerImageVersionId":30513,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ⚡ Further Notebooks In This Course ⚡","metadata":{}},{"cell_type":"markdown","source":"**Notebooks:**\n1. [LLM 01 - How to use LLMs with Hugging Face](https://www.kaggle.com/code/aliabdin1/llm-01-llms-with-hugging-face)\n2. [LLM 02 - Embeddings, Vector Databases, and Search](https://www.kaggle.com/code/aliabdin1/llm-02-embeddings-vector-databases-and-search)\n3. [LLM 03 - Building LLM Chain](https://www.kaggle.com/code/aliabdin1/llm-03-building-llm-chain)\n4. [LLM 04a - Fine-tuning LLMs](https://www.kaggle.com/code/aliabdin1/llm-04a-fine-tuning-llms)\n4. [LLM 04b - Evaluating LLMs](https://www.kaggle.com/code/aliabdin1/llm-04b-evaluating-llms)\n5. [LLM 05 - Biased LLMs and Society](https://www.kaggle.com/code/aliabdin1/llm-05-llms-and-society)\n6. [LLM 06 - LLMOps](https://www.kaggle.com/code/aliabdin1/llm-06-llmops)\n\n**Hands-on Lab Notebooks:**\n1. [LLM 01L - How to use LLMs with Hugging Face Lab](https://www.kaggle.com/code/aliabdin1/llm-01l-llms-with-hugging-face-lab)\n2. [LLM 02L - Embeddings, Vector Databases, and Search Lab](https://www.kaggle.com/code/aliabdin1/llm-02l-embeddings-vector-databases-and-search)\n3. [LLM 03L - Building LLM Chains Lab](https://www.kaggle.com/code/aliabdin1/llm-03l-building-llm-chains-lab)\n4. [LLM 04L - Fine-tuning LLMs Lab](https://www.kaggle.com/code/aliabdin1/llm-04l-fine-tuning-llms-lab)\n5. [LLM 05L - Biased LLMs and Society Lab](https://www.kaggle.com/code/aliabdin1/llm-05l-llms-and-society-lab)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9594d237-d684-4c90-a1b5-7a6fbc45d5f2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":"# LLMOps\nIn this example, we will walk through some key steps for taking an LLM-based pipeline to production.  Our pipeline will be familiar to you from previous modules: summarization of news articles using a pre-trained model from Hugging Face.  But in this walkthrough, we will be more rigorous about LLMOps.\n\n**Develop an LLM pipeline**\n\nOur LLMOps goals during development are (a) to track what we do carefully for later auditing and reproducibility and (b) to package models or pipelines in a format which will make future deployment easier.  Step-by-step, we will:\n* Load data.\n* Build an LLM pipeline.\n* Test applying the pipeline to data, and log queries and results to MLflow Tracking.\n* Log the pipeline to the MLflow Tracking server as an MLflow Model.\n\n**Test the LLM pipeline**\n\nOur LLMOps goals during testing (in the staging or QA stage) are (a) to track the LLM's progress through testing and towards production and (b) to do so programmatically to demonstrate the APIs needed for future CI/CD automation.  Step-by-step, we will:\n* Register the pipeline to the MLflow Model Registry.\n* Test the pipeline on sample data.\n* Promote the registered model (pipeline) to production.\n\n**Create a production workflow for batch inference**\n\nOur LLMOps goals during production are (a) to write scale-out code which can meet scaling demands in the future and (b) to simplify deployment by using MLflow to write model-agnostic deployment code.  Step-by-step, we will:\n* Load the latest production LLM pipeline from the Model Registry.\n* Apply the pipeline to an Apache Spark DataFrame.\n* Append the results to a Delta Lake table.\n\n### Notes about this workflow\n\n**This notebook vs. modular scripts**: Since this demo is in a single notebook, we will divide the workflow from development to production via notebook sections.  In a more realistic LLM Ops setup, you would likely have the sections split into separate notebooks or scripts.\n\n**Promoting models vs. code**: We track the path from development to production via the MLflow Model Registry.  That is, we are *promoting models* towards production, rather than promoting code.  For more discussion of these two paradigms, see [\"The Big Book of MLOps\"](https://www.databricks.com/resources/ebook/the-big-book-of-mlops).\n\n### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n1. Walk through a simple but realistic workflow to take an LLM pipeline from development to production.\n1. Make use of MLflow Tracking and the Model Registry to package and manage the pipeline.\n1. Scale out batch inference using Apache Spark and Delta Lake.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a7aedb9-c606-4651-b104-197583a4ae88","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":"## Classroom Setup","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"110f2c83-d8e1-46c1-84b3-4681826892fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"#%run ../Includes/Classroom-Setup","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"946d51ea-2f35-4498-8f42-da4d205f6666","inputWidgets":{},"title":""}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mkdir cache","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:23:20.935489Z","iopub.execute_input":"2023-06-27T12:23:20.936035Z","iopub.status.idle":"2023-06-27T12:23:22.040817Z","shell.execute_reply.started":"2023-06-27T12:23:20.935995Z","shell.execute_reply":"2023-06-27T12:23:22.039217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate --quiet","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:23:22.042813Z","iopub.execute_input":"2023-06-27T12:23:22.043189Z","iopub.status.idle":"2023-06-27T12:23:35.679251Z","shell.execute_reply.started":"2023-06-27T12:23:22.043153Z","shell.execute_reply":"2023-06-27T12:23:35.678001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark --quiet","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:23:35.68165Z","iopub.execute_input":"2023-06-27T12:23:35.68214Z","iopub.status.idle":"2023-06-27T12:23:49.248974Z","shell.execute_reply.started":"2023-06-27T12:23:35.68209Z","shell.execute_reply":"2023-06-27T12:23:49.247456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install delta-spark --quiet","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:23:49.253829Z","iopub.execute_input":"2023-06-27T12:23:49.254336Z","iopub.status.idle":"2023-06-27T12:24:02.629207Z","shell.execute_reply.started":"2023-06-27T12:23:49.254291Z","shell.execute_reply":"2023-06-27T12:24:02.627646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this notebook we'll use the <a href=\"https://huggingface.co/datasets/xsum\" target=\"_blank\">Extreme Summarization (XSum) Dataset</a>  with the <a href=\"https://huggingface.co/t5-small\" target=\"_blank\">T5 Text-To-Text Transfer Transformer</a> from Hugging Face.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4a5fcc0-4ee5-44de-b32d-2763d3ea7032","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"!pip install mlflow --quiet","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:28:28.220484Z","iopub.execute_input":"2023-06-27T12:28:28.220981Z","iopub.status.idle":"2023-06-27T12:28:46.809983Z","shell.execute_reply.started":"2023-06-27T12:28:28.220934Z","shell.execute_reply":"2023-06-27T12:28:46.80864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pyngrok --quiet","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:29:07.695879Z","iopub.execute_input":"2023-06-27T12:29:07.69658Z","iopub.status.idle":"2023-06-27T12:29:23.964877Z","shell.execute_reply.started":"2023-06-27T12:29:07.696511Z","shell.execute_reply":"2023-06-27T12:29:23.963665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare data","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"becd5f54-a454-43fd-8432-c42dffa0ac42","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import pipeline","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36692cdf-14db-43d7-97ba-67f74fb4d43f","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:24:02.631142Z","iopub.execute_input":"2023-06-27T12:24:02.631577Z","iopub.status.idle":"2023-06-27T12:24:10.580706Z","shell.execute_reply.started":"2023-06-27T12:24:02.631534Z","shell.execute_reply":"2023-06-27T12:24:10.579552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xsum_dataset = load_dataset(\n    \"xsum\", version=\"1.2.0\", cache_dir=\"../working/cache\"\n)  # Note: We specify cache_dir to use pre-cached data.\nxsum_sample = xsum_dataset[\"train\"].select(range(10))\ndisplay(xsum_sample.to_pandas())","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d08a6927-127a-4b93-9e00-6eb5a732d66f","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:24:10.582313Z","iopub.execute_input":"2023-06-27T12:24:10.582722Z","iopub.status.idle":"2023-06-27T12:24:11.397279Z","shell.execute_reply.started":"2023-06-27T12:24:10.582688Z","shell.execute_reply":"2023-06-27T12:24:11.395776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Later on, when we show Production inference, we will want a dataset saved for it.  See the production section below for more information about Delta, the format we use to save the data here.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a68bc9e2-3ee2-4a0c-a82b-a8d0aa0f4ab4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"import pyspark\nfrom delta import *\n\nbuilder = pyspark.sql.SparkSession.builder.appName(\"LLM 06 - LLMOps\") \\\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n\nspark = configure_spark_with_delta_pip(builder).getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:24:11.398849Z","iopub.execute_input":"2023-06-27T12:24:11.399224Z","iopub.status.idle":"2023-06-27T12:24:19.339949Z","shell.execute_reply.started":"2023-06-27T12:24:11.399192Z","shell.execute_reply":"2023-06-27T12:24:19.338482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from pyspark.sql import SparkSession\n\n#Building Spark Session\n#spark = (SparkSession.builder\n#                  .appName('LLM 06 - LLMOps')\n#                  .config(\"spark.executor.memory\", \"1G\")\n#                  .config(\"spark.executor.cores\",\"4\")\n#                  .getOrCreate())","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:24:19.342913Z","iopub.execute_input":"2023-06-27T12:24:19.343874Z","iopub.status.idle":"2023-06-27T12:24:19.355204Z","shell.execute_reply.started":"2023-06-27T12:24:19.343793Z","shell.execute_reply":"2023-06-27T12:24:19.353797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prod_data_path = \"../working/cache/m6_prod_data\"\ntest_spark_dataset = spark.createDataFrame(xsum_dataset[\"test\"].to_pandas())\ntest_spark_dataset.write.format(\"delta\").mode(\"overwrite\").save(prod_data_path)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2366bdd2-b9bb-4673-8a85-b2b48d739fb6","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:24:19.357466Z","iopub.execute_input":"2023-06-27T12:24:19.35838Z","iopub.status.idle":"2023-06-27T12:24:43.239092Z","shell.execute_reply.started":"2023-06-27T12:24:19.358312Z","shell.execute_reply":"2023-06-27T12:24:43.237776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Develop an LLM pipeline","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0fce7691-6755-4771-a813-fd4179194382","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":"### Create a Hugging Face pipeline","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f94999ea-e0c4-4c29-b2b2-d7b367869ebd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"from transformers import pipeline\n\n# Later, we plan to log all of these parameters to MLflow.\n# Storing them as variables here will help with that.\nhf_model_name = \"t5-small\"\nmin_length = 20\nmax_length = 40\ntruncation = True\ndo_sample = True\n\nsummarizer = pipeline(\n    task=\"summarization\",\n    model=hf_model_name,\n    min_length=min_length,\n    max_length=max_length,\n    truncation=truncation,\n    do_sample=do_sample,\n    model_kwargs={\"cache_dir\": \"../working/cache\"},\n)  # Note: We specify cache_dir to use pre-cached models.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e29e203-301c-4be8-b759-74f2e9703a8e","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:25:55.303639Z","iopub.execute_input":"2023-06-27T12:25:55.304117Z","iopub.status.idle":"2023-06-27T12:25:59.899552Z","shell.execute_reply.started":"2023-06-27T12:25:55.304084Z","shell.execute_reply":"2023-06-27T12:25:59.898489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now examine the `summarizer` pipeline summarizing a document from the `xsum` dataset.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"627f4614-78e1-4403-8c26-ecc61e50e9ce","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"doc0 = xsum_sample[\"document\"][0]\nprint(f\"Summary: {summarizer(doc0)[0]['summary_text']}\")\nprint(\"===============================================\")\nprint(f\"Original Document: {doc0}\")","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa380885-e296-4dd5-a996-20810340b805","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:26:14.026215Z","iopub.execute_input":"2023-06-27T12:26:14.026779Z","iopub.status.idle":"2023-06-27T12:26:16.496969Z","shell.execute_reply.started":"2023-06-27T12:26:14.026737Z","shell.execute_reply":"2023-06-27T12:26:16.49568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Track LLM development with MLflow\n\n[MLflow](https://mlflow.org/) has a Tracking component that helps you to track exactly how models or pipelines are produced during development.  Although we are not fitting (tuning or training) a model here, we can still make use of tracking to:\n* Track example queries and responses to the LLM pipeline, for later review or analysis\n* Store the model as an [MLflow Model flavor](https://mlflow.org/docs/latest/models.html#built-in-model-flavors), thus packaging it for simpler deployment","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"680019b8-9377-4767-8c31-2c551c57f0f2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"# Apply to a batch of articles\nimport pandas as pd\n\nresults = summarizer(xsum_sample[\"document\"])\ndisplay(pd.DataFrame(results, columns=[\"summary_text\"]))","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"720b79b6-40cf-4c05-9f53-da926834bb7d","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:27:21.855281Z","iopub.execute_input":"2023-06-27T12:27:21.855796Z","iopub.status.idle":"2023-06-27T12:27:37.296157Z","shell.execute_reply.started":"2023-06-27T12:27:21.855761Z","shell.execute_reply":"2023-06-27T12:27:37.29515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[MLflow Tracking](https://mlflow.org/docs/latest/tracking.html) is organized hierarchically as follows:\n* **An [experiment](https://mlflow.org/docs/latest/tracking.html#organizing-runs-in-experiments)** generally corresponds to the creation of 1 primary model or pipeline.  In our case, this is our LLM pipeline.  It contains some number of *runs*.\n   * **A [run](https://mlflow.org/docs/latest/tracking.html#organizing-runs-in-experiments)** generally corresponds to the creation of 1 sub-model, such as 1 trial during hyperparameter tuning in traditional ML.  In our case, executing this notebook once will only create 1 run, but a second execution of the notebook will create a second run.  This version tracking can be useful during iterative development.  Each run contains some number of logged parameters, metrics, tags, models, artifacts, and other metadata.\n      * **A [parameter](https://mlflow.org/docs/latest/tracking.html#concepts)** is an input to the model or pipeline, such as a regularization parameter in traditional ML or `max_length` for our LLM pipeline.\n      * **A [metric](https://mlflow.org/docs/latest/tracking.html#concepts)** is an output of evaluation, such as accuracy or loss.\n      * **An [artifact](https://mlflow.org/docs/latest/tracking.html#concepts)** is an arbitrary file stored alongside a run's metadata, such as the serialized model itself.\n      * **A [flavor](https://mlflow.org/docs/latest/models.html#storage-format)** is an MLflow format for serializing models.  This format uses the underlying ML library's format (such as PyTorch, TensorFlow, Hugging Face, or your custom format), plus metadata.\n\nMLflow has an API for tracking queries and predictions [`mlflow.llm.log_predictions()`](https://mlflow.org/docs/latest/python_api/mlflow.llm.html), which we will use below.  Note that, as of MLflow 2.3.1 (Apr 28, 2023), this API is Experimental, so it may change in later releases.  See the [LLM Tracking page](https://mlflow.org/docs/latest/llm-tracking.html) for more information.\n\n***Tip***: We wrap our model development workflow with a call to `with mlflow.start_run():`.  This context manager syntax starts and ends the MLflow run explicitly, which is a best practice for code which may be moved to production.  See the [API doc](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.start_run) for more information.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f440ac11-f97a-44d2-a232-105805695f73","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"import mlflow\nimport mlflow.pytorch","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:30:27.283163Z","iopub.execute_input":"2023-06-27T12:30:27.283743Z","iopub.status.idle":"2023-06-27T12:30:28.060032Z","shell.execute_reply.started":"2023-06-27T12:30:27.283692Z","shell.execute_reply":"2023-06-27T12:30:28.058686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_ipython().system_raw(\"mlflow ui --port 5000 &\")\nmlflow.pytorch.autolog()","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:30:30.124999Z","iopub.execute_input":"2023-06-27T12:30:30.125445Z","iopub.status.idle":"2023-06-27T12:30:32.881692Z","shell.execute_reply.started":"2023-06-27T12:30:30.125409Z","shell.execute_reply":"2023-06-27T12:30:32.880388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMP: please create a auth token from https://dashboard.ngrok.com/auth by creating an account. \n# the below auth ticket will not work for anyone re-running the notebook.\n    \nfrom pyngrok import ngrok\nfrom getpass import getpass\n\n# Terminate open tunnels if exist\nngrok.kill()\n\n# Setting the authtoken (optional)\n# Get your authtoken from https://dashboard.ngrok.com/auth\n#NGROK_AUTH_TOKEN = \"2Padn9VzXvPy7nJXe6eAUTR3Dbd_6cXCwQeLNLwZDCWL5ypKs\"\n#ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n\n# Open an HTTPs tunnel on port 5000 for http://localhost:5000\nngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\nprint(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)","metadata":{"execution":{"iopub.status.busy":"2023-06-27T12:31:42.42861Z","iopub.execute_input":"2023-06-27T12:31:42.429088Z","iopub.status.idle":"2023-06-27T12:31:42.970267Z","shell.execute_reply.started":"2023-06-27T12:31:42.429053Z","shell.execute_reply":"2023-06-27T12:31:42.968695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tell MLflow Tracking to user this explicit experiment path,\n# which is in your home directory under the Workspace browser (left-hand sidebar).\nmlflow.set_experiment(\"/Users/aliabdin/LLM 06 - MLflow experiment\")\n\nwith mlflow.start_run():\n    # LOG PARAMS\n    mlflow.log_params(\n        {\n            \"hf_model_name\": hf_model_name,\n            \"min_length\": min_length,\n            \"max_length\": max_length,\n            \"truncation\": truncation,\n            \"do_sample\": do_sample,\n        }\n    )\n\n    # --------------------------------\n    # LOG INPUTS (QUERIES) AND OUTPUTS\n    # Logged `inputs` are expected to be a list of str, or a list of str->str dicts.\n    results_list = [r[\"summary_text\"] for r in results]\n\n    # Our LLM pipeline does not have prompts separate from inputs, so we do not log any prompts.\n    mlflow.llm.log_predictions(\n        inputs=xsum_sample[\"document\"],\n        outputs=results_list,\n        prompts=[\"\" for _ in results_list],\n    )\n\n    # ---------\n    # LOG MODEL\n    # We next log our LLM pipeline as an MLflow model.\n    # This packages the model with useful metadata, such as the library versions used to create it.\n    # This metadata makes it much easier to deploy the model downstream.\n    # Under the hood, the model format is simply the ML library's native format (Hugging Face for us), plus metadata.\n\n    # It is valuable to log a \"signature\" with the model telling MLflow the input and output schema for the model.\n    signature = mlflow.models.infer_signature(\n        xsum_sample[\"document\"][0],\n        mlflow.transformers.generate_signature_output(\n            summarizer, xsum_sample[\"document\"][0]\n        ),\n    )\n    print(f\"Signature:\\n{signature}\\n\")\n\n    # For mlflow.transformers, if there are inference-time configurations,\n    # those need to be saved specially in the log_model call (below).\n    # This ensures that the pipeline will use these same configurations when re-loaded.\n    inference_config = {\n        \"min_length\": min_length,\n        \"max_length\": max_length,\n        \"truncation\": truncation,\n        \"do_sample\": do_sample,\n    }\n\n    # Logging a model returns a handle `model_info` to the model metadata in the tracking server.\n    # This `model_info` will be useful later in the notebook to retrieve the logged model.\n    model_info = mlflow.transformers.log_model(\n        transformers_model=summarizer,\n        artifact_path=\"summarizer\",\n        task=\"summarization\",\n        inference_config=inference_config,\n        signature=signature,\n        input_example=\"This is an example of a long news article which this pipeline can summarize for you.\",\n    )","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db200b49-9690-4309-9bfb-bbcedb33c16d","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:33:04.377511Z","iopub.execute_input":"2023-06-27T12:33:04.37806Z","iopub.status.idle":"2023-06-27T12:33:21.467219Z","shell.execute_reply.started":"2023-06-27T12:33:04.378022Z","shell.execute_reply":"2023-06-27T12:33:21.465848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Query the MLflow Tracking server\n\n**MLflow Tracking API**: We briefly show how to query the logged model and metadata in the MLflow Tracking server, by loading the logged model.  See the [MLflow API](https://mlflow.org/docs/latest/python_api/mlflow.html) for more information about programmatic access.\n\n**MLflow Tracking UI**: You can also use the UI.  In the right-hand sidebar, click the beaker icon to access the MLflow experiments run list, and then click through to access the Tracking server UI.  There, you can see the logged metadata and model.  Note in particular that our LLM inputs and outputs have been logged as a CSV file under model artifacts.\n\nGIF of MLflow UI:\n![GIF of MLflow UI](https://files.training.databricks.com/images/llm/llmops.gif)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"91035fdb-cbce-4c15-b3e0-b9f42c038eb9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":"Now, we can load the pipeline back from MLflow as a [pyfunc](https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html) and use the `.predict()` method to summarize an example document.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78d8ee76-90fe-44ca-98ce-21ef98f73d6f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"loaded_summarizer = mlflow.pyfunc.load_model(model_uri=model_info.model_uri)\nloaded_summarizer.predict(xsum_sample[\"document\"][0])","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c2c1669-3909-41ac-bd88-c34b033a7d86","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:33:33.714915Z","iopub.execute_input":"2023-06-27T12:33:33.715406Z","iopub.status.idle":"2023-06-27T12:33:36.981666Z","shell.execute_reply.started":"2023-06-27T12:33:33.715343Z","shell.execute_reply":"2023-06-27T12:33:36.980446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The `.predict()` method can handle more than one document at a time, below we pass in all the data from `xsum_sample`.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1ba5413-70c8-4af4-9435-98b335dec3db","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"results = loaded_summarizer.predict(xsum_sample.to_pandas()[\"document\"])\ndisplay(pd.DataFrame(results, columns=[\"generated_summary\"]))","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"65cedf11-d7d6-4016-8349-f4ec7226a967","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:34:01.228038Z","iopub.execute_input":"2023-06-27T12:34:01.228515Z","iopub.status.idle":"2023-06-27T12:34:19.029905Z","shell.execute_reply.started":"2023-06-27T12:34:01.228479Z","shell.execute_reply":"2023-06-27T12:34:19.028739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We are now ready to move to the staging step of deployment.  To get started, we will register the model in the MLflow Model Registry (more info below).","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1138a60-5e7b-447d-81b7-7007d9081f5e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"# Define the name for the model in the Model Registry.\n# We filter out some special characters which cannot be used in model names.\nmodel_name = \"summarizer - aliabdin\"\nmodel_name = model_name.replace(\"/\", \"_\").replace(\".\", \"_\").replace(\":\", \"_\")\nprint(model_name)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82475e55-15a1-4e74-9063-519fda0b1c5a","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:34:42.201618Z","iopub.execute_input":"2023-06-27T12:34:42.202089Z","iopub.status.idle":"2023-06-27T12:34:42.210422Z","shell.execute_reply.started":"2023-06-27T12:34:42.202054Z","shell.execute_reply":"2023-06-27T12:34:42.208952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Register a new model under the given name, or a new model version if the name exists already.\nmlflow.register_model(model_uri=model_info.model_uri, name=model_name)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4db59f9-1ca6-41da-8370-dbbb3dcb4c97","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:34:49.136698Z","iopub.execute_input":"2023-06-27T12:34:49.137137Z","iopub.status.idle":"2023-06-27T12:34:49.251803Z","shell.execute_reply.started":"2023-06-27T12:34:49.137107Z","shell.execute_reply":"2023-06-27T12:34:49.250555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test the LLM pipeline\n\nDuring the Staging step of development, our goal is to move code and/or models from Development to Production.  In order to do so, we must test the code and/or models to make sure they are ready for Production.\n\nWe track our progress here using the [MLflow Model Registry](https://mlflow.org/docs/latest/model-registry.html).  This metadata and model store organizes models as follows:\n* **A registered model** is a named model in the registry, in our case corresponding to our summarization model.  It may have multiple *versions*.\n   * **A model version** is an instance of a given model.  As you update your model, you will create new versions.  Each version is designated as being in a particular *stage* of deployment.\n      * **A stage** is a stage of deployment: `None` (development), `Staging`, `Production`, or `Archived`.\n\nThe model we registered above starts with 1 version in stage `None` (development).\n\nIn the workflow below, we will programmatically transition the model from development to staging to production.  For more information on the Model Registry API, see the [Model Registry docs](https://mlflow.org/docs/latest/model-registry.html).  Alternatively, you can edit the registry and make model stage transitions via the UI.  To access the UI, click the Experiments menu option in the left-hand sidebar, and search for your model name.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a60736e-d7e2-4d15-8a1b-50863cfc2398","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"from mlflow import MlflowClient\n\nclient = MlflowClient()","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"110f9874-2284-4e79-b100-ab25d4dd7da2","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:03.78875Z","iopub.execute_input":"2023-06-27T12:35:03.789263Z","iopub.status.idle":"2023-06-27T12:35:03.796788Z","shell.execute_reply.started":"2023-06-27T12:35:03.789226Z","shell.execute_reply":"2023-06-27T12:35:03.794734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client.search_registered_models(filter_string=f\"name = '{model_name}'\")","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4435ab1b-7cb2-4d68-8641-ef730864c1c6","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:05.849944Z","iopub.execute_input":"2023-06-27T12:35:05.850475Z","iopub.status.idle":"2023-06-27T12:35:05.862901Z","shell.execute_reply.started":"2023-06-27T12:35:05.850427Z","shell.execute_reply":"2023-06-27T12:35:05.861541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In the metadata above, you can see that the model is currently in stage `None` (development).  In this workflow, we will run manual tests, but it would be reasonable to run both automated evaluation and human evaluation in practice.  Once tests pass, we will promote the model to stage `Production` to mark it ready for user-facing applications.\n\n*Model URIs*: Below, we use model URIs to tell MLflow which model and version we are referring to.  Two common URI patterns for the MLflow Model Registry are:\n* `f\"models:/{model_name}/{model_version}\"` to refer to a specific model version by number\n* `f\"models:/{model_name}/{model_stage}\"` to refer to the latest model version in a given stage","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b7abb09-e969-4f51-bdba-2c6bbf37249f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"model_version = 1\ndev_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\ndev_model","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b7524f15-81fa-423b-9ca7-cd949798a13c","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:10.237964Z","iopub.execute_input":"2023-06-27T12:35:10.238446Z","iopub.status.idle":"2023-06-27T12:35:11.556217Z","shell.execute_reply.started":"2023-06-27T12:35:10.238404Z","shell.execute_reply":"2023-06-27T12:35:11.554794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"*Note about model dependencies*:\nWhen you load the model via MLflow above, you may see warnings about the Python environment.  It is very important to ensure that the environments for development, staging, and production match.\n* For this demo notebook, everything is done within the same notebook environment, so we do not need to worry about libraries and versions.  However, in the Production section below, we demonstrate how to pass the `env_manager` argument to the method for loading the saved MLflow model, which tells MLflow what tooling to use to recreate the environment.\n* To create a genuine production job, make sure to install the needed libraries.  MLflow saves these libraries and versions alongside the logged model; see the [MLflow docs on model storage](https://mlflow.org/docs/latest/models.html#storage-format) for more information.  While using Databricks for this course, you can also generate an example inference notebook which includes code for setting up the environment; see [the model inference docs](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html#use-model-for-inference) for batch or streaming inference for more information.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5e96846-5bb7-4337-9450-72e5d47bc4bf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":"### Transition to Staging\n\nWe will move the model to stage `Staging` to indicate that we are actively testing it.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e5dcadb8-aa46-47da-9393-d2523b2819f8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"client.transition_model_version_stage(model_name, model_version, \"staging\")","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dec34bce-5773-42e7-90b5-64da94198785","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:17.598652Z","iopub.execute_input":"2023-06-27T12:35:17.599155Z","iopub.status.idle":"2023-06-27T12:35:17.617608Z","shell.execute_reply.started":"2023-06-27T12:35:17.599118Z","shell.execute_reply":"2023-06-27T12:35:17.616513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"staging_model = dev_model\n\n# An actual CI/CD workflow might load the `staging_model` programmatically.  For example:\n#   mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{Staging}\")\n# or\n#   mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9b148bc-b9af-44c4-92d1-d71c976afa08","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:21.294517Z","iopub.execute_input":"2023-06-27T12:35:21.295033Z","iopub.status.idle":"2023-06-27T12:35:21.301032Z","shell.execute_reply.started":"2023-06-27T12:35:21.294998Z","shell.execute_reply":"2023-06-27T12:35:21.299512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We now \"test\" the model manually on sample data. Here, we simply print out results and compare them with the original data.  In a more realistic setting, we might use a set of human evaluators to decide whether the model outperformed the previous model or system.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aca3feeb-555d-41bb-ab4f-51801b05d0ab","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"results = staging_model.predict(xsum_sample.to_pandas()[\"document\"])\ndisplay(pd.DataFrame(results, columns=[\"generated_summary\"]))","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"021e64b0-58ca-46df-baa7-252ce251a9c3","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:23.79339Z","iopub.execute_input":"2023-06-27T12:35:23.793876Z","iopub.status.idle":"2023-06-27T12:35:43.605709Z","shell.execute_reply.started":"2023-06-27T12:35:23.79384Z","shell.execute_reply":"2023-06-27T12:35:43.604562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transition to Production\n\nThe results look great!  :) Let's transition the model to Production.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"93ba17cf-b6d1-4134-843a-d47b11f9a679","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"client.transition_model_version_stage(model_name, model_version, \"production\")","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a20d97de-5580-4e86-8772-4299fec898d9","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:52.302793Z","iopub.execute_input":"2023-06-27T12:35:52.303328Z","iopub.status.idle":"2023-06-27T12:35:52.320738Z","shell.execute_reply.started":"2023-06-27T12:35:52.303284Z","shell.execute_reply":"2023-06-27T12:35:52.319695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create a production workflow for batch inference\n\nOnce the LLM pipeline is in Production, it may be used by one or more production jobs or serving endpoints.  Common deployment locations are:\n* Batch or streaming inference jobs\n* Model serving endpoints\n* Edge devices\n\nHere, we will show batch inference using Apache Spark DataFrames, with Delta Lake format.  Spark allows simple scale-out inference for high-throughput, low-cost jobs, and Delta allows us to append to and modify inference result tables with ACID transactions.  See the [Apache Spark page](https://spark.apache.org/) and the [Delta Lake page](https://delta.io/) more more information on these technologies.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"94697790-c251-4483-bc83-08782c9caad3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"# Load our data as a Spark DataFrame.\n# Recall that we saved this as Delta at the start of the notebook.\n# Also note that it has a ground-truth summary column.\nprod_data = spark.read.format(\"delta\").load(prod_data_path).limit(10)\ndisplay(prod_data)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9bdf516f-6663-4849-b791-2840c6c3f7cd","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:35:59.379584Z","iopub.execute_input":"2023-06-27T12:35:59.380071Z","iopub.status.idle":"2023-06-27T12:35:59.447555Z","shell.execute_reply.started":"2023-06-27T12:35:59.380037Z","shell.execute_reply":"2023-06-27T12:35:59.4465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below, we load the model using `mlflow.pyfunc.spark_udf`.  This returns the model as a Spark User Defined Function which can be applied efficiently to big data.  *Note that the deployment code is library-agnostic: it never references that the model is a Hugging Face pipeline.*  This simplified deployment is possible because MLflow logs environment metadata and \"knows\" how to load the model and run it.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4825b68a-b1b3-4650-9a5f-c35f792af8de","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"# MLflow lets you grab the latest model version in a given stage.  Here, we grab the latest Production version.\nprod_model_udf = mlflow.pyfunc.spark_udf(\n    spark,\n    model_uri=f\"models:/{model_name}/Production\",\n    env_manager=\"local\",\n    result_type=\"string\",\n)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e768cc5-04b9-496a-863e-248cc5ff408c","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:36:08.625019Z","iopub.execute_input":"2023-06-27T12:36:08.625548Z","iopub.status.idle":"2023-06-27T12:36:09.654465Z","shell.execute_reply.started":"2023-06-27T12:36:08.62551Z","shell.execute_reply":"2023-06-27T12:36:09.65332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run inference by appending a new column to the DataFrame\n\nbatch_inference_results = prod_data.withColumn(\n    \"generated_summary\", prod_model_udf(\"document\")\n)\ndisplay(batch_inference_results)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ba8f30a0-9588-4160-9f98-4b9ff10f1e78","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:36:16.461526Z","iopub.execute_input":"2023-06-27T12:36:16.46253Z","iopub.status.idle":"2023-06-27T12:36:16.552236Z","shell.execute_reply.started":"2023-06-27T12:36:16.462478Z","shell.execute_reply":"2023-06-27T12:36:16.550637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can now write out our inference results to another Delta table.  Here, we append the results to an existing table (and create the table if it does not exist).","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"41a4c0e8-95a9-4524-bb08-7b923940a884","inputWidgets":{},"title":""}}},{"cell_type":"code","source":"inference_results_path = \"../working/cache/m6-inference-results\".replace(\n    \"/dbfs\", \"dbfs:\"\n)\nbatch_inference_results.write.format(\"delta\").mode(\"append\").save(\n    inference_results_path\n)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04c40e64-2383-4d76-9420-657a5f673937","inputWidgets":{},"title":""},"execution":{"iopub.status.busy":"2023-06-27T12:36:41.031784Z","iopub.execute_input":"2023-06-27T12:36:41.032278Z","iopub.status.idle":"2023-06-27T12:37:18.585972Z","shell.execute_reply.started":"2023-06-27T12:36:41.032245Z","shell.execute_reply":"2023-06-27T12:37:18.584491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's it!  To create a production job, we could for example take the new lines of code above, put them in a new notebook, and schedule it as an automated workflow.  MLflow can be integrated with essentially any deployment system, but for more information specific to this Databricks workspace, see the \"Use model for inference\" documentation for [AWS](https://docs.databricks.com/machine-learning/manage-model-lifecycle/index.html#use-model-for-inference), [Azure](https://learn.microsoft.com/en-us/azure/databricks/machine-learning/manage-model-lifecycle/#--use-model-for-inference), or [GCP](https://docs.gcp.databricks.com/machine-learning/manage-model-lifecycle/index.html#use-model-for-inference).\n\nWe did not cover model serving for real-time inference, but MLflow models can be deployed to any cloud or on-prem serving systems.  For more information, see the [open-source MLflow Model Registry docs](https://mlflow.org/docs/latest/model-registry.html) or the [Databricks Model Serving docs](https://docs.databricks.com/machine-learning/model-serving/index.html).\n\nFor other topics not covered, see [\"The Big Book of MLOps.\"](https://www.databricks.com/resources/ebook/the-big-book-of-mlops)","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a4619c32-f16c-42a0-a49b-34da36688e4d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":"## Summary\n\nWe have now walked through a full example of going from development to production.  Our LLM pipeline was very simple, but LLM Ops for a more complex workflow (such as fine-tuning a custom model) would be very similar.  You still follow the basic Ops steps of:\n* Development: Creating the pipeline or model, tracking the process in the MLflow Tracking server and saving the final pipeline or model.\n* Staging: Registering a new model or version in the MLflow Model Registry, testing it, and promoting it through Staging to Production.\n* Production: Creating an inference job, or creating a model serving endpoint.","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6cc2baf-f355-4dc3-a881-1b03d566c227","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":"&copy; 2023 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>","metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"949a8dd7-1d1d-41b0-ad24-ad2aa99e86ad","inputWidgets":{},"title":""}}}]}