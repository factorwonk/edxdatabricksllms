{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63c325cc-62ae-415d-a76d-80550f165927",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b127411c-1a66-48d4-8a59-0cf92aba4e6a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 02L - Embeddings, Vector Databases, and Search\n",
    "\n",
    "\n",
    "In this lab, we will apply the text vectorization, search, and question answering workflow that you learned in the demo. The dataset we will use this time will be on talk titles and sessions from [Data + AI Summit 2023](https://www.databricks.com/dataaisummit/). \n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Learn how to use Chroma to store your embedding vectors and conduct similarity search\n",
    "1. Use OpenAI GPT-3.5 to generate response to your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8a5516-dd92-4f7e-a427-29c186bad044",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting chromadb==0.3.21\n  Downloading chromadb-0.3.21-py3-none-any.whl (46 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.4/46.4 kB 1.2 MB/s eta 0:00:00\nCollecting tiktoken==0.3.3\n  Downloading tiktoken-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 9.5 MB/s eta 0:00:00\nRequirement already satisfied: requests>=2.28 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (2.28.1)\nRequirement already satisfied: pydantic>=1.9 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (1.10.6)\nRequirement already satisfied: fastapi>=0.85.1 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (0.98.0)\nCollecting hnswlib>=0.7\n  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status 'done'\n  Getting requirements to build wheel: started\n  Getting requirements to build wheel: finished with status 'done'\n  Preparing metadata (pyproject.toml): started\n  Preparing metadata (pyproject.toml): finished with status 'done'\nCollecting numpy>=1.21.6\n  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nRequirement already satisfied: pandas>=1.3 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (1.4.4)\nCollecting clickhouse-connect>=0.5.7\n  Downloading clickhouse_connect-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (964 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 964.5/964.5 kB 16.7 MB/s eta 0:00:00\nCollecting duckdb>=0.7.1\n  Downloading duckdb-0.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.8/17.8 MB 48.1 MB/s eta 0:00:00\nRequirement already satisfied: uvicorn[standard]>=0.18.3 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (0.23.2)\nCollecting posthog>=2.4.0\n  Downloading posthog-3.4.1-py2.py3-none-any.whl (41 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.1/41.1 kB 6.7 MB/s eta 0:00:00\nRequirement already satisfied: sentence-transformers>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from chromadb==0.3.21) (2.2.2)\nRequirement already satisfied: regex>=2022.1.18 in /databricks/python3/lib/python3.10/site-packages (from tiktoken==0.3.3) (2022.7.9)\nRequirement already satisfied: urllib3>=1.26 in /databricks/python3/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (1.26.11)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2022.1)\nCollecting lz4\n  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 63.9 MB/s eta 0:00:00\nRequirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.21) (2022.9.14)\nCollecting zstandard\n  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.4/5.4 MB 79.5 MB/s eta 0:00:00\nRequirement already satisfied: starlette<0.28.0,>=0.27.0 in /databricks/python3/lib/python3.10/site-packages (from fastapi>=0.85.1->chromadb==0.3.21) (0.27.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.21) (2.8.2)\nCollecting backoff>=1.10.0\n  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\nCollecting monotonic>=1.5\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from posthog>=2.4.0->chromadb==0.3.21) (1.16.0)\nRequirement already satisfied: typing-extensions>=4.2.0 in /databricks/python3/lib/python3.10/site-packages (from pydantic>=1.9->chromadb==0.3.21) (4.3.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.21) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.21) (3.3)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.64.1)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.16.4)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.9.1)\nRequirement already satisfied: torch>=1.6.0 in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.13.1+cpu)\nRequirement already satisfied: nltk in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (3.7)\nRequirement already satisfied: torchvision in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.14.1+cpu)\nRequirement already satisfied: sentencepiece in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (0.1.99)\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (4.30.2)\nRequirement already satisfied: scikit-learn in /databricks/python3/lib/python3.10/site-packages (from sentence-transformers>=2.2.2->chromadb==0.3.21) (1.1.1)\nRequirement already satisfied: h11>=0.8 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.14.0)\nRequirement already satisfied: click>=7.0 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (8.0.4)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.17.0)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (6.0)\nRequirement already satisfied: httptools>=0.5.0 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.6.0)\nRequirement already satisfied: python-dotenv>=0.13 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (1.0.0)\nRequirement already satisfied: websockets>=10.4 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (11.0.3)\nRequirement already satisfied: watchfiles>=0.13 in /databricks/python3/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.21) (0.19.0)\nRequirement already satisfied: packaging>=20.9 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (21.3)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.6.0)\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (2022.7.1)\nRequirement already satisfied: anyio<5,>=3.4.0 in /databricks/python3/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb==0.3.21) (3.5.0)\nRequirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.3.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /databricks/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (0.13.3)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk->sentence-transformers>=2.2.2->chromadb==0.3.21) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=2.2.2->chromadb==0.3.21) (2.2.0)\nCollecting numpy>=1.21.6\n  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 62.5 MB/s eta 0:00:00\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /databricks/python3/lib/python3.10/site-packages (from torchvision->sentence-transformers>=2.2.2->chromadb==0.3.21) (9.2.0)\nRequirement already satisfied: sniffio>=1.1 in /databricks/python3/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi>=0.85.1->chromadb==0.3.21) (1.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=2.2.2->chromadb==0.3.21) (3.0.9)\nBuilding wheels for collected packages: hnswlib\n  Building wheel for hnswlib (pyproject.toml): started\n  Building wheel for hnswlib (pyproject.toml): finished with status 'done'\n  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287598 sha256=b2bde5f2224f66d534bc68d78dba14ceb4b34ad0d984d290b32731cb5d7ac59b\n  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\nSuccessfully built hnswlib\nInstalling collected packages: monotonic, zstandard, numpy, lz4, duckdb, backoff, tiktoken, posthog, hnswlib, clickhouse-connect, chromadb\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.5\n    Not uninstalling numpy at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bacc7280-c8fe-42bb-8e36-87a5e5441ec2\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: tiktoken\n    Found existing installation: tiktoken 0.4.0\n    Not uninstalling tiktoken at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-bacc7280-c8fe-42bb-8e36-87a5e5441ec2\n    Can't uninstall 'tiktoken'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npetastorm 0.12.1 requires pyspark>=2.1.0, which is not installed.\ndatabricks-feature-store 0.14.3 requires pyspark<4,>=3.1.2, which is not installed.\nydata-profiling 4.2.0 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.4 which is incompatible.\nnumba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.4 which is incompatible.\nmleap 0.20.0 requires scikit-learn<0.23.0,>=0.22.0, but you have scikit-learn 1.1.1 which is incompatible.\nSuccessfully installed backoff-2.2.1 chromadb-0.3.21 clickhouse-connect-0.7.0 duckdb-0.10.0 hnswlib-0.8.0 lz4-4.3.3 monotonic-1.6 numpy-1.24.4 posthog-3.4.1 tiktoken-0.3.3 zstandard-0.22.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb==0.3.21 tiktoken==0.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ef88297-c98e-44c3-8e98-cf5284cdc363",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9902549e-2d38-4c19-916c-ffcfbc6f171b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| Enumerating serving endpoints...found 5...(0 seconds)\n| Removing the working directory \"dbfs:/mnt/dbacademy-users/labuser5400519@vocareum.com/large-language-models/working\"...(0 seconds)\n\nSkipping download of existing archive to \"dbfs:/mnt/dbacademy-datasets/large-language-models/v03\" \n| Validating local assets:\n| | Listing local files...(0 seconds)\n| | Validation completed...(0 seconds total)\n|\n| Skipping the unpacking of datasets to \"dbfs:/mnt/dbacademy-users/labuser5400519@vocareum.com/large-language-models/datasets\" \n|\n| Dataset installation completed (0 seconds)\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "321a7318-f3c8-490c-86d3-f37e1d09d8d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Ready!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/labuser5400519@vocareum.com/large-language-models/working\n| DA.paths.user_db:     /dbfs/mnt/dbacademy-users/labuser5400519@vocareum.com/large-language-models/working/database.db\n| DA.paths.datasets:    /dbfs/mnt/dbacademy-users/labuser5400519@vocareum.com/large-language-models/datasets\n\nSetup completed (15 seconds)\n\nThe models developed or used in this course are for demonstration and learning purposes only.\nModels may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "print(\"Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fe390b8-004e-4078-8df3-1685fe05d52e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3369a0dd-f012-45c3-8a35-2e09faf688ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Title</th><th>Abstract</th></tr></thead><tbody><tr><td>Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse</td><td>Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\n",
       " \n",
       " Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \n",
       " \n",
       " These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\n",
       " - Modular and composable code\n",
       " - Unit testing framework\n",
       " - Incremental event processing with spark structured streaming \n",
       " - Granular resource tuning for better performance and cost efficacy\n",
       " \n",
       " Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse.</td></tr><tr><td>Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System</td><td>Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community.</td></tr><tr><td>From Snowflake to Enterprise-Scale Apache Spark™</td><td>Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers.\n",
       " \n",
       " The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™  solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs.\n",
       " \n",
       " In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to:\n",
       " \n",
       " * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation\n",
       " * Handle X100 queries per second on a single Spark application with sub-second query latency\n",
       " * Protect Spark application from misbehaving users \n",
       " * Optimize SQL-based queries</td></tr><tr><td>The Future of Data Orchestration: Asset-Based Orchestration</td><td>Data orchestration is a core component for any batch data processing platform and we’ve been using patterns that haven't changed since the 1980s. \n",
       " \n",
       " In this session, I’ll be introducing a new pattern and way of thinking for data orchestration known as asset-based orchestration, with data freshness sensors to trigger pipelines. I will demo this new pattern using popular tools of the modern data stack - dbt, airbyte, dagster, and databricks.</td></tr><tr><td>Photon for Dummies: How Does this New Execution Engine Actually Work?</td><td>Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it.\n",
       " \n",
       " If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. \n",
       " \n",
       " Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as:\n",
       " - Why is a Java Virtual Machine the current bottleneck for spark enhancements?\n",
       " - What does vectorized even mean? And how was it done before?\n",
       " - Why is the relationship status between Spark and Photon 'It’s complicated'?\n",
       " \n",
       " In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. \n",
       " \n",
       " I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to. \n",
       " \n",
       " </td></tr><tr><td>Monitoring Delta Live Tables</td><td>In this session we will share how Volvo Group monitors Databricks jobs and DLTs to stay ahead of the curve. Volvo Group uses Databricks for - amongst others - material planning. Based on live input from warehouse systems, the system predicts which orders to place with the suppliers to ensure the availability of spare parts and keep the trucks running. Since this is a critical component, with reverse ETL feeding the recommendation back into the warehouse systems, strict monitoring to avoid pipeline congestion is required. In this talk I will:\n",
       " * Explain a vision of data quality (deliver trustworthy data in time); which metrics to set up to monitor data quality\n",
       " * How it helps Volvo Group; make the SLA, jointly use with FinOps to improve ingestion pipelines, avoid congestion\n",
       " * Setup; use Jobs API and Databricks SQL workspace to build and consult the monitoring dashboard\n",
       " * Demo\n",
       " * Get notebook from GitHub</td></tr><tr><td>Data Quality: Fast and Slow</td><td>Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \n",
       " \n",
       " In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\n",
       " \n",
       " This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical.</td></tr><tr><td>Taking Your Cloud Vendor to the Next Level: Solving Complex Challenges with Azure Databricks</td><td>Akamai's content delivery network (CDN) processes about 30% of the internet's daily traffic, resulting in a massive amount of data that presents engineering challenges, both internally and with cloud vendors. In this session, we will discuss the barriers faced while building a data infrastructure on Azure, Databricks, and Kafka to meet strict SLAs, hitting the limits of some of our cloud vendors’ services. We will describe the iterative process of re-architecting a massive-scale data platform using the aforementioned technologies. We will also delve into how today, Akamai is able to quickly ingest and make available to customers TBs of data, as well as efficiently query PBs of data and return results within 10 seconds for most queries. This discussion will provide valuable insights for attendees and organizations seeking to effectively process and analyze large amounts of data.</td></tr><tr><td>Unleashing the Power of Interactive Analytics at Scale with Databricks and Delta Lake: Lessons Learned from Building Akamai's Web Security Analytics Product</td><td>Akamai is a leading content delivery network (CDN) and cybersecurity company, operating hundreds of thousands of servers in more than 135 countries worldwide.\n",
       " In this talk, we will share our experiences and lessons learned from building and maintaining the Web Security Analytics (WSA) product, an interactive analytics platform powered by Databricks and Delta Lake, that enables customers to efficiently analyze and take informed action on a high volume of streaming security events. \n",
       " \n",
       " The WSA platform must be able to serve hundreds of queries per minute, scanning hundreds of terabytes of data from a six petabyte data lake, with most queries returning results within ten seconds (for both aggregation queries and needle in a haystack queries).\n",
       " The talk will cover how to use Databricks SQL warehouses and job clusters cost-effectively, and how to improve query performance through the use of tools and techniques such as Delta Lake, Databricks Photon, and partitioning. \n",
       " This talk will be valuable for anyone looking to build and operate a high-performance analytics platform.</td></tr><tr><td>ABN Story: Migrating to Future Proof Data Platform</td><td>ABN AMRO Bank is one of the top leading banks in the Netherlands; it is the 3rd largest bank in the Netherlands by revenue and number when it comes to mortgages within the Netherlands. We have an objective to become a fully data-driven bank and this goal is well supported by our top management.\n",
       " \n",
       " ABN AMRO started its data journey almost seven years ago and has built a data platform off-premises with Hadoop technologies. This data platform has been used by more than 200 data providers, 150 data consumers, and overall more than 3000 Datasets.\n",
       " \n",
       " To become a fully digital bank and address the limitation of the on-premises platform, we needed a future-proof data platform DIAL (digital integration and access layer.) ABN AMRO decided to build an Azure cloud-native data platform with the help of Microsoft and Databricks. Last year this cloud-native platform was ready for our data providers and data consumers. Six months ago we started the journey of migrating all the content from the on-premises data platform to the Azure data platform, this was a very large-scale migration and was achieved in 6 months.\n",
       " \n",
       " In this session, we will focus on two things :\n",
       " 1. Share our strategy for migration from on-premises to a cloud-native platform. \n",
       " 2. Share how we used Databricks products in our data platform and how the Databricks team helped us in the overall migration.\n",
       " \n",
       " </td></tr><tr><td>Simon + Denny Live: Ask Us Anything</td><td>Simon and Denny have been discussing and debating all things Delta, Lakehouse and Apache Spark™ on their regular webshow. Whether you want advice on lake structures, want to hear their opinions on the latest trends and hype in the data world, or you simply have a tech implementation question to throw at two seasoned experts, these two will have something to say on the matter. In their previous shows, Simon and Denny focused on building out a sample lakehouse architecture, refactoring and tinkering as new features came out, but now we're throwing the doors open for any and every question you might have.\n",
       " \n",
       " So if you've had a niggling question and these two can help, this is the session for you. There will be a question submission form shared prior to the event, so the team will be prepped with a whole bunch of topics to talk through. Simon and Denny want to hear YOUR questions, which they can field from a wealth of industry experience, wide ranging community engagement and their differing perspectives as external consultant and internal Databricks respectively. There's also a chance they'll get distracted and go way off track talking about coffee, sci-fi, nerdery or the English weather. It happens.</td></tr><tr><td>Sometimes Less is More: A Deep Dive into the Evolution of a Large, Complex, Real-Time Data Pipeline</td><td>This session will dive into technical detail around one of my complex cybersecurity endpoint detection and response (EDR) data pipelines - which spans multiple industries - that is being joined with an indicator of compromise (IoC) feed. I'll detail the stages we went through in the initial evolution of the pipeline and why it ultimately made more sense to apply less schema structure to it and focus on using a non-obvious partitioning strategy when working with this data source. All code is in Python/Pyspark and uses Delta Live Tables.</td></tr><tr><td>Use Apache Spark™  from Anywhere: Remote connectivity with Spark Connect</td><td>Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark™. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\n",
       " \n",
       " However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\n",
       " \n",
       " Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages.\n",
       " \n",
       " This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere.</td></tr><tr><td>Seven Things You Didn't Know You Can Do with Databricks Workflows</td><td>Databricks workflows has come a long way since the initial days of orchestrating simple notebooks and jar/wheel files. Now we can orchestrate multi-task jobs and create a chain of tasks with lineage and DAG with either fan-in or fan-out among multiple other patterns or even run another Databricks job directly inside another job. Databricks workflows takes its tag: “orchestrate anything anywhere” pretty seriously and is a truly fully-managed, cloud-native orchestrator to orchestrate diverse workloads like Delta Live Tables, SQL, Notebooks, Jars, Python Wheels, dbt, SQL, Apache Spark™, ML pipelines with excellent monitoring, alerting and observability capabilities as well. Basically it is a one-stop product for all orchestration needs for an efficient Lakehouse. And what is even better is, it gives full flexibility of running your jobs in a cloud-agnostic and cloud-independent way and is available across AWS, Azure and GCP. In this talk we will discuss and deep dive on some of the very interesting features and will showcase end-to-end demos of the features which will allow you to take full advantage of Databricks workflows for orchestrating the Lakehouse.</td></tr><tr><td>Using DMS and DLT for Change Data Capture </td><td>[Abstract TBD]</td></tr><tr><td>Deep Dive into the New Features of Apache Spark™  3.4</td><td>In 2022, Apache Spark™ was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. \n",
       " \n",
       " We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling.</td></tr><tr><td>Delta Live Tables: A Modern Approach to Data Pipelines</td><td>Data engineers have the difficult task of cleansing complex, diverse data, and transforming it into a usable source to drive data analytics, data science, and machine learning. They need to know the data infrastructure platform in depth, build complex queries in various languages and stitch them together for production. Join this session to learn how Delta Live Tables (DLT) simplifies the complexity of data transformation and ETL. DLT is the first ETL framework to use modern software engineering practices to deliver reliable and trusted data pipelines at any scale. Discover how analysts and data engineers can innovate rapidly with simple pipeline development and maintenance, how to remove operational complexity by automating administrative tasks and gaining visibility into pipeline operations, how built-in quality controls and monitoring ensure accurate BI, data science, and ML, and how simplified batch and streaming can be implemented with self-optimizing and auto-scaling data pipelines.</td></tr><tr><td>Journey Towards Uniting Metastores</td><td>This session will provide a brief overview about Nationwide’s journey towards implementing Unity Catalog at an Enterprise Level. We will cover the following topics.\n",
       " - Identity management structure\n",
       " - Compute framework\n",
       " - Naming standards and usage best practices\n",
       " - And a little bit about how Delta Sharing will help us ingest 3rd party data\n",
       " Unity Catalog has been a core feature towards strengthening Data Lakehouse architecture for multiple business units.</td></tr><tr><td>How CIBC Integrated Purview and Unity Catalog for Our Data Governance Within a Data Mesh Architecture Leveraging the Lakehouse</td><td>This session will focus on our journey to integrate Purview and Unity Catalog for our data governance within data mesh architecture, while leveraging the lakehouse. Our approach is to build four foundational pillars of data mesh that will uphold data management practices based on consumer-driven and domain-centric data sources. \n",
       " Principle 1: decentralized ownership\n",
       " Principle 2: federated governance\n",
       " Principle 3: data as a product\n",
       " Principle 4: self-service infrastructure\n",
       " Native access controls within the self-service infrastructure are enhanced by pushing policy defined access to unity to enable fine-grain control across the data landscape.</td></tr><tr><td>The Future of Data Access Control: Booz Allen Hamilton’s Approach to Securing our Databricks Lakehouse with Immuta</td><td>In this session, I’ll review how we utilize Attribute-Based Access Control (ABAC) to enforce policy via Immuta. I’ll discuss the differences between the ABAC and legacy Role-Based Access Control (RBAC) approaches to control access and how the RBAC approach is not sufficient to keep up with today’s growing big data market. With so much data available, there also comes substantial risk. Data can contain many sensitive data elements, including PII and PHI. Industry leaders like Databricks are pushing the boundaries of data technology, which leads to constantly evolving data use cases. And that’s a good thing! However, the RBAC approach is struggling to keep up with those advancements.\n",
       " So what is RBAC? It’s an approach to data access that permits system access based on the end-user’s role. For legacy systems, it’s meant as a simple but effective approach to securing data. Are you a manager? Then you’ll get access to data meant for managers. This is great for small deployments with clearly defined roles. Here at Booz Allen, we invested in Databricks because we have an environment of over 30k users and billions of rows of data.\n",
       " To mitigate this problem and align with our forward-thinking company standard, we introduced Immuta into our stack. Immuta uses ABAC to allow for dynamic data access control. Users are automatically assigned certain attributes, and access is based on those attributes instead of just their role. This allows for more flexibility and allows data access control to easily scale without the need to constantly map a user to their role. Using attributes, we can write policies in one place and have them applied across all of our data platforms. This makes for a truly holistic data governance approach and provides immediate ROI and time savings for the company.</td></tr><tr><td>Increasing Trust in Your Data: Enabling a Data Governance Program on Databricks Using Unity Catalog and ML-Driven MDM</td><td>As part of Comcast Effectv’s transformation into a completely digital advertising agency, it was key to develop an approach to manage and remediate data quality issues related to customer data so that the sales organization is using reliable data to enable data-driven decision making. Like many organizations, Effectv's customer lifecycle processes are spread across many systems utilizing various integrations between them. This results in key challenges like duplicate and redundant customer data that requires rationalization and remediation. Data is at the core of Effectv’s modernization journey with the intended result of winning more business, accelerating order fulfillment, reducing make-goods and identifying revenue.\n",
       "  \n",
       "In partnership with Slalom Consulting, Comcast Effectv built a traditional lakehouse on Databricks to ingest data from all of these systems but with a twist; they anchored every engineering decision in how it will enable their data governance program. \n",
       "  \n",
       "In this talk, we will touch upon the data transformation journey at Effectv and dive deeper into the implementation of data governance leveraging Databricks solutions such as Delta Lake, Unity Catalog and DB SQL. Key focus areas include how we baked master data management into our pipelines by automating the matching and survivorship process, and bringing it all together for the data consumer via DBSQL to use our certified assets in bronze, silver and gold layers.\n",
       "  \n",
       "By making thoughtful decisions about structuring data in Unity Catalog and baking MDM into ETL pipelines, you can greatly increase the quality, reliability, and adoption of single-source-of-truth data so your business users can stop spending cycles on wrangling data and spend more time developing actionable insights for your business.</td></tr><tr><td>Activate Your Lakehouse with Unity Catalog</td><td>Building a Lakehouse is straightforward today thanks to many open-source technologies and Databricks. However, it can be taxing to extract value from Lakehouses as they grow in size without robust data operations. Join us to learn how YipitData uses the Unity Catalog to streamline data operations and discover best practices to scale your own Lakehouse.\n",
       " \n",
       " At YipitData, our 15+ petabyte Lakehouse is a self-service data platform built with Databricks and AWS, supporting analytics for a 250+ data team. We will share how leveraging Unity Catalog accelerates our mission to help financial institutions and corporations leverage alternative data by:\n",
       " \n",
       " - Enabling clients to universally access our data through a spectrum of channels, including Sigma, Delta Sharing, and multiple clouds \n",
       " - Fostering collaboration across internal teams using a data mesh paradigm that yields rich insights\n",
       " - Strengthening the integrity and security of data assets through ACLs, data lineage, audit logs, and further isolation of AWS resources\n",
       " - Reducing the cost of large tables without downtime through automated data expiration and ETL optimizations on managed delta tables\n",
       " \n",
       " Through our migration to Unity Catalog, we have gained tactics and philosophies to seamlessly flow our data assets internally and externally. Data platforms need to be value-generating, secure, and cost-effective in today's world. We are excited to share how Unity Catalog delivers on this and helps you get the most out of your Lakehouse.</td></tr><tr><td>Self Service Data Analytics and governance at enterprise scale with unity catalog</td><td>This session focuses on one of the first Unity Catalog implementations for a large-scale enterprise. In this scenario, a cloud scale analytics platform with 7500 active users based on the lakehouse approach is used. In addition, there is potential for 1500 further users who are subject to special governance rules. They are consuming 600+TB of data stored in Delta Lake - continuously growing at more than 1TB per day. This might grow due to local country data.\n",
       " Therefore, the existing data platform must be extended to enable users to combine global and local data from their countries. A new data management was required, which reflects the strict information security rules at a need to know base. Core requirements are: read only from global data, write into local and share the results.\n",
       " Due to a very pronounced information security awareness and a lack of the technological possibilities it was not possible to interdisciplinary analyze and exchange data so easy or at all so far. Therefore, a lot of business potential and gains could not be identified and realized.\n",
       " With the new developments in the technology used and the basis of the Lakehouse approach, thanks to Unity Catalog, we were able to develop a solution that could meet high requirements for security and process. And enables globally secured interdisciplinary data exchange and analysis at scale.\n",
       " This solution enables the democratization of the data. This results not only in the ability to gain better insights for business management, but also to generate entirely new business cases or products that require a higher degree of data integration and encourage the culture to change.\n",
       " We highlight technical challenges and solutions, present best practices and point out benefits of implementing Unity catalog for enterprises.</td></tr><tr><td>PII Detection at Scale on the Lakehouse</td><td>SEEK is Australia’s largest online employment marketplace and a market leader spanning ten countries across Asia Pacific and Latin America. SEEK provides employment opportunities for roughly 16 million monthly active users and process 25 million candidate applications to listings.\n",
       " \n",
       " Processing millions of resumès involves handling and managing highly sensitive candidate information, usually inputted in a highly unstructured format. With recent high-profile data leaks in Australia, personally identifiable information (PII) protection has become a major focus area for large digital organizations. The first step is detection, and SEEK has developed a custom framework built using HuggingFace transformers fine tuned with nuances around employment. For example, “Software Engineer at Databricks” is not PII, but “CEO at Databricks” is PII. \n",
       " \n",
       " After identifying and anonymizing PII in stream and batch data, SEEK uses Unity Catalog’s data lineage to track PII through their reporting, ETL, and other downstream ML use-cases and govern access control achieving an organization-wide data management capability driven by deep learning and enforcement using Databricks.</td></tr><tr><td>Essential Data Security Strategies for the Modern Enterprise Data Architecture</td><td>Balancing critical data requirements is a 24-7 task for enterprise-level organizations that must straddle the need to open specific gates to enable self-service data access while closing other access points to maintain internal and external compliance. Data breaches can cost U.S. businesses an average of $9.4 million per occurrence; ignoring this leaves organizations vulnerable to severe losses and crippling costs. \n",
       " \n",
       "The 2022 Gartner Hype Cycle for Data Security reports that more and more enterprises are modernizing their data architecture with cloud and technology partners to help them collect, store and manage business data; a trend that does not appear to be letting up.\n",
       " \n",
       "According to Gartner®, “by 2025, 30% of enterprises will have adopted bDSP (Broad Data Security Platform), up from less than 10% in 2021, due to the pent-up demand for higher levels of data security and the rapid increase in product capabilities.\"</td></tr><tr><td> \n",
       "Moving to both a modern data architecture and data-driven culture sets enterprises on the right trajectory for growth</td><td> but it’s important to keep in mind individual public cloud platforms are not guaranteed to protect and secure data. To solve this</td></tr><tr><td>Using Open-Source Tools to Build Privacy-Conscious Data Systems</td><td>With the rapid proliferation of consumer data privacy laws across the world, it is becoming a strict requirement for data organizations to be mindful of data privacy risks. Privacy violation fines are reaching record highs and will only get higher as governments continue to crack down on the runaway abuse of user data. To continue producing value without becoming a liability, data systems must include privacy protections at a foundational level.\n",
       " \n",
       " The most practical way to do this is to enable privacy as code, shifting privacy left and including it as a foundational part of the organization's software development life cycle. The promise of privacy as code is that data organizations can be liberated from inefficient, manual workflows for producing the compliance deliverables their legal teams need, and instead ship at speed with pre-defined privacy guardrails built into the structure of their preferred workflows.\n",
       " \n",
       " Despite being an emerging and complex problem, there are already powerful open source tools available designed to help organizations of all sizes achieve this outcome. Fides is an open source privacy as code tool, written in Python and Typescript, that is engineered to tackle a variety of privacy problems throughout the application lifecycle. The most relevant feature for data organizations is the ability to annotate systems and their datasets with data privacy metadata, thus enabling automatic rejection of dangerous or illegal uses. Fides empowers data organizations to be proactive, not reactive, in terms of protecting user privacy and reducing organizational risk. Moving forward data privacy will need to be top of mind for data teams.</td></tr><tr><td>Leveraging Unity Catalog for Data Governance for Grab’s Use Case</td><td>Grab has been looking for a tool that provides data access to over 2000 users to support our daily operations. This tool should provide granular access control and easily scale to over thousands of users and tables, with data governance and security in mind.\n",
       " \n",
       " Through various evaluations, Grab concluded that Unity Catalog is a good fit that addresses all the requirements of data governance and security. For example, it provides row-level-security (RLS) and dynamic-data-masking (DDM) capabilities (through dynamic views), centralized governance and security at both table and object level. \n",
       " This ensures sensitive PII data is well protected, and we are able to open up tables to a wider audience ensuring only entitled data is shown. More importantly, we are able to meet the required auditing standards.\n",
       " \n",
       " With Unity catalog, we managed to:\n",
       " 1. Build a custom solution that allows users to seamlessly access data with UC.\n",
       " 2. Easily interface and integrate into existing Grab’s in-house entitlement system to translate access requirements, unifying the end-to-end user experience for requesting data access permission. \n",
       " 3. Extend UC’s capability by leveraging existing UC APIs and Databricks workflows to easily automate administrative tasks. For example, we have fully automated the creation of RLS and DDM views on-the-fly.\n",
       " 4. Provide out of the box, comprehensive auditing capabilities easily.\n",
       " \n",
       " As a result, we managed to achieve this in a short period of time to support over 2000 users to achieve better governance and significant cost savings with an ROI of about 2.7x</td></tr><tr><td>Unity Catalog: Flexibility to Fit Your Organization</td><td>Databricks Unity Catalog provides the flexibility to meet the needs of any-sized organization, from startups to companies the size of Nike. \n",
       " \n",
       " Unity Catalog and lakehouse architecture have been chosen by Nike to store and process all of Nike's data. Unity Catalog has resolved systemic challenges in the Data Lake, resulting in 1) a simpler and transparent Data Lake, and, 2) allowing for scaled data and analytics use cases with greater efficiency. \n",
       " \n",
       " Designing Unity Catalog for the scale of Nike met the following goals:\n",
       " \n",
       " * Ownership of data: how all data produced in the lake has clear team ownership, communication channels, and accountability\n",
       " * Data environments: accelerated development of pipelines against real Production Data with zero risk\n",
       " * Consistent team semantics: all teams on Lakehouse follow the same semantics, allowing for scaled onboarding and consistent development expectations\n",
       " * Consistent governed data access: a single simplified, governed, data access process for all data in the Lake\n",
       " * Monitoring of unused data and data pipelines: simplified observability through Unity Catalog Data Lineage to quickly identify any unused data or data pipelines, resulting in less storage and compute costs\n",
       " \n",
       " These capabilities are powering Nike as a data driven organization, allowing the company to quickly make informed decisions.</td></tr><tr><td>Automating Sensitive Data (PII/PHI) Detection and Quarantining to a Databricks Clean Room</td><td>Healthcare datasets contain both personally identifiable information (PII) and personal health information (PHI) that needs to be de-identified in order to protect patient confidentiality and ensure HIPAA compliance. This privacy data is easily detected when it’s provided in columns  labeled with names such as “SSN,” First Name,” “Full Name,” and “DOB;” however, it is much harder to detect when it is hidden within columns labeled “Doctor Notes,” “Diagnoses,” or “Comments.” HealthVerity, a leader in the HIPAA-compliant exchange of real-world data (RWD) to uncover patient, payer and genomic insights and power innovation for the healthcare industry, ensures healthcare datasets are de-identified from PII and PHI using elaborate privacy procedures. During this presentation, we will demonstrate how to use a low-code/no-code platform to simplify and automate data pipelines that leverage prebuilt ML models to scan data for PHI/PII leakage and quarantine those rows in Unity Catalog when leakage is identified and move them to a Databricks clean room for analysis.</td></tr><tr><td>Cross-Platform Data Lineage with OpenLineage</td><td>There are more data tools available than ever before, and it's easier to build a pipeline than it's ever been. This has resulted in an explosion of innovation, but it also means that data within today's organizations has become increasingly distributed. It can't be contained within a single brain, a single team, or a single platform. Data lineage can help by tracing the relationships between datasets and providing a map of your entire data universe. OpenLineage provides a standard for lineage collection that spans multiple platforms, including Apache Airflow, Apache Spark™, Flink®, and dbt. This empowers teams to diagnose and address widespread data quality and efficiency issues in real time. In this session, Julien Le Dem from Astronomer will show how to trace data lineage across Apache Spark and Apache Airflow. He will walk through the OpenLineage architecture and provide a live demo of a running pipeline with real-time data lineage.</td></tr><tr><td>Ahold Delhaize's Journey to Implementing Unity Catalog at Scale</td><td>In this session we will explore the steps we took to implement Unity Catalog at Ahold Delhaize's data platform. This powerful tool has completely transformed the way we access and manage data objects, making it easier than ever for our entire organization to find and use the data we need. The catalog brings us a governance layer that allows us to more securely and easily share restricted or confidential data by providing row and column level security per project. But that's not all, we're also setting up the Unity Catalog as a self-service tool, allowing users to easily customize and set up their own catalogs, schema's, views or user groups using simple YAML configuration files. We will cover topics such as:\n",
       " * Architecture where we came from and the improvement Unity Catalog has made\n",
       " * Our self-service way of working by showing code snippets\n",
       " * Our fully automatic deployments by explaining our flow\n",
       " By the end of this talk, you will have a better understanding of the process of implementing Unity Catalog at scale.</td></tr><tr><td>Multi-Cloud Data Governance on the Databricks Lakehouse</td><td>Across industries, a multi-cloud setup has quickly become the reality for large organizations. Multi-cloud introduces new governance challenges as permissions models often do not translate from one cloud to the other and if they do, are insufficiently granular to accommodate privacy requirements and principles of least privilege. This problem can be especially acute for Data and AI workloads that rely on sharing and aggregating large and diverse data sources across business unit boundaries and where governance models need to incorporate assets such as table rows/columns and ML features and models. \n",
       " \n",
       " In this session we will provide guidelines on how best to overcome these challenges for companies that have adopted the Databricks Lakehouse as their collaborative space for data teams across the organization, by exploiting some of the unique product features of the Databricks platform.\n",
       " We will focus on a common scenario: a data platform team providing data assets to two different ML teams, one using the same cloud and the other one using a different cloud. We will explain the step-by-step setup of a unified governance model by leveraging the following components and conventions:\n",
       " \n",
       " - Unity Catalog for implementing fine-grained access control across all data assets: files in cloud storage, rows and columns in tables and ML features and models\n",
       " - The Databricks Terraform provider to automatically enforce guardrails and permissions across clouds \n",
       " - Account level SSO Integration and identity federation to centralize administer access across workspaces\n",
       " - Delta sharing to seamlessly propagate changes in provider data sets to consumers in near real-time\n",
       " - Centralized audit logging for a unified view on what asset was accessed by whom</td></tr><tr><td>How Comcast Effectv Drives Data Observability with Databricks and Monte Carlo</td><td>Comcast Effectv, the 2,000-employee advertising wing of Comcast, America’s largest telecommunications company, provides custom video ad solutions powered by aggregated viewership data. As a global technology and media company connecting millions of customers to personalized experiences and processing billions of transactions, Comcast Effectv was challenged with handling massive loads of data, monitoring hundreds of data pipelines, and managing timely coordination across data teams. In this talk, Robinson Creighton, Sr. DataOps Lead of Advanced Analytics at Effectv, and Lior Gavish, CTO & co-founder at Monte Carlo, will discuss Comcast Effectv’s journey to building a more scalable, reliable lakehouse and driving data observability at scale with Monte Carlo. This has enabled Effectv to have a single pane of glass view of their entire data environment to ensure consumer data trust across their entire AWS, Databricks, and Looker environment.</td></tr><tr><td>Unity Catalog and Marketplace: Door to a Data Driven Organization</td><td>Being data-driven can help organizations make better decisions, improve efficiency, increase competitiveness, and reduce compliance risks.\n",
       " \n",
       " Organizations expect the workforce to embrace data but at the same time want the process to be governance-driven, auditable, traceable and cost-center based for back charging.\n",
       " \n",
       " Unity Catalog is a tool that helps organizations manage and organize their data assets in a unified manner. By using Unity Catalog, you can create a central repository for all of your data assets, including structured and unstructured data, and provide access to this repository to authorized users within your organization. This can help you build a data-driven organization by enabling easy access to data for analysis and decision-making, and by promoting data governance and data management best practices.\n",
       " When Unity Catalog is made the front door for an organization’s data estate then it enables the organization to unlock the following:\n",
       " Governance-driven data discovery\n",
       " Governance-driven data access\n",
       " Governance-driven ephemeral workspace access\n",
       " Cost center based back charging\n",
       " Auditable and traceable\n",
       " \n",
       " Internal marketplace will help organizations publish and provide access to data within the organization’s workspaces via an out-of-the-box lightweight solution powered by Unity Catalog. \n",
       " \n",
       " Depending on the level of complexity and back charging, isolation requirements, organizations can choose to use the out-of-the-box internal marketplace or build the end-to-end process of publishing, provisioning, destroying, back charging process via Unity Catalog plus Workspace APIs.</td></tr><tr><td>Data Mesh Realization on Databricks: Making Data Engineering and Consumption Self-Service Driven for Data Platforms</td><td>\"Our client, a consulting giant, as part of the famous \"\"Big 4\"\" is creating tax</td></tr><tr><td>Distributing Data Governance: How Unity Catalog Allows for a Collaborative Approach</td><td>As one of the world’s largest content delivery network (CDN) and security solutions provider, Akamai owns thousands of data assets of various shapes and sizes, some even go up to multiple PBs.\n",
       " Several departments within the company leverage Databricks for their data and AI workloads, which means we have over a hundred Databricks workspaces within a single Databricks account, where some of the assets are shared across products, and some are product-specific.\n",
       "  \n",
       " In this presentation, we will describe how to use the capabilities of Unity Catalog to distribute the administration burden between departments, while still maintaining a unified governance model.\n",
       " \n",
       " We will also share the benefits we’ve found in using Unity Catalog, beyond just access management, such as: \n",
       " * Visibility into which data assets we have in the organization\n",
       " * Ability to identify and potentially eliminate duplicate data workloads between departments \n",
       " * Removing boilerplate code for accessing external sources\n",
       " * Increasing innovation of product teams by exposing the data assets in a better, more efficient way</td></tr><tr><td>Unity Catalog at Scale in Retail Data Engineering and Data Science</td><td>Retail data science, insights, and media company 84.51° helps Kroger and other organizations create more personalized and valuable experiences for shoppers. Powered by cutting edge science, we leverage first-party retail data from nearly one out of two US households totaling over 2B transactions a year. Databricks is integral to the data science and machine learning work at 84.51° and manages 35+ PB of first-party customer data across 150+ Databricks workspaces. Before Unity Catalog, this data was structured in a decentralized data mesh which had issues including problematic data sharing, minimal data governance, and no auditing of data extracts. In this session, we will discuss how 84.51° incrementally rolled out Unity Catalog company-wide in a way that involved minimal disruption of existing processes and how UC-enabled solutions solved several real-world problems including:\n",
       " • Being able to migrate an on-prem solution to a Databricks SQL Warehouse with HIPAA auditing and data governance which resulted in reduced on-prem costs and enabled new solutions/products to bring significant new revenue sources. \n",
       " • Leveraging Delta Sharing to eliminate duplication of data and processing and enabling new insights to provide a deeper, more personal approach to customer engagement.\n",
       "Databricks has also been key to the 84.51° Collaborative Cloud, which provides our clients' data scientists a platform that provides clean and ready-to-use transaction and UPC-level data from 60 million households empowering companies to get value out of data science, regardless of where they sit on the readiness spectrum. Unity Catalog will be key to extend the collaborative cloud functionality and cleanroom technology with improved security and governance.</td></tr><tr><td>Lakehouse as a FAIR Platform</td><td>FAIR (findable, accessible, interoperable, reusable) data and data platforms are becoming more and more important in public sector. Lakehouse platform is strongly aligned with these principles. Lakehouse provides tools required to both adhere to FAIR but also to FAIRify data that isn't FAIR compliant. In this session we will cover parts of the lakehouse that enable end users to FAIRify data products, how to build good robust data products and which parts of Lakehouse align to which principles in FAIR. We'll demonstrate how DLT is crucial for data transformations on nonFAIR data, how Unity Catalog unlocks discoverability (F) and governed data access (A), and how marketplace, cleanrooms and Delta Sharing unlock interoperability and data exchange (I and R). All of these concepts are massive enablers for highly regulated industries such as Public Sector. It undeniably important to align Lakehouse to standards that are widely adopted by standards and policy makers and regulators. These principles transcend all industries and all use cases.</td></tr><tr><td>Engineers Shouldn't Write Data Governance Policies</td><td>Controlling permissions for accessing data assets can be messy, time consuming, and usually a combination of both. The teams responsible for creating the business rules that govern who should have access to what data are usually different from the teams responsible for administering the grants to achieve that access. On the other side of the equation, the end user who needs access to a data asset may be left waiting for grants to be made as the decision is passed between teams. That is, if they even know the correct path to getting access in the first place.\n",
       "  \n",
       " Separating the concerns of managing data governance at a business level and implementing data governance at an engineering level is the best way to clarify data access permissions. In practice, this involves building systems to enable data governance enforcement based on business rules, with little to no understanding of the individual system where the data lives. \n",
       " \n",
       " In practice, with a concrete business rule, such as “only users from the finance team should have access to critical financial data,” we want a system that deals only with those constituent concepts. For example, “the data is marked as critical financial” and “the user is a part of the finance team”). By abstracting away any source system components, such as “the tables in the finance schema” and “someone who’s a member of the finance databricks group,” the access policies applied will then model the business rules as closely as possible.\n",
       " \n",
       " This talk will focus on how to establish and align the processes, policies, and stakeholders involved in making this type of system work seamlessly. Sharing the experience and learnings of our team at Instacart, we will aim to help attendees streamline and simplify their data security and access strategies.</td></tr><tr><td>Managing Data Encryption in Apache Spark™</td><td>Sensitive data sets can be encrypted directly by new Apache Spark™  versions (3.2 and higher). Setting a number of configuration parameters and dataframe options will trigger the Apache Parquet modular encryption mechanism that protects select columns with column-specific keys. The upcoming Spark 3.4 version will also support uniform encryption, where all dataframe columns are encrypted with the same key.\n",
       " \n",
       " Spark data encryption is already leveraged by a number of companies to protect personal or business confidential data in their production environments. The main integration effort is focused on key access control and on building a Spark/Parquet plug-in code that can interact with company’s key management service (KMS).\n",
       " \n",
       " In this talk, we will briefly cover the basics of Spark/Parquet encryption usage, and dive into the details of encryption key management that will help in integrating this Spark data protection mechanism in your deployment. You will learn how to run a HelloWorld encryption sample, and how to extend it into a real world production code integrated with your organization’s KMS and access control policies. We will talk about the standard envelope encryption approach to big data protection, the performance-vs-security trade-offs between single and double envelope wrapping, internal and external key metadata storage. We will see a demo, and discuss the new features such as uniform encryption and two-tier management of encryption keys.</td></tr><tr><td>Enabling Data Governance at Enterprise Scale Using Unity Catalog</td><td>For some time, Amgen has been building multiple enterprise platforms on the latest technology with a key focus on tech rationalization, data democratization, overall user experience, increase reusability and cost-effectiveness. One of these platforms is the enterprise data fabric platform. This platform specifically focuses on pulling in data across functions into a single platform building capabilities around the connectedness of data, and access governance.\n",
       " For a while, we have been trying to setup robust data governance capabilities which are simple, yet easy to manage through Databricks. As part of this process we evaluated products like privacera, Immuta, and Okera. While these tools could solve a few immediate needs like managing tables and database level, for the use cases like maintaining governance on highly restricted data domains like EDW(Finance) and Workday(HR), we felt that for the long term we will require a more Databricks native solution because of below reasons:\n",
       " - There were ways to bypass these tools on databricks cluster\n",
       " - Unsupported DBR runtime\n",
       " - Complexity of fine-grained security\n",
       " - Policy management – AWS IAM + Intool policies\n",
       "  To address these challenges, and for large-scale enterprise adoption of our governance capability, we started working on UC integration with our governance processes. Following tech benefits we realized:\n",
       " - Independent of Databricks runtime\n",
       " - Easy fine-grained access control\n",
       " - Eliminated management of IAM roles\n",
       " - Dynamic access control using UC and dynamic views\n",
       " As a result of these technical benefits, we were able to implement fine-grained and complex governance policies around the restricted data set of Amgen including Finance and Workday. At this point we have around 100K objects mapped in the Unity Catalog and growing rapidly.</td></tr><tr><td>Lineage System Table in Unity Catalog</td><td>Unity Catalog provides fully automated data lineage for all workloads in SQL, R, Python, Scala and across all asset types at Databricks. The aggregated view has been available to end users through data explorer and API. In this session, we are excited to share that lineage is available via delta table in their UC metastore. It stores full history of recent lineage records and it is near real time. Additionally, customers can query it through standard SQL interface. With that, customers can get significant operational insights about their workload for impact analysis, troubleshooting, quality assurance, data discovery, and data governance. Together with the system table platform effort, which provides query history, job run operational data, audit logs and more, lineage table will be a critical piece to link all the data asset and entity asset together. It will provide better Lakehouse observability and unification to customers.</td></tr><tr><td>Combining Privacy Solutions to Solve Data Access at Scale</td><td>The trend that has made data easier to collect and analyze, has only aggravated privacy risks. Luckily, a range of privacy technologies have emerged to enable private data management (differential privacy, synthetic data, confidential computing).  In isolation, those technologies have had a limited impact because they did not always bring the 10x improvement expected by data leaders.\n",
       " \n",
       " Combining these privacy technologies has been the real game changer. We will demonstrate that the right mix of technologies brings the optimal balance of privacy and flexibility at the scale of the data warehouse.  We will illustrate this by real-life applications of Sarus in three domains:\n",
       "Healthcare: how to make hospital data available for research at scale in full compliance\n",
       "Finance: how to pool data between several banks to fight criminal transactions\n",
       "Marketing: how to build insights on combined data from partners and distributors\n",
       " \n",
       " The examples will be illustrated using data stored in Databricks and queried using Sarus differential privacy engine.</td></tr><tr><td>Map Your Lakehouse Content with DiscoverX</td><td>An enterprise lakehouse contains many different datasets which are related to different sources and might belong to different business units. \n",
       " These datasets can span across hundreds of tables, and each table has a different schema, and those schemas evolve over time. The Cyber security domain is a good example where datasets come from many different source systems and land in the lakehouse.\n",
       " With such a complex dataset ecosystem, answers to simple questions like “Have we ever detected this IP address?” or “Which columns contain IP addresses?” can become impractical and expensive.\n",
       " DiscoverX can automate the discovery of all columns that might contain specific patterns, (e.g. IP addresses, MAC addresses, fully qualified domain names, etc.) and automatically generate search and indexing queries that span across multiple tables and columns.</td></tr><tr><td>Post-Merger: Implementing Unity Catalog Across Multiple Accounts</td><td>Warner Media and Discovery have recently merged to form Warner Bros Discovery. Owning two Databricks accounts and wanting to maintain their separation, our data governance team has successfully implemented Unity Catalog as our data governance solution across both accounts, allowing our teams to collaboratively and securely use the data assets of two organizations. This session is aimed at sharing that success story, including initial challenges, our approach, our architecture, the actual implementation, and user success post-implementation.</td></tr><tr><td>Advanced Governance with Collibra on Databricks</td><td>Define security policies in Collibra that are seamlessly enforced on Databricks</td></tr><tr><td>How Nestle is Leveraging Unity Catalog for Governance At Scale</td><td>Governance with Unity Catalog across multiple regions, markets, teams and environments at Nestle. Establish data isolation at all levels</td></tr><tr><td>Wrangling Your Security Data: Cybersecurity as a Data Management Problem</td><td>Today, the average Fortune 500 company manages 100+ cybersecurity vendors. The sheer number of enterprise cybersecurity toolsets, each capturing and generating its own data, creates significant data management and data engineering challenges for all enterprises. Because each of these security tools has separate logs, outputs, and databases, it prevents any security team from meaningfully understanding their overall security posture and using their own data effectively. We believe that cybersecurity is fundamentally a data management problem. In this presentation, we will show how treating cybersecurity as a data management problem, and subsequently opening disparate cybersecurity data to queries can help organizations secure their systems and data more effectively. In this presentation, practitioners will walk away understanding the value of modeled and organized security on a data warehouse. We will leverage a real-world use case that shows how Databricks can help solve vulnerability management and end-to-end workflows for security practitioners.</td></tr><tr><td>Lakehouse Architecture to Advance Security Analytics at the Department of State</td><td>In 2023, the Department of State surged forward on implementing a Lakehouse architecture to get faster, smarter, and more effective on cybersecurity log monitoring and incident response. In addition to getting us ahead of federal mandates, this approach promises to enable advanced analytics and machine learning across our highly federated global IT environment while minimizing costs associated with data retention and aggregation. \n",
       " \n",
       " This session will include a high-level overview of the technical and policy challenge and a technical deeper dive on the tactical implementation choices made. We’ll share lessons learned related to governance and securing organizational support, connecting between multiple cloud environments, and standardizing data to make it useful for analytics. \n",
       " \n",
       " And finally, we’ll discuss how the Lakehouse leverages Databricks in multi-cloud environments to promote decentralized ownership of data while enabling strong, centralized data governance practices.</td></tr><tr><td>Engineering Data Lakehouse Systems for the Next Ten Years of Growth</td><td>For most of my professional life, I've dealt with data. As a data practitioner, I developed algorithms to solve real-world problems leveraging machine learning techniques. As an engineer, I led the direction that brought the value of my hands-on machine learning experience into our products and services by building upon cutting-edge and emerging technologies. This experience has taught me that scaling data systems is harder than you might think. \n",
       "  \n",
       " Supporting the operations of scalable data environments poses a challenge greater than the known application-level support due to the complexity of managing data together with the application. Why is data adding so much complexity? Well, data is big, so all systems are now becoming distributed. Data changes and evolves, and it's hard to create repeatable, automated pipelines. Plus, technology is advancing at an alarming rate and changes are messy.\n",
       "  \n",
       " Interested in learning about all the most recent updates in the data space? How does that impact our data systems? In this talk, you will learn about the challenges of product quality, delivery velocity, production monitoring, and outage recovery, see how those can be met using best practices in the tech stack, and develop empathy for those who manage scalable data environments.</td></tr><tr><td>Optimize Your Delta Lake</td><td>Delta tables in Databricks just work. They're so easy to setup to deliver value in your organization. What about all those features and optimization that you can use to super charge your delta lakehouse? \n",
       " \n",
       " In this lighting talk we will discuss structuring, partitioning, optimizing, delta log retention of your Delta Tables all in 15 minutes or less!</td></tr><tr><td>Lakehouses for Data Engineers: What You Need to Consider to Build Efficient ETL Pipelines</td><td>ETL pipelines are ubiquitous in data-warehousing and Lakehouse ecosystem to make business decisions by building raw and derived tables from a plethora of fragmented data. As companies are investing in building data applications for richer user experiences, ETL pipelines need to solve some of the modern-day challenges of helping deliver low-latency analytics. \n",
       " \n",
       " In this lightning talk, we’ll focus on how to build efficient Apache Spark™  ETL pipelines for Lakehouses where you can effectively ingest and utilize streaming data. Will cover details on:\n",
       " - leveraging incrementally processing framework to improve compute efficiency\n",
       " - index data to deliver low-latency analytics\n",
       " - advanced concurrency control mechanisms to improve throughput</td></tr><tr><td>Bringing Data in-House! Behind the New York Jets' Winning Game Plan to Create a Data Lakehouse on Databricks</td><td>On the field, the New York Jets are one of the most recognizable brands in professional sports. Off the field, they’re a small business within a finite market with lots of competition. In order to fuel revenue growth, improve operational efficiencies, and drive fan engagement, the Jets brought its data infrastructure in-house and moved to Databricks. \n",
       " \n",
       " This 40-minute presentation will include a discussion of the New York Jets data journey including its overall business and technical goals, the organization's multiple attempts at building a data stack, what drove the shift to Databricks, its current architectural approach, and the benefits of leveraging Databricks as its cloud data platform. \n",
       " \n",
       " Benefits to data practitioners:\n",
       "  - Learn about the New York Jets data journey and the similarities to other organizations’ data journeys including challenges, build vs. buy, and the decision \n",
       " making processes required to implement a modern data stack\n",
       "  - Discussion around the Data Lakehouse architectural pattern and the steps required to implement this on Databricks\n",
       "  - Identification of the benefits of a customer 360 analysis and process and technologies required to create this record \n",
       "  - Understand the benefits of leveraging Databricks as a cloud data platform \n",
       "  - Highlight future opportunities to leverage Databricks for ML / AI use cases to improve upon current baseline analytics</td></tr><tr><td>Databricks-As-Code: How to Effectively Automate a Secure Lakehouse Using Terraform for Resource Provisioning</td><td>At Rivian, we have automated >95% of our Databricks resource provisioning workflows using an in-house Terraform module, affording us a lean admin team to manage 750+ users. In this talk, we will cover the following elements of our approach and how others can benefit from improved team efficiency.\n",
       " \n",
       " User and service principal management\n",
       " Our permission model on Unity Catalog for data governance\n",
       " Workspace and secrets resource management\n",
       " Managing internal package dependencies using init scripts\n",
       " Facilitating dashboards, SQL queries and their associated permissions\n",
       " Scaling source of truth Petabyte scale Delta Lake table ingestion jobs and workflows</td></tr><tr><td>Real-Time Reporting and Analytics for Construction Data Powered by Delta Lake and DBSQL</td><td>Procore is a construction project management software that helps construction professionals efficiently manage their projects and collaborate with their teams. Our mission is to connect everyone in construction on a global platform. \n",
       " \n",
       " Procore is the system of record for all construction projects. Our customers need to access the data in near real-time for construction insights. Enhanced reporting is a self-service operational reporting module that allows quick data access with consistency to thousands of tables and reports.\n",
       " \n",
       " Procore Data platform rebuilt the module (originally built on the relational database) using Databricks and Delta lake. We used Apache Spark™  streaming to maintain the consistent state on the ingestion side from Kafka and plan to leverage the fully capable functionalities of DBSQL using the serverless SQL warehouse to read the medallion models (built via DBT) in Delta Lake. In addition, the Unity Catalog and the Delta share features helped us share the data across regions seamlessly. This design enabled us to improve the p95 and p99 read time by x% (which were initially timing out).\n",
       " \n",
       "  To learn about the learnings and experience of building a Data Lakehouse architecture, attend this talk.</td></tr><tr><td>Made in Italy: How Barilla Uses Databricks Lakehouse to Optimize its Operations</td><td>Barilla Data2Value digital transformation program democratized data usage. The Databricks centered platform provides BI for a supply chain extended in 100 countries in the world, across 20 factories, >50 logistic hubs, 1000 suppliers, >10.000 B2B clients and >100.000 transports every year. By using metadata-driven and event-driven approaches, Barilla is able to meet the different needs of its 1000 data consumers from top management to operation management with about 50 use cases, consuming TB of data daily and execute more than 150 data pipelines. During the presentation, Barilla digital transformation strategy will be discussed, the platform architecture, the expected evolution as well as the lessons learned along the way with EY. Will be explained how Barilla has reduced the time to insight using Unity Catalog and MLFlow. Mariama Kamanda, a data scientist from the Barilla Acceleration Team, will provide testimony on the impact of Databricks Lakehouse daily journey. Will be explained how Barilla saved M$ every year by improving the factory losses through advanced analytics and machine learning leveraging Databricks.</td></tr><tr><td>Using Lakehouse to Fight Cancer: Ontada’s Journey to Establish a RWD Platform on Databricks Lakehouse</td><td>Ontada, a McKesson business, is an oncology real-world data and evidence, clinical education and provider of technology business dedicated to transforming the fight against cancer. Core to Ontada’s mission is using real-world data (RWD) and evidence generation to improve patient health outcomes and to accelerate life science research. \n",
       " \n",
       " To support its mission, Ontada embarked on a journey to migrate its enterprise data warehouse (EDW) from an on-premise Oracle database to Databricks Lakehouse. This move allows Ontada to now consume data from any source, including structured and unstructured data from its own EHR and genomics lab results, and realize faster time to insight. In addition, using the Lakehouse has helped Ontada eliminate data silos, enabling the organization to realize the full potential of RWD – from running traditional descriptive analytics to extracting biomarkers from unstructured data. \n",
       " \n",
       " The presentation will cover the following topics: \n",
       " - Oracle to Databricks: migration best practices and lessons learned \n",
       " - People, process, and tools: expediting innovation while protecting patient information using Unity Catalog \n",
       " - Getting the most out of the Databricks Lakehouse: from BI to genomics, running all analytics under one platform \n",
       " - Hyperscale biomarker abstraction: reducing the manual effort needed to extract biomarkers from large unstructured data (medical notes, scanned/faxed documents) using spaCY and John Snow Lab NLP libraries \n",
       " \n",
       " Join this session to hear how Ontada is transforming RWD to deliver safe and effective cancer treatment.</td></tr><tr><td>CPG Lakehouse Analytics: Rapidly Implementing Walmart’s Luminate API at the Hershey Company</td><td>Accurate, reliable, and timely data is critical for CPG companies to stay ahead in highly competitive retailer relationships, and for a company like the Hershey Company, the commercial relationship with Walmart is one of the most important. The team at Hershey found themselves with a looming deadline for their legacy analytics services and targeted a migration to the brand new Walmart Luminate API.\n",
       "  \n",
       " Working in partnership with Advancing Analytics, the Hershey Company leveraged a metadata-driven Lakehouse architecture to rapidly onboard the new Luminate API, helping the category management teams to overhaul how they measure, predict, and plan their business operations.\n",
       "  \n",
       " In this session, we will discuss the impact Luminate has had on Hershey's business covering key areas such as sales, supply chain, and retail field execution, and the technical building blocks that can be used to rapidly provision business users with the data they need, when they need it. We will discuss how key technologies enable this rapid approach, with Databricks Autoloader ingesting and shaping our data, Delta Streaming processing the data through the Lakehouse and Databricks SQL providing a responsive serving layer.\n",
       "  \n",
       " The session will be jointly run by Hersheys and Advancing Analytics, with Jordan Donmoyer, manager of Hershey's direct data team and Dan Davies, director of Walmart sales analytics providing the commercial commentary, and Simon Whiteley and Zach Stagers from Advancing Analytics covering the technical journey.</td></tr><tr><td>Building a Minimalistic Open Lakehouse Using Open Source Projects Apache Spark™, Project Nessie and Iceberg</td><td>A Lakehouse architecture is a combination of various components such as Storage, File format, Table format, and Catalog. What truly makes a lakehouse 'open' is data being stored in open-source table & file formats like Iceberg, Delta & Parquet respectively and the technology being open-sourced for easy & quick adoption by the community. Like any new technology, implementation of a lakehouse may seem daunting at first. However, when we break down the architecture to its open components, this becomes easy to adopt & scale.\n",
       " \n",
       " Though this session, the idea is to help data engineers getting their leg into the world of data lakehouses, easily learn & implement it. We will go through a Notebook-style presentation to show beginners how to build a minimalistic functional lakehouse using Apache Spark, Project Nessie & Iceberg.\n",
       " \n",
       " We will talk about:\n",
       " - configuring the 3 different components\n",
       " - creating tables from raw data files\n",
       " - ingesting new data from various sources into the tables, querying it and making updates\n",
       " - time travel, compaction, etc capabilities</td></tr><tr><td>Labcorp Data Platform Journey: From Selection to Go-Live in six months</td><td>Please join with us to learn about the Labcorp data platform transformation from on-premises Hadoop to AWS Databricks Lakehouse. We will share best practices and lessons learned from cloud-native data platform selection, implementation, and migration from Hadoop (with in 6 months) with Unity Catalog. We will share steps taken to retire several legacy on-premises technologies and leverage Databricks native features like Spark streaming, workflows, job pools, cluster policies and Spark JDBC within Databricks platform. Lessons learned in Implementing Unity Catalog and building a security and governance model that scales across applications. Demos and walkthrough’s of batch frameworks, streaming frameworks, data compare tools used across several applications to improve data quality and speed of delivery. Discover how we have improved operational efficiency, resiliency and reduced TCO, and how we scaled building workspaces and associated cloud infrastructure using Terraform provider.</td></tr><tr><td>Deep Dive Into Grammarly's Data Platform</td><td>Grammarly helps 30 million people and 50,000 teams to communicate more effectively. Using the Databricks Lakehouse Platform, we are able to rapidly ingest, transform, aggregate, and query complex data sets from an ecosystem of sources, all governed by Unity Catalog. This presentation overviews Grammarly’s data platform and the decisions that shaped the implementation. We will dive deep into some architectural challenges the Grammarly Data Platform team overcame as we developed a self-service framework for incremental event processing.\n",
       " \n",
       " Our investment in the Lakehouse and Unity Catalog has dramatically improved the speed of our data value chain: making 5 billion events (ingested, aggregated, de-identified, and governed) available to stakeholders (data scientists, business analysts, sales, marketing) and downstream services (feature store, reporting/dashboards, customer support, operations) available within 15 minutes. As a result, we have improved our query cost performance (110% faster at 10% the cost) compared to our legacy system on AWS EMR.\n",
       " \n",
       " I will share architecture diagrams, their implications at scale, code samples, and problems solved and to be solved in a technology-focused discussion about Grammarly’s iterative Lakehouse Data Platform.</td></tr><tr><td>Simplifying Migrations to Lakehouse</td><td>Following on from Perenti Global's successful presentation at the ANZ (Syd) Data and AI summit in Nov 2022, they were asked by Bede Hackney if they would be interested in supporting Databricks events on a bigger stage, ramping up their reference-ability etc... they are confirmed as able to support us at this event and are excited to do so. \n",
       " Link to client presentation noting tweaks to be made upon confirmation of attendance --> https://docs.google.com/presentation/d/1OHMsyFhV-gq6lcVvXqkp-0LY8wS6C6TS/edit#slide=id.p1\n",
       " \n",
       " \n",
       " High level Synopsis:\n",
       " - Challenges with Legacy Platforms \n",
       " - Perenti Databricks Migration Journey\n",
       " - Reimagining Migrations the Databricks Way\n",
       " - The Databricks Migration Methodology & Approach</td></tr><tr><td>Having Your Cake and Eating it Too: How Vizio Built a Next-Generation ACR Data Platform While Lowering TCO</td><td>As the top manufacturer of smart TVs, Vizio uses TV data to drive its business and provide customers with best digital experiences. Our company's mission is to continually improve the viewing experience for our customers, which is why we developed our award-winning automatic content recognition (ACR) platform. When we first built our data platform almost ten years ago, there was no single platform to run a data as a service business, so we got creative and built our own by stitching together different AWS services and a data warehouse. As our business needs and data volumes have grown exponentially over the years, we made the strategic decision to replatform on Databricks Lakehouse, as it was the only platform that could satisfy all our needs out-of-the-box such as BI analytics, real-time streaming, and AI/ML. Now the Lakehouse is our sole source of truth for all analytics and machine learning projects. The technical value of the Databricks Lakehouse platform, such as traditional data warehousing low-latency query processing with complex joins thanks to Photon to using Apache Spark™  structured streaming; analytics and model serving, will be covered in this session as we talk about our path to the Lakehouse.</td></tr><tr><td>From Insights to Recommendations: How SkyWatch Predicts Demand for Satellite Imagery Using Databricks</td><td>SkyWatch is on a mission to democratize earth observation data and make it simple for anyone to use.\n",
       " \n",
       " In this session, you will learn about how SkyWatch aggregates demand signals for EO market and turns them into monetizable recommendations for satellite operators.\n",
       " Skywatch’s head of data engineering, Vincent, and data engineer, Aayush, will share how the team built a serverless architecture that synthesizes customer requests for satellite images and identifies geographic locations with high demand, helping satellite operators maximize revenue and satisfying a broad range of EO data hungry consumers.\n",
       " \n",
       " This session will cover the usage of\n",
       " \n",
       " - Databricks in-built H3 functions \n",
       " - Open Source Mosaic library to process geospatial data\n",
       " - Delta Lake to efficiently store data leveraging optimization techniques like Z-Ordering\n",
       " - Databricks Serverless SQL endpoint to build REST API with AWS Lambda and step functions.\n",
       " - Databricks with AWS serverless architecture (Lambda and step functions)</td></tr><tr><td>Determining When to Use GPU for Your ETL Pipelines at Scale</td><td>Assuming you have hundreds of jobs and/or clusters in your Databricks workspace, what is the best way to determine if those pipelines can take advantage of GPU for speed and/or cost saving? We are presenting NVIDIA GPU Qualification Tool applied at scale to project potential cost saving for your entire workspace.</td></tr><tr><td>Massive Data Processing in Adobe Using Delta Lake:  A Year In</td><td>At Adobe Experience Platform, we ingest TBs of data everyday and manage PBs of data for our customers as part of the unified profile offering. At the heart of this is a bunch of complex ingestion of a mix of normalized and denormalized data with various linkage scenarios power by a central identity linking graph. This helps power various marketing scenarios that are activated in multiple platforms and channels like email, advertisements etc. We will go over how we built a cost effective and scalable data pipeline using Apache Spark™  and Delta Lake and share our experiences after one year in production.\n",
       " \n",
       " - What are we storing?\n",
       " - Multi-source, multi-channel problem\n",
       " - Access pattern to optimize for\n",
       " - Custom high performance query engine\n",
       " - Data representation and nested schema evolution\n",
       " - Performance trade-offs with various formats\n",
       " - Go over anti-patterns used \n",
       " - String FTW\n",
       " - Data manipulation using UDFs \n",
       " - Writer worries and how to wipe them away\n",
       "  - Locking for contention mechanism\n",
       " - Gotchas\n",
       " - Concurrency\n",
       " - Column size\n",
       " - Update frequency\n",
       " - Transaction management for a healthy state\n",
       " - Staging tables FTW \n",
       " - Why we can't live without them\n",
       " - Datalake replication lag tracking\n",
       " - Instrumentation of the data pipeline gives more confidence to the reade\n",
       " - Maintenance jobs\n",
       " - Go over essentials of compaction and vacuuming\n",
       " - Performance time!\n",
       " - What scale are we operating at?\n",
       " - Settings like autoCompact and optimizeWrite\n",
       " - Timings with and without delta\n",
       " - Cost \n",
       "  - Lesson learned and burnt</td></tr><tr><td>DHL eCommerce US – Building a scalable and robust Cloud Data Platform as enabler for Enterprise Analytics</td><td>DHL eCommerce Solutions Americas is a high volume B2C domestic and international parcel delivery business and that is on mission to become a data-driven organization to by democratizing the use of data and to maximizing the business value.\n",
       "  \n",
       " Learn how the DHL eCommerce Solutions Americas data team built a centralized Enterprise grade scalable cloud data platform processing 100s of millions of events daily. This single source of truth powering all analytics use cases helps DHL ecommerce US team to deliver business insights 5x faster.\n",
       " We will dive into how we solved challenging key requirements including secure multi-tenant central data platform, scalable data ingestion and processing, cost effective big data repository, fast and regular data refreshes, diverse analytics workload needs.\n",
       " \n",
       " Finally we will share some example usecases operated on this platform and the business value those usecases are generating along various lines of the business including operations, commercial, customer service, sales, product management, etc.</td></tr><tr><td>Learnings From the Field: Migration From Oracle DW and IBM DataStage to Databricks on AWS</td><td>Legacy data warehouses are costly to maintain, unscalable and cannot deliver on data science, ML and real-time analytics use cases. Migrating from your enterprise data warehouse to Databricks lets you scale as your business needs grow and accelerate innovation by running all your data, analytics and AI workloads on a single unified data platform.\n",
       " \n",
       " In the first part of this talk we will guide you through the well-designed process and tools that will help you from the assessment phase to the actual implementation of an EDW migration project. Also, we will address ways to convert PL/SQL proprietary code to an open standard python code and take advantage of PySpark for ETL workloads and Databricks SQL’s data analytics workload power.\n",
       " \n",
       " The second part of this talk will be based on an EDW migration project of SNCF (French national railways); one of the major enterprise customers of Databricks in France. Databricks partnered with SNCF to migrate its real estate entity from Oracle DW and IBM DataStage to Databricks on AWS. We will walk you through the customer context, urgency to migration, challenges, target architecture, nitty-gritty details of implementation, best practices, recommendations, and learnings in order to execute a successful migration project in a very accelerated time frame.</td></tr><tr><td>Why a Major Japanese Financial Institution Chose Databricks to Accelerate its Data and AI-Driven Journey</td><td>In this session, we will introduce a case study of migrating the Japanese largest data analysis platform to Databricks.\n",
       " \n",
       "NTT DATA is one of the largest system integrators in Japan. In the Japanese market, many companies are working on BI, and we are now in the phase of using AI. Our team provides solutions that provide data analytics infrastructure to drive the democratization of data and AI for leading Japanese companies.\n",
       " \n",
       " The customer in this case study is the largest insurance company in Japan. This project has the following characteristics:\n",
       "  - As a financial institution, security requirements are very strict.\n",
       "  - Since it is used company-wide, including group companies, it is necessary to support various use cases.\n",
       " \n",
       " We started operating a data analysis platform on AWS in 2017. Over the next five years, we leveraged AWS-managed services such as Amazon EMR, Amazon Athena, and Amazon Sage Maker to modernize our architecture.\n",
       " \n",
       " In the near future, in order to promote the use cases of AI as well as BI, we have begun to consider upgrading to a platform that realizes both BI and AI.\n",
       " \n",
       " This session will cover:\n",
       "  - Challenges in developing AI on a DWH-based data analysis platform and why a data lake house is the best choice\n",
       "  - Examining the architecture of a platform that supports both AI and BI use cases. In this case study, we will introduce the results of a comparative study of a proposal based on Databricks, a proposal based on Snowflake, and a proposal combining Snowflake and Databricks.\n",
       " \n",
       " This session is recommended for those who want to accelerate their business by utilizing AI as well as BI.\n",
       " \n",
       " \n",
       "</td></tr><tr><td>Eliminating Shuffles in Delete Update, and Merge</td><td>If you’ve ever had to delete a set of records for regulatory compliance, update a set of records to fix an issue in the ingestion pipeline, or apply changes in a transaction log to a fact table, you know that row-level operations are becoming critical for modern data lake workflows. Even though the industry has seen a tremendous amount of innovation in this area, row-level operations can be still fairly expensive if the underlying data has to be shuffled. This talk will explain how Apache Spark can completely avoid shuffles during row-level operations by leveraging storage-partitioned joins, a key to efficiently modify data at PB scale.</td></tr><tr><td>When Self-Service Meets Earnings Calls: A Journey Towards FinOps</td><td>At Disney Streaming, we extensively use Databricks and the delta lake as an operational and analytical tool. But as we shift focus from growth to profit, our data platform needed to pivot from self-service and ease use and towards automatic tracking, auditing, and continuous efficiency chasing. \n",
       " \n",
       " In this talk, I'll explain our FinOps journey; how we democratized cost data, increased transparency, tightened policies, and got dozens of analytical and engineering teams to fit cost savings into their roadmap.\n",
       "</td></tr><tr><td>Delta Lake Migration At Scale</td><td>Adobe Experience Platform (AEP) serves a large number Adobe digital experience customers with tens of thousands of datasets in non-Delta format. To leverage Delta Lake, we migrate existing tables to Delta format after resolving the following challenges: 1). Short downtime to minimize customer impact; 2). Adding new system metadata; 3). Migration automation for scalability; and 4). Error detection and catastrophic rollback. This talk will cover the solutions to the above challenges and lessons learned from migration. Finally, scalable migration service becomes a core capability of the data platform.</td></tr><tr><td>Databricks Cost Management</td><td>With Databricks you only pay for the compute resources you use. With growing usage it is advisable to have a cost management strategy to avoid surprises at the end of the month. It is also useful to monitor and break down costs for budgeting purposes or to calculate the return on investment (ROI) for specific projects.\n",
       " \n",
       " In this talk I will:\n",
       " - Describe the Databricks pricing model\n",
       " - Show how to analyze and break down costs\n",
       " - Discuss best practises for cost optimization</td></tr><tr><td>Data Asset Bundles: A Standard, Unified Approach to Deploying Data Products on Databricks</td><td>In this talk we will introduce data asset bundles, provide a demonstration of how they work for a variety of data products, and how to fit them into an overall CICD strategy for the well-architected Lakehouse.\n",
       " \n",
       " Data teams produce a variety of assets; datasets, reports and dashboards, ML models, and business applications. These assets depend upon code (notebooks, repos, queries, pipelines), infrastructure (clusters, SQL warehouses, serverless endpoints), and supporting services/resources like Unity Catalog, Databricks Workflows, and DBSQL dashboards. Today, each organization must figure out a deployment strategy for the variety of data products they build on Databricks as there is no consistent way to describe the infrastructure and services associated with project code.\n",
       " \n",
       " Data asset bundles is a new capability on Databricks that standardizes and unifies the deployment strategy for all data products developed on the platform. It allows developers to describe the infrastructure and resources of their project through a YAML or JSON configuration file, regardless of whether they are producing a report, dashboard, online ML model, or Delta Live Tables pipeline. Behind the scenes, these configuration files use Terraform to manage resources in a Databricks workspace, but knowledge of Terraform is not required to use Data Asset Bundles.</td></tr><tr><td>Lakehouses: The Best Start to Your Graph Data and Analytics Journey</td><td>Data architects and IT executives are continually looking for the best ways to integrate graph data and analytics into their organizations to improve business outcomes. This presentation outlines how the Data Lakehouse provides the perfect starting point for a successful journey. We will explore how the Data Lakehouse offer the unique combination of scalability, flexibility, and speed to quickly and effectively ingest, pre-process, curate, and analyze graph data to create powerful analytics. Additionally, this talk will discuss the benefits of using the Data Lakehouse over traditional graph databases and how it can help improve time to insight, time to production and overall satisfaction. \n",
       " \n",
       "At the end of this presentation, attendees will: \n",
       " - Understand the benefits of using a Data Lakehouse for graph data and analytics \n",
       " - Learn how to get started with a successful Lakehouse implementation (demo)\n",
       " - Discover the advantages of using a Data Lakehouse over graph databases\n",
       " - Learn specifically where graph databases integrate and perform better together\n",
       " \n",
       " Key Takeaways: \n",
       " - Data Lakehouses provide the perfect starting point for a successful graph data and analytics journey \n",
       " - Data Lakehouses offer scalability, flexibility, and speed to quickly and effectively analyze graph data \n",
       " - The Data Lakehouse is a cost-effective alternative to traditional graph database shortening your time to insight and de-risk your project.\n",
       " \n",
       " Presentation Components:\n",
       " The customer journey\n",
       " Access patterns\n",
       " Reference architecture (logical, physical)\n",
       " Customer stories\n",
       " Demo (notebook + live graph visualization)</td></tr><tr><td>Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse</td><td>As you embark on a lakehouse project or evolve your existing data lake, you may want to improve your security posture and take advantage of new security features—there may even be a security team at your company that demands it! Databricks has worked with thousands of customers to securely deploy the Databricks Platform to meet their architecture and security requirements. While many organizations deploy security differently, we have found a common set of guidelines and features among organizations that require a high level of security. In this session, we will detail the security features and architectural choices frequently used by these organizations and walk through a series of threat models for the risks that most concern security teams. While this session is great for people who already know Databricks—don’t worry—that knowledge isn’t required. \n",
       "You will walk away with a full handbook detailing all of the concepts, configurations, check lists, security analysis tool (SAT), and security reference architecture (SRA) automation scripts  from the session so that you can make immediate progress when you get back to the office. \\\n",
       "Security can be hard, but we’ve collected the hard work already done by some of the best in the industry, and built tools, to make it easier. Come learn how. See how good looks like via a demo.\n",
       "</td></tr><tr><td>Data Globalization at Conde Nast Using Delta Sharing</td><td>Databricks has been an essential part of the Conde Nast architecture for the last few years.\n",
       " \n",
       " Prior to building our centralized data platform, “evergreen”, we had similar challenges as many other organizations; siloed data, duplicated efforts for engineers, and a lack of collaboration between data teams. These problems led to mistrust in data sets and made it difficult to scale to meet the strategic globalization plan we had for Conde Nast.\n",
       " \n",
       " Over the last few years we have been extremely successful in building a centralized data platform on Databricks in AWS, fully embracing the lakehouse vision from end-to-end. Now, our analysts and marketers can derive the same insights from one dataset and data scientists can use the same datasets for use cases such as personalization, subscriber propensity models, churn models and on-site recommendations for our iconic brands.\n",
       " \n",
       " In this talk, we’ll discuss how we plan to incorporate Unity Catalog and Delta Sharing as the next phase of our globalization mission. The evergreen platform has become the global standard for data processing and analytics at Conde. In order to manage the worldwide data and comply with GDPR requirements, we need to make sure data is processed in the appropriate region and PII data is handled appropriately. At the same time, we need to have a global view of the data to allow us to make business decisions at the global level. We’ll talk about how delta sharing allows us a simple, secure way to share de-identified datasets across regions in order to make these strategic business decisions, while complying with security requirements. Additionally, we’ll discuss how Unity Catalog allows us to secure, govern and audit these datasets in an easy and scalable manner.</td></tr><tr><td>Unlocking the Value of Data Sharing in Financial Services With Lakehouse</td><td>The emergence of secure data sharing is already having a tremendous economic impact, in large part due to the increasing ease and safety of sharing financial data. McKinsey predicts that the impact of open financial data will be 1-4.5% of GDP globally by 2030. This indicates there is a narrowing window on a massive opportunity for financial institutions and it is critical that they prioritize data sharing. This session will first address the ways in which Delta Sharing and Unity Catalog on a Databricks Lakehouse architecture provides a simple and open framework for building a Secure Data Sharing platform in the financial services industry. Next we will use a Databricks environment to walk through different use cases for open banking data and secure data sharing, demonstrating how they will be implemented using Delta Sharing, Unity Catalog, and other parts of the Lakehouse platform. The use cases will include examples of new product features such as Databricks to Databricks sharing, change data feed and streaming on Delta Sharing, table/column lineage, and the Delta Sharing Excel plugin to demonstrate state of the art sharing capabilities.\n",
       " Spencer Cook, a Sr. Solutions Architect at Databricks focussed on the Financial Services industry will discuss secure data sharing on Databricks Lakehouse and will demonstrate architecture and code for common sharing use cases in the finance industry.</td></tr><tr><td>Extending Lakehouse Architecture with Collaborative Identity</td><td>Lakehouse architecture has become a valuable solution for unifying data processing for AI, but faces limitations in maximizing data’s full potential. Additional data infrastructure is helpful for strengthening data consolidation and data connectivity with third-party sources, which are necessary for building full data sets for accurate audience modeling.\n",
       "\n",
       "In this session, LiveRamp will demonstrate to data and analytics decision-makers how to build on the Lakehouse architecture with extensions for collaborative identity graph construction, including how to simplify and improve data enrichment, data activation and data collaboration. LiveRamp will also introduce a complete data marketplace, which enables easy, pseudonymized data enhancements that widen the attribute set for better behavioral model construction.\n",
       "\n",
       "With these techniques and technologies, enterprises across financial services, retail, media, travel and more can safely unlock partner insights and ultimately produce more accurate inputs for personalization engines, and more engaging offers and recommendations for customers.\n",
       "</td></tr><tr><td>Writing Data-Sharing Apps Using Node.js and Delta Sharing</td><td>Javascript remains the top programming language today, with most code repositories written using JavaScript on GitHub. However, JavaScript is evolving beyond just a language for web application development into a language built for tomorrow. Everyday tasks like data wrangling, data analysis, and predictive analytics are possible today directly from a web browser. For example, many popular data analytics libraries, like Tensorflow.js, now support JavaScript SDKs. Another popular library, Danfo.js, makes it possible to wrangle data using familiar Pandas-like operations, shortening the learning curve and arming the typical data engineer or data scientist with another data tool in their toolbox. In this presentation, we’ll explore using the Node.js connector for Delta Sharing to build a data analytics app that summarizes a Twitter dataset.</td></tr><tr><td>Creating the First Sports and Entertainment Data Marketplace Powered by Pumpjack Dataworks, Revelate, Immuta and Databricks</td><td>Creating a secure and easily actionable marketplace is no simple task. Add to this governance requirements of privacy frameworks and responsibilities of protecting consumer data, and things get harder. With Pumpjack Dataworks partnering with Databricks, Immuta, and Revelate, we bring secure, privacy-focused data products directly to data consumers. With Delta Share, the service solves redundancy of data purchases, time to action, and lowers cost of data acquisition while providing new revenue streams for rights holders. Its a win-win solution for bringing Data Acquisition and Commercial teams together into the new age of data.</td></tr><tr><td>Data Interoperability Across Clouds and Regions</td><td>L’Oréal has the largest and richest data repository in the beauty industry, with a 110-year heritage solely dedicated to analyzing, measuring and magnifying the beauty needs and desires of consumers around the world. As L’Oréal’s ambition is to become the leader in beauty tech, a profound IT transformation started three years ago to meet their most strategic priorities: \n",
       " - Hyper personalization\n",
       " - Consumer experience online and offline \n",
       " - Preventive care\n",
       " - Integrating artificial intelligence into the business\n",
       " \n",
       " This IT transformation now needs to reach all of their services and teams worldwide with the same level of security, quality and exacting standards. To do so, data must be unlocked and democratized across the world in order to meet their strategic priorities. This is why L’Oréal and Databricks worked together on the data interoperability value proposition in order to :\n",
       " - Facilitate multi-cloud interoperability exchanges thanks to open standards technologies\n",
       " - Secure and trace through a centralized governance tool to respond to the growing regulatory environment and the application of other data laws (such as in Vietnam)\n",
       " - Optimize and rationalize data exchanges to reduce transported volumes and meet L'Oréal's sustainable development objectives\n",
       " - Ensure the quality of the data exchanged through quality and validation checks</td></tr><tr><td>Data Extraction and Sharing Via the Delta Sharing Protocol: Overfetching, Underfetching and Other Lessons and Tips for Development Learned While Building the Delta Sharing Excel Add-in</td><td>The Delta Sharing open protocol for secure sharing and distribution of Lakehouse data is designed to reduce friction in getting data to users. Delivering custom data solutions from this protocol further leverages the technical investment committed to your Delta Lake infrastructure.\n",
       " \n",
       " There are key design and computational concepts unique to Delta Sharing to know when undertaking development. And there are pitfalls and hazards to avoid when delivering modern cloud data to traditional data platforms and users.\n",
       " \n",
       " In this session we introduce Delta Sharing Protocol development and examine our journey and the lessons learned while creating the Delta Sharing Excel Add-in. We will demonstrate scenarios of overfetching, underfetching, and interpretation of types. We will suggest methods to overcome these development challenges.\n",
       " \n",
       " The session will combine live demonstrations that exercise the Delta Sharing REST protocol with detailed analysis of the responses. The demonstrations will elaborate on optional capabilities of the protocol’s query mechanism, and how they are used and interpreted in real-life scenarios.\n",
       " \n",
       " As a reference baseline for data professionals, the Delta Sharing exercises will be framed relative to SQL counterparts. Specific attention will be paid to how they differ, and how Delta Sharing’s Change Data Feed (CDF) can power next-generation data architectures.\n",
       " \n",
       " The session will conclude with a survey of available integration solutions for getting the most out of your Delta Sharing environment, including frameworks, connectors, and managed services.\n",
       " \n",
       " Attendees are encouraged to be familiar with REST, JSON, and modern programming concepts. A working knowledge of Delta Lake, the Parquet file format, and the Delta Sharing Protocol are advised.</td></tr><tr><td>Data sharing & beyond with Delta Sharing</td><td>Stepping into this brave new digital world we are certain that data will be a central product for many organizations. The way to convey their knowledge and their assets will be through data and analytics. Delta Sharing was the world's first open protocol for secure and scalable real-time data sharing. Through our customer conversations, there is a lot of anticipation of how Delta Sharing can be extended to non-tabular assets, such as machine learning experiments and models.\n",
       " \n",
       " In this talk, we will cover how we extended the Delta Sharing protocol to other sharing workflows, enabling sharing of ML models, arbitrary files and more. The development resulted in Arcuate, a Databricks Labs project with a data sharing flavour. The talk will start with the high-level approach and how it can be extended to cover other similar use cases. It will then move to our implementation and how it integrates seamlessly with Databricks-managed Delta Sharing server and notebooks. We finally conclude with lessons learned, and our visions for a future of data sharing & beyond</td></tr><tr><td>Ad Measurement: From Impressions to Attribution</td><td>\"Effectv, the advertising sales division of Comcast, delivers linear and digital advertising to help advertisers reach potential customers. In this talk, Joe Walsh, Director of Attribution & Measurement and Derek Sugden, Ad Measurement Lead, will discuss their team's journey of measuring the impact of advertising campaigns on customer behavior by combining Comcast's household-level exposure data with various types of third-party conversion data sourced externally. They will highlight the challenges of sharing and receiving data securely while preserving customer privacy, and share how they have implemented an internal \"\"clean room\"\" concept to restrict roles and usage of a cluster</td></tr><tr><td>Clean Room Primer: Using Clean Rooms on Databricks to Utilize More and Better Data in your BI, ML, and Beyond</td><td>In this session, Habu’s Chief Product Officer, Matt Karasick and Senior Architect, Anil Puliyeril, will discuss the foundational changes in the ecosystem, the implications of data insights on marketing, analytics, and measurement, and how companies are coming together to collaborate through data clean rooms in new and exciting ways to power mutually beneficial value for their businesses while preserving privacy and governance.\n",
       " \n",
       " We will delve into the concept and key features of clean room technology and how they can be used to access more and better data for business intelligence (BI), machine learning (ML), and other data-driven initiatives. By examining real-world use cases of clean rooms in action, attendees will gain a clear understanding of the benefits they can bring to industries like CPG, retail, and media & entertainment. \n",
       " \n",
       " In addition, we will unpack the advantages of using Databricks as a clean room platform, specifically showcasing how interoperable clean rooms can be leveraged to enhance BI, ML and other compute scenarios. By the end of this talk, you will be equipped with the knowledge and inspiration to explore how clean rooms can unlock new collaboration opportunities that drive better outcomes for your business and improved experiences for consumers.</td></tr><tr><td>Embrace First-Party Data Collaboration to Lower Acquisition Costs with Lookalike Audiences in Media Cleanrooms</td><td>Third-party tracking is dead! Yet customer acquisition costs continue to rise to meet the demands of customer attention. Lookalike audience modeling leveraging first-party customer data has offered organizations a cost-effective means to identifying and engaging new customers at scale. However, as we move into a privacy-centric era, collaboration with first-party data across organizations becomes increasingly challenging. So how can we work together to meet both needs? The answer is data clean rooms for the lakehouse, fueled by first-party customer data to build lookalike audience profiles and drive highly personalized ads and experiences in media and commerce.\n",
       " \n",
       " Using Snowplow's generation of granular first-party customer behavioral data with governance metadata and consent tracking, managed through data clean rooms in the lakehouse, organizations can collaborate securely across their first-party datasets. For example, brands and advertisers can join their first-party onsite data with third-party datasets shared by their agency and advertising partners to boost CTR and ROAS with lookalike audiences.\n",
       " \n",
       "This session you focus on:\n",
       " - Best practices for creating lookalike audiences with first-party customer data to lower acquisition costs\n",
       " - See a live demonstration of how audiences derived through the Databricks Lakehouse can be securely exchanged with customers and partners to foster data-driven innovations creating a customer behavioral profile using Snowplow \n",
       " - Data engineering and preparation of Snowplow Behavioral Data with DBT and Databricks\n",
       " - Development and training of an ML model using PySpark on Databricks to create a model to drive contextual advertising\n",
       " - Implementation of the model for scoring\n",
       " - Engage in Q&A</td></tr><tr><td>null</td><td>S&P merged with IHS Markit unlocking massive market opportunities, but it also created organizational and technology challenges that made it difficult to centralize all their data and make it readily available for analytics and AI. This impacted cross-team collaboration and limited their ability to provide innovative solutions for their clients looking to make smarter, sustainable investment choices. There were infrastructure complexities and they significantly increased data volumes that limited their ability to democratize data and insights. Their legacy data warehouse also struggled with unifying alternative data sources with other data sets. This impacted various use cases across the business including credit risk, ESG, and Marketplace Workbench. The Sustainable1 team has spearheaded getting all their datasets readily available for sharing across all of their internal divisions via Delta Share, and also to share S&P data externally with their customers. </td></tr><tr><td>How to Create and Manage a High-Performance Analytics Team</td><td>Data Science and analytics teams are unique. Large and small corporations want to build and manage analytics teams to convert their data and analytic assets into revenue and competitive advantage, but many are failing before they make their first hire. In this presentation, the audience will learn how to structure, hire, manage and grow an analytics team. Organizational structure, project and program portfolios, neurodiversity, developing talent, and more will be discussed. Questions and discussion will be encouraged and engaged in. The audience will leave with a deeper understanding of how to succeed in turning data and analytics into tangible results.</td></tr><tr><td>What it Takes to Democratize AI and ML in a Large Company: The Importance of User Enablement and Technical Training</td><td>The biggest critical factor to success in a Cloud transformation is people. As such, having a change management process in place to manage the impact of the transformation and user enablement is foundational to any large program. In this session we will dive into how TD bank democratizes data, mobilizes our 2000+ analytics user community and the tactics we used to successfully enable new use cases on Cloud. The session will focus on\n",
       " \n",
       " To democratize data:\n",
       " • Centralize a data platform that is accessible to all employees and allow for easy data sharing \n",
       " • Implement privacy and security to protect data and use data ethically\n",
       " • Compliance and governance for using data in responsible and compliant way\n",
       " • Simplification of processes and procedures to reduce redundancy and faster adoption\n",
       "\n",
       " To mobilize end users:\n",
       " • Increase data literacy: provide training and resources for employees to increase their abilities and skills\n",
       " • Foster a culture of collaborationand openness: cross-functional teams to collaborate and share ideas\n",
       " • Encourage exploration of innovative ideas that impact the organization's values and customers\n",
       " \n",
       "Technical enablement and adoption tactics we've used at TD Bank:\n",
       " • Hands-on training for over 1300+ analytics users with emphasis on learn by doing, to relate to real-life situations\n",
       " • Online tutorials and documentations to be used as self-paced study \n",
       " • Workshops and office hours on specific topics to empower business users\n",
       " • Coaching to work with teams on a specific use case/complex issue and provide recommendations for a faster, cost effective solutions\n",
       " • Offer certification and encourage continuous education for employees to keep up to date with latest \n",
       " • Feedback loop: get user feedback on training and user experience to improve future trainings</td></tr><tr><td>Weaving the Data Mesh in the Department of Defense</td><td>The Chief Digital and AI Office (CDAO) was created to lead the strategy and policy on data, analytics, and AI adoption across the Department of Defense. To enable that vision, the Department must achieve new ways to scale and standardize delivery under a global strategy while enabling decentralized workflows that capture the wealth of data and domain expertise. \n",
       " \n",
       " CDAO’s strategy and goals are aligned with data mesh principles. This alignment starts with providing enterprise-level infrastructure and services to advance the adoption of data, analytics, and AI, creating the self-service data infrastructure as a platform. And it continues through implementing policy for federated computational governance centered around decentralizing data ownership to become domain-oriented but enforcing the quality and trustworthiness of data. CDAO seeks to expand and make enterprise data more accessible through providing data as a product and leveraging a federated data catalog to designate authoritative data and common data models. This results in domain-oriented, decentralized data ownership to empower the business domains across the Department to increase mission and business impact that result in significant cost savings, saving lives, and data serving as a “public good.”\n",
       " \n",
       " Please join us in our presentation as we discuss how the CDAO leverages modern, innovative implementations that accelerate the delivery of data and AI throughout one of the largest distributed organizations in the world; the Department of Defence.  We will walk through how this enables delivery in various Department of Defence use cases.</td></tr><tr><td>The Modern Composable CX Stack: Deconstructing the Data Lifecycle From Infrastructure to Orchestration</td><td> In today’s privacy-conscious world, it’s imperative for IT to have ownership and full control of sensitive customer data. But more than ever, business stakeholders are in dogged pursuit of customer data that can drive better customer understanding and more effective campaigns. For IT, maintaining the integrity of data and architecture while driving customer 360 and other customer experience initiatives can become costly, time consuming and a source of friction in the organization.\n",
       " \n",
       " Hear from ActionIQ’s SVP of Product Justin DeBrabant and Northwestern Mutual’s Chief Data Officer Don Vu as they discuss how Northwestern Mutual is leveraging Databricks and ActionIQ’s CX Hub to bring together business and technical teams around existing data and architecture investments while enabling powerful CX initiatives.\n",
       " \n",
       " You’ll learn:\n",
       " - How to maintain IT choice, agility and control across the stack with composable technology \n",
       " - Strategies to optimize and showcase existing data investments of IT while elevating the stack \n",
       " - Tactics to drive business impact from data management, governance and modernization initiatives</td></tr><tr><td>Leveraging Product Thinking to Scale a Data Platform to a 400-person (Or More!) Data & Analytics Team</td><td>There exists a large and complex set of big data and analytics tools. How do you decide which tools to use? When to use them? In what way? And how to ensure availability and productivity? In this session we'll talk about how we organized the data platform team so that we could provide the best data environment for the 400+ engineers and data scientists at Anheuser-Busch InBev. \n",
       " \n",
       " Data platforms are generally a set of tools that enable data analysis activities. They are built on many different technologies and processes. How to guarantee reliability and constant evolutions for our users was the problem we started to face during the first months after the launch of our Data Platform. To solve this issue, we used an agile mindset and product management framework, already consolidated in software management teams. As a result, we divided the data platform team into squads, each one having its own OKRs and backlog aimed at improving user productivity. \n",
       " \n",
       " Having well-defined responsibilities for each squad according to the stages of the data journey within the platform made it possible to establish clear priorities for evolution in terms of benefits generated for users and the company. This also allowed us to have a highly specialized operations team to each part of the architecture, as each squad is responsible for keeping its product up and running. \n",
       " \n",
       " In this session, we will show the evolution of the team's structure over three years, as well as the challenges encountered. In addition, we will show how the daily work of the squads is organized.</td></tr><tr><td>Experience the New Era of Data & AI: Taking Bold Steps</td><td>Being an organization operating in >65 countries, PETRONAS has a complex and  growing data landscape across its value chain. It is undoubted that digital transformation is a necessity; however, its success is underpinned by data being a strategic enabler. At PETRONAS, we took bold steps in pioneering new frontiers through the undertaking of several strategic initiatives since 2020 to accelerate the digital transformation in PETRONAS:\n",
       " \n",
       " 1. Liberalization of data & analytics\n",
       " Data & analytics are made available and accessible across PETRONAS through a paradigm shift in ways of working with PETRONAS having 'default access rights' to data.\n",
       "  \n",
       " 2. Insight velocity via single source of truth data platform for analytics\n",
       " Enterprise data hub as a dual cloud-based data platform with integrated capabilities and robust technology stacks, whereby data (internal and external) and analytic products that brings the biggest impact and value is curated for insights.\n",
       " \n",
       " 3. Analytics for everyone\n",
       " Everyone can now step-up in using data and advanced analytics without the need for programming skillsets through advanced analytics in EDH that is low-code and augmented by AI\n",
       " \n",
       " 4. Line of sight across multiple data platforms and applications\n",
       " Data Governance Control Tower to provide the line of sight on areas such as metadata management, data access, security and data quality across all data platforms and applications\n",
       " \n",
       " 5. Modernized data governance adaptive data governance; a modern approach, with centralized governance and decentralised assurance to achieve a centralized, inclusive and resilient data governance, while removing bottlenecks and breaking down siloes through federated execution across the business\n",
       " \n",
       " PETRONAS has achieved major milestones with the above. As such, we would like to share our knowledge & expertise.</td></tr><tr><td>Advancing Customer Centricity: Mission Data's Data & Analytics Transformation</td><td>Overview - \n",
       " Mission Data was a three-year transformation focused on leveraging data to better understand our members and engage with them in a personalized way to help them better manage their finances. We’re accomplishing this by transforming Navy Federal into a member-centric, data-driven and AI-powered organization. Leveraging member data in new and innovative ways allows us to create a superior member experience by delivering value back to our members to show them that we know them, and this has always been at the core of our Member Centric strategy. Due to the evolving nature of the broader technological marketplace, consumers have come to expect increasing degrees of personalization across their experiences. Navy Federal members expect the same from their financial institutions and Mission Data is enabling the financial service and guidance our members have come to expect through personalized insights. A personalized insight takes the guess work out of managing finances for members. We show them we know them AND we do it for them…From providing target savings guidance, tracking financial goals, or letting members know they’ve reached a milestone achievement. Delivering these types of insights tailored to our member’s individual needs ultimately deepens their relationship and loyalty with Navy Federal as their trusted financial partner. This is an effort we’re accelerating as we democratize the Mission Data Platform—making member data accessible and actionable across the enterprise. In 2019, we developed a strategy and roadmap for Navy Federal to build the data and analytics capabilities needed to deliver these types of experiences. Key Enablers in the Journey: People, Mindset, Change, Adoption.</td></tr><tr><td>The C-Level Guide to Data Strategy Success With the Lakehouse</td><td>Join us for a practical session on implementing a data strategy leveraging people, process, and technology to meet the growing demands of your business stakeholders for faster innovation at lower cost. Dael Williamson and Robin Sutara, EMEA Field CTOs at Databricks, will share real-world examples on best practices and things to avoid as you drive your strategy from the board to the business units in your organization</td></tr><tr><td>Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse</td><td>Abstract\n",
       " How customers implemented Data Mesh on Databricks and how standardizing on delta format enabled Delta-to-Delta Share to non-Databricks consumers.\n",
       " \n",
       " Presentation Agenda:\n",
       " \n",
       " 1. Current State of the IT Landscape\n",
       " a. Data Silos (problems with organizations not having connected data in the ecosystem)\n",
       " b. A look back on why we moved away from Data warehouses and choose cloud in the first place\n",
       " c. What caused the data chaos in the cloud (instrumentation and too much stitching together) ~ periodic table list of services of the cloud\n",
       " \n",
       " 2. How to strike the balance between Autonomy and Centralization\n",
       " \n",
       " 3. Why Databricks Unity Catalog puts you in the right path to implementing Data Mesh strategy\n",
       " \n",
       " 4. What are the process and features that enable and end-to-end Implementation of a Data Strategy\n",
       " \n",
       " 5. How customers were able to successfully implement the Data mesh on out of the box Unity Catalog and Delta Sharing without overwhelming their IT tool Stack\n",
       " \n",
       " 6. Use-Cases\n",
       " a. Delta-to-Delta Data Sharing \n",
       " b. Delta-to-Others Data Sharing\n",
       " \n",
       " 7. How do you navigate when data today is available across regions, across clouds,on-prem and external systems\n",
       " a. Change Data Feed to share only “data that has changed”\n",
       " \n",
       " 8. Data stewardship\n",
       " a. Why ABAC is important\n",
       " b. How file based access policies and governance play an important role\n",
       " \n",
       " 9. Future State and its pitfalls\n",
       " a. Egress Costs\n",
       " b. Data compliances</td></tr><tr><td>Event Driven Real-Time Supply Chain Ecosystem Powered by Lakehouse</td><td>As the backbone of Australia’s supply chain, the Australia Rail Track Corporation (ARTC) plays a vital role in the management and monitoring of goods transportation across 8,500km of its rail network throughout Australia. ARTC provides weighbridges along their track which read train weights as they pass at speeds of up to 60 kilometers an hour. This information is highly valuable and is required both by ARTC and their customers to provide accurate haulage weight details, analyze technical equipment, and help ensure wagons have been loaded correctly. \n",
       " \n",
       " 750 trains run across a network of 8500 km in a day and generate real-time data at approximately 50 sensor platforms. \n",
       " \n",
       " With the help of structured streaming and Delta Lake, ARTC was able to analyze and store: \n",
       " \n",
       " 1. Precise train location\n",
       " 2. Weight of the train in real-time\n",
       " 3. Train crossing time to the second level \n",
       " 4. Train speed, temperature, sound frequency, and friction \n",
       " 5. Train schedule lookups \n",
       " \n",
       " Once all the IoT data has been pulled together from an IoT event hub, it is processed in real-time using structured streaming and stored in Delta Lake. To understand the train GPS location API calls are then made per minute per train from the Lakehouse. API calls are made in real-time to another scheduling system to lookup customer info. Once the processed, enriched, data is stored in Delta Lake, an API layer was also created on top of it to expose this data to all consumers. \n",
       " \n",
       " Outcome:\n",
       " \n",
       " Increased transparency on weight data as it is now made available to customers. \n",
       "A digital data ecosystem that now ARTC’s customers use to meet their KPIs/ planning. \n",
       "Ability to determine temporary speed restrictions across the network to improve train scheduling accuracy and also schedule network maintenance based on train schedules/ speed.</td></tr><tr><td>Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark™</td><td>Apache Spark™’s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important.\n",
       " \n",
       " Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa’s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data.</td></tr><tr><td>Taking Control of Streaming Healthcare Data</td><td>Chesapeake Regional Information System for our Patients (CRISP), a nonprofit healthcare information exchange (HIE), initially partnered with Slalom to build a Databricks data lakehouse architecture in response to the analytics demands of the COVID-19 pandemic, since then they have expanded the platform to additional use cases. Recently they have worked together to engineer streaming data pipelines to process healthcare messages, such as HL7, to help CRISP become vendor independent.\n",
       " \n",
       " Our talk will focus on the improvements CRISP has made to their data lakehouse platform to support streaming use cases and the impact these changes have had for the organization. We will touch on using Databricks Auto Loader to efficiently ingest incoming files, ensuring data quality with Delta Live Tables, and sharing data internally with a SQL warehouse, as well as some of the work CRISP has done to parse and standardize HL7 messages from hundreds of sources. These efforts have allowed CRISP to stream over 4 million messages daily in near real-time with the scalability it needs to continue to onboard new healthcare providers so it can continue to facilitate care and improve health outcomes.</td></tr><tr><td>Deploying the Lakehouse to Improve the Viewer Experience on Discovery+</td><td>In this session we will discuss how real-time data streaming can be used to gain insights into user behavior and preferences, and how this data is being used to provide personalized content and recommendations on discovery+. We will examine techniques that enables faster decision making and insights on accurate real time data including data masking and data validation. To enable a wide set of data consumers from data engineers to data scientists to data analysts, we will discuss how Unity Catalog is leveraged for secure data access and sharing while still allowing teams flexibility. Operating at this scale requires examining the value being created by the data being processed and optimizing along the way and we will share some of our success in this area.</td></tr><tr><td>Apache Spark™ Streaming and Delta Live Tables Accelerates KPMG Clients For Real-Time IoT Insights</td><td>Unplanned downtime in manufacturing costs firms up to a trillion dollars annually. Time that materials spend sitting on a production line is lost revenue. Even just 15 hours of downtime a week adds up to over 800 hours of downtime yearly. The use of internet of things or IoT devices can cut this time down by providing details of machine metrics. However, IoT predictive maintenance is challenged by the lack of effective, scalable infrastructure and machine learning solutions. IoT data can be the size of multiple terabytes per day and can come in a variety of formats. Furthermore, without any insights and analysis, this data becomes just another table. \n",
       " \n",
       " The KPMG Databricks IoT Accelerator is an end-to-end solution enabling manufacturing plant operators to have a bird’s eye view of their machines’ health and empowers proactive machine maintenance across their portfolio of IoT devices. The Databricks Accelerator ingests IoT streaming data at scale and implements the Databricks medallion architecture while leveraging delta live tables to clean and process data. Real time machine learning models are developed from IoT machine measurements and are managed in MLflow. The AI predictions and IoT device readings are compiled in the gold table powering downstream dashboards like Tableau. Dashboards inform machine operators of not only machines’ ailments, but action they can take to mitigate issues before they arise. Operators can see fault history to aid in understanding failure trends, and can filter dashboards by fault type, machine, or specific sensor reading.</td></tr><tr><td>Optimizing batch and streaming aggregations</td><td>I've got a client recently who asked me to optimize their batch and streaming workloads. It happened to be aggregations using DataFrame.groupBy operation with a custom Scala UDAF over a data stream from Kafka. Just a single simple-looking request that turned itself up into a a-few-month-long hunt to find a more performant query execution planning than ObjectHashAggregateExec that kept falling back to a sort-based aggregation (i.e. the worst possible aggregation runtime performance). It quickly taught us that an aggregation using a custom Scala UDAF cannot be planned other than ObjectHashAggregateExec but at least tasks don't always have to fall back. And that's just batch workloads. When you throw in streaming semantics and think of the different output modes, windowing and streaming watermark optimizing aggregation can take a long time to do right.\n",
       " \n",
       " During the talk you'll learn the different execution strategies of aggregation (groupBy) in Apache Spark and what issues I faced while hunting down the tiniest performance improvements. You will learn why an aggregation with a Scala UDAF cannot be planned using HashAggregateExec and what problems you may come across while using streaming aggregation over the built-in Kafka data source. Lots of optimization tips and tricks with quite a few looks at the internals of aggregation query execution.</td></tr><tr><td>Structured Streaming: Demystifying Arbitrary Stateful Operations</td><td>Let’s face it -- data is messy. And your company’s business requirements? Even messier! You’re staring at your screen, knowing there is a tool that will let you give your business partners the information they need as quickly as they need it. There’s even a Python version of it now. But…it looks kinda scary. You’ve never used it before, and you don’t know where to start. Yes, we’re talking about the dreaded flatMapGroupsWithState! But fear not - we’ve got you covered. In this session we’ll take a real-word use case and use it to show you how to break down flatMapGroupsWithState into its basic building blocks. We’ll explain each piece in both Scala and the newly-released Python, and at the end we’ll illustrate how it all comes together to enable the implementation of arbitrary stateful operations with Spark Structured Streaming.</td></tr><tr><td>How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience</td><td>Disney+ uses Amazon Kinesis and Databricks Apache Spark™ streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers.</td></tr><tr><td>The Future is Open: Data Streaming in an Omni-Cloud Reality</td><td>An assortment of low-code connectors boasts the ability to make data available for analytics in real time. However, many such turn-key solutions face challenges: cost-inefficient EDW targets; inability to evolve schema; forbiddingly expensive data exports due to cloud and vendor lock-in. The alternative: an open data lake that unifies batch and streaming workloads. Increasingly, we must adapt to a multi-cloud reality. Delta Lake decouples data storage from proprietary formats, dramatically reducing data extraction costs. Exporting data from delta tables eliminates the compute and storage API costs required by EDW. Bronze landing zones in Delta format build the foundation for a medallion architecture. Apache Spark™ Structured Streaming provides a unified ingestion strategy. Streaming triggers allow us to switch back and forth between batch and stream with one-line code changes. Streaming aggregation enables us to incrementally compute on data that arrives near each other.\n",
       " It is a misconception that streaming requires always-on clusters and thus is prohibitively expensive. Streaming means the ability to automatically handle new data and avoid reprocessing of historical data; real time is optional depending on use cases. Autoloader is a technique to discover newly arrived data and ensure exactly once, incremental processing. DLT further simplifies streaming jobs, accelerating the development cycle.\n",
       " Workflows bring it all together and the open source dbx project applies SWE best practices. Workflows can orchestrate tasks such as DLT pipelines, spark jobs, SQL scripts, as well as dbt transformations. Developers can define compute resources, scheduling, and dependencies in descriptive yaml format. Both task definitions and job code are versioned with popular source control systems.</td></tr><tr><td>Getting Insight From Anything: Gathering Data With IoT Devices and Delta Live</td><td>The drive to make data-driven decisions shapes the strategy of many companies. However, this dream has a problem; just because you need to make a decision does not mean you have the data to make it! Often these decisions require data from sources typically unconnected to the internet. This could be an architect determined to understand how the construction of her building is progressing, or a manufacturer wanting to live their best Industry 4.0 life. It's not uncommon for companies to find this journey to become “smart” is a long one; with difficult ethical ground to cover, or prohibitive costs associated with procuring the necessary equipment. However, the development of cheap and accessible IoT devices allows for the gathering of data from anything to be more within reach than ever.\n",
       " \n",
       " This talk will look at efforts to build proof-of-concept IoT systems. We will discuss our (continuing) journey of using and managing hardware and the problems we faced along the way. The main focus however will be the software stack. Azure IoT Hub is used for device management, with Power BI supporting the front end. However, the centre-piece is Delta Live Tables - allowing us a method of near real-time analysis of our data. We will discuss the advantages of using Delta Live for this, and how we wish to extend our IoT systems in the future.</td></tr><tr><td>Practical Pipelines: A Houseplant Alerting System With ksqlDB</td><td>Houseplants can be hard; in many cases, over-watering and under-watering can have the same symptoms. Take away the guesswork involved in caring for your houseplants while also gaining valuable experience in building a practical, event-driven pipeline in your own home! This session explores the process of building a houseplant monitoring and alerting system using a Raspberry Pi and Apache Kafka. \n",
       " \n",
       " Moisture and temperature readings are captured from sensors in the soil and streamed into Kafka. From here, we’ll use stream processing to transform the data, create a summary view of the current state, and drive real-time push alerts through Telegram. In this session, I’ll talk about how I ingest the data followed by the tools – including ksqlDB and Kafka Connect – that help transform the raw data into useful information, and finally, I’ll show how to use Kafka Producers and Consumers to make the entire application more interactive.\n",
       " \n",
       " By the end of this session you’ll have everything you need to start building practical streaming pipelines in your own home. Roll up your sleeves – let’s get our hands dirty!</td></tr><tr><td>Disaster Recovery Strategies for Structured Streams</td><td>In recent years, many businesses have adopted real-time streaming applications to enable faster decision making, quicker predictions and improved customer experiences. Few of these applications are driving critical business use cases like financial fraud detection, loan application processing, personalized offers, etc. These business critical applications need robust disaster recovery strategies to recover from the catastrophic events to reduce the lost uptime. However, most organizations find it hard to set up disaster recovery for streaming applications as it involves continuous data flow. Streaming state and temporal behavior of data brings additional complexities to the DR strategy.\n",
       " \n",
       " A reliable disaster recovery strategy includes backup, failover and failback approaches for the streaming application. Unlike the batch applications, these steps include many moving elements and need a very sophisticated approach to ensure that the services are failing over the DR region and meet the set RTO and RPO requirements. In this session, we will cover following topics with a FINSERV use case demo:\n",
       " 1. Backup strategy: backup of delta tables, message bus services and checkpoint including offsets\n",
       " 2. Failover strategy: failover strategy to disable services in the primary region and start the services in the secondary region with minimum data loss \n",
       " 3. Failback strategy: failback strategy to restart the services in the primary region once all the services are restored \n",
       " 4. Common challenges and best practices for backup</td></tr><tr><td>Near Real-Time Data Linkage and Entity Resolution for High Volumes of Continuous and Unsynchronized Data Streams</td><td>Citrix Analytics is a cloud-based service that provides analytical insights from data across Citrix product portfolio. It collates data from various sources and proactively detects security threats.\n",
       " \n",
       " The data consists of information from various entities like users, devices, applications, network, files and shares, along with their correlation over a period. This data is analyzed by various ML algorithms to detect any suspicious activity. The ML models need correlated data in real-time so vulnerabilities can be detected instantly and remediation actions can be taken.\n",
       " \n",
       " The data pipeline comprises of multiple event sources, each one of them producing high volumes of continuous data. This data generated from different sources need to be correlated using common identifiers to produce final stream of data with low latency and high data quality.\n",
       " \n",
       " The proposed solution is to build scalable, extensible, fault-tolerant system for stateful joining of two or more high volume event streams that are not fully synchronised, event ordering is not guaranteed, and certain events arrive a bit late. The system also ensures to combine the events/link so the data is in near real-time with low latency so that there is no impact on the downstream applications like ML models in determining the suspicious behavior. Apart from combining events the system also ensures to propagate the needed entities to other product streams to help in entity resolution. If any of the needed data is yet to arrive, we can configure few parameters based on use cases, so as to achieve the required eventual or attribute consistency.\n",
       " \n",
       " The architecture is an agnostic streaming framework, but our implementation leverages Spark Streaming and Kafka. The solution is implemented and is processing billions of events per day.</td></tr><tr><td>Realtime ML in Marketplace @ Lyft</td><td>Lyft is a ride-sharing company which is a two-sided marketplace; balancing supply and demand using various levers (passenger pricing, driver incentive etc.) to maintain an efficient system. Lyft has built a real-time optimization platform that helps to build the product faster. This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. This infrastructure consumes a massive number of events from different sources to make real-time product decisions.\n",
       " Rakesh discusses how Lyft organically evolved and scaled the streaming platform that provides a consistent view of the marketplace to aid an individual team independently run their optimization. The platform offers online and offline feature access that helps teams to back test their model in the future. It provides various other powerful capabilities such as replaying the production ML feature in PyNotebook, feature validation, near-realtime model training, executing multi-layer of models in a DAG, etc. \n",
       " He is also going to elaborate things that helped him scale the systems to process millions of events per minute and power T0 products with tighter latency SLA.</td></tr><tr><td>Streaming Data Analytics With Power BI and Databricks</td><td>In this session we will present a series of end-to-end technical demos illustrating the synergy between Databricks and Power BI for streaming use cases. \n",
       "  - Scenario 1: DLT + Power BI Direct Query and Auto Refresh\n",
       "  - Scenario 2: Structured Streaming + Power BI streaming datasets \n",
       "  - Scenario 3: DLT + Power BI composite datasets\n",
       "  - Considerations when to choose which scenario</td></tr><tr><td>How Coinbase Built and Optimized SOON, a Streaming Ingestion Framework</td><td>Data with low latency is important for real-time incident analysis and metrics. Though we have up-to-date data in OLTP databases, they cannot support those scenarios. Data need to be replicated to a data warehouse to serve queries doing group-bys, and joins across tables from different systems. At Coinbase, we designed SOON (Spark cOntinuOus iNgestion) based on Kafka, Kafka Connect, and Spark as an incremental table replication solution to replicate tables of any size from any database to Delta Lake in a timely manner. It also supports Kafka events ingestion naturally.\n",
       " \n",
       " SOON incrementally ingests Kafka events as appends, updates, and deletes to an existing table on Delta Lake. The events are grouped into two categories: CDC (change data capture) events generated by Kafka Connect source connectors, and non-CDC events by the frontend or backend services. Both types can be appended or merged into the Delta Lake. Non-CDC events can be in any format, but CDC events must be in the standard SOON CDC schema. We implemented Kafka Connect SMTs to transform raw CDC events into this standardized format. SOON unifies all streaming ingestion scenarios such that users only need to learn one onboarding experience and the team only needs to maintain one framework.\n",
       " \n",
       " We care about the ingestion performance. The biggest append-only table onboarded has ingress traffic at hundreds of thousands events per second; the biggest CDC-merge table onboarded has a snapshot size of a few TBs and CDC update traffic at hundreds of events per second. A lot of innovative ideas are incorporated in SOON to improve its performance, such as min-max range merge optimization, KMeans merge optimization, no-update merge for deduplication, generated columns as partitions, etc.</td></tr><tr><td>High Volume Intelligent Streaming with Sub-Minute SLA for Near Real-Time Data Replication</td><td>Come and learn an innovative solution built around Databricks structured streaming and Delta Live Tables (DLT) to replicate thousands of tables from on-premisis to cloud-based relational databases. A highly desirable pattern for many enterprises across the industries to replicate on-premises data to cloud-based data lakes and data stores in near real time for consumption. This powerful architecture can offload legacy platform workloads and accelerate cloud journey. The intelligent cost-efficient solution leverages thread-pools, multi-task jobs, Kafka, spark structured streaming and DLT. This session will go into detail about problems, solutions, lessons-learned and best practices.</td></tr><tr><td>Using Cisco Spaces Firehose API as a Stream of Data for Real-Time Occupancy Modelling</td><td>Honeywell manages the control of equipment for hundreds of thousands of buildings worldwide. Many of our outcomes relating to energy and comfort rely on knowing where people are in the building at any one time. This is so we can target health and comfort conditions more suitably to areas where are more densely populated. Many of these buildings have Cisco IT infrastructure in them. Using their WIFI points and the RSSI signal strength from people’s laptops and phones, Cisco can calculate the number of people in each area of the building. Cisco Spaces offer this data up as a real-time streaming source. Honeywell HBT has utilised this stream of data by writing delta live table pipelines to consume this data source.\n",
       " Honeywell buildings can now receive this firehose data from hundred of concurrent customers and provide this occupancy data as a service to our vertical offerings in commercial, health, real estate and education. We will discuss the benefits of using DLT to handle this sort of incoming stream data, and illustrate the pain points we had and the resolutions we undertook in successfully receiving the stream of Cisco data. We will illustrate how our DLT pipeline was designed, and how it scaled to deal with huge quantities of real time streaming data.</td></tr><tr><td>Leveraging IoT Data at Scale to Mitigate Global Water Risks Using Apache Spark Streaming and Delta</td><td>Every year, billions of dollars are lost due to water risks from storms, floods, and droughts. Water data scarcity and excess are issues that risk models cannot overcome, creating a world of uncertainty. Divirod is building a platform of water data by normalizing diverse data sources of varying velocity into one unified data asset. In addition to publicly available third-party datasets, we are rapidly deploying our own IoT sensors. \n",
       " \n",
       " These sensors ingest signals at a rate of about 100,000 messages per hour into preprocessing, signal-processing, analytics, and postprocessing workloads in one spark-streaming pipeline to enable critical real-time decision-making processes. By leveraging streaming architecture, we were able to reduce end-to-end latency from 10s of minutes to just a few seconds. \n",
       " \n",
       " We are leveraging Delta to provide a single query interface across multiple tables of this continuously changing data. This enables data science and analytics workloads to always use the most current and comprehensive information available. In addition to the obvious schema transformations, we implement data-quality metrics and datum conversions to provide a trustworthy unified dataset.</td></tr><tr><td>Unlocking Near Real-Time Data Replication with CDC, Apache Spark Streaming, and Delta Lake</td><td>Tune into DoorDash's journey to migrate from a flaky ETL system with 24-hour data delays, to standardizing a CDC streaming pattern across 150+ databases to produce near real-time data in a scalable, configurable, and reliable manner. During this journey, understand how we use Delta Lake to build a self-serve, read-optimized data lake with data latencies of 15 minutes, whilst reducing operational overhead. Furthermore, understand how certain tradeoffs like conceding to a non-real-time system allow for multiple optimizations but still permit for OLTP query use-cases, and the benefits it provides.</td></tr><tr><td>Streaming Schema Drift Discovery and Controlled Mitigation</td><td>\"When creating streaming work loads with Databricks, it can sometimes be difficult to capture and understand the current structure of your source data. For example, what happens if you are ingesting JSON events from a vendor, and the keys are very sparsely populated, or contain dynamic content? Ideally, data engineers want to \"\"lock in\"\" a target schema in order to minimize complexity and maximize performance for known access patterns. What do you do when your data sources just don't cooperate with that vision?</td></tr><tr><td> \n",
       " The first step is to quantify how far your current source data is drifting from your established Delta table. But how? In this talk I will demonstrate a way to capture and visual drift across all of your streaming tables. The next question is</td><td> \"\"Now that I see all of the data I'm missing</td></tr><tr><td>Real-Time Streaming Solution for a Call Center Analytics: Business Challenges and Technical Enablement</td><td>A large international client with a business footprint in North America, Europe and Africa, reached out to us with an interest in having a real-time streaming solution designed and implemented for its call center handling incoming and outgoing client calls. The client had a previous bad experience with another vendor, who overpromised and underdelivered on the latency of the streaming solution. The previous vendor delivered an over-complex streaming data pipeline resulting in the data taking over 5 minutes to reach a visualization layer. The client felt that architecture was too complex and involved many services integrated together. Our immediate challenges involved gaining the client's trust and proving that our design and implementation quality would supercede a previous experience. To resolve an immediate challenge of the overly complicated pipeline design, we deployed a Databricks Lakehouse architecture with Azure Databricks at the center of the solution. Our reference archiitecture integrated Genesys Cloud -> App Services -> Event Hub -> Databricks <-> Data Lake -> Power BI. The streaming solution proved to be low latency (seconds) during the POV stage, which led to subsequent productionalization of the pipeline with deployment of jobs, DLTs pipeline, including multi-notebook workflow and business and performance metrics dashboarding relied on by the call center staff for a day-to-day performance monitoring and improvements.</td></tr><tr><td>Top Mistakes to Avoid in Streaming Applications</td><td>Nowadays, streaming use cases become very important in almost all companies. When you think of streaming, Apache Spark™ Structured Streaming is at the top of the list in the technology. As we are dealing with many customer streaming use cases, we learned many do’s and don’t of the streaming application. I will be discussing the top mistakes to avoid when working with streaming applications with regard to different sources and sinks like DLT, Kafka, Delta, and so on. If you are avoiding these mistakes while architecting/running/restarting the application, then you will avoid the costly mistakes at a later point in time.</td></tr><tr><td>Embracing the Future of Data Engineering: The Serverless, Real-Time Lakehouse in Action</td><td>As we venture into the future of data engineering, streaming and serverless technologies take center stage. In this fun, hands-on, in-depth and interactive session you can learn about the essence of future data engineering today. In this session we will tackle the challenge of processing streaming events created by hundreds of sensors in the conference room from a serverless web app (bring your phone and be a part of it!). The focus is on the system architecture and the involved products. Which Databricks product, capability and settings will be most useful for our scenario? What does streaming really mean and why does it make our life easier? What are the exact benefits of serverless and how much serverless is a particular solution?\n",
       "Leveraging the power of the Databricks Lakehouse Platform, I will demonstrate how to create a streaming data pipeline with delta live tables ingesting data from AWS Kinesis. Further, I’ll utilize advanced Databricks workflows triggers for efficient orchestration and real-time alerts feeding into a real-time dashboard. And since I don’t want you to leave with empty hands - I will use Delta Sharing to share the results of the demo we built with every participant in the room.\n",
       "Join me in this hands-on exploration of cutting-edge data engineering techniques and witness the future in action!</td></tr><tr><td>Building and Managing Data Platform for 13+ PB Delta Lake and 1000’s of Users:  AT&T'S Story</td><td>Data runs AT&T’s business, just like it runs most businesses these days. Data can lead to a greater understanding of a business and when translated correctly into information can provide human and business systems valuable insights to make better decisions. Unique to AT&T is the volume of data we support, how much of our work that is driven by AI and the scale at which data and AI drive value for our customers and stakeholders. Our Cloud migration journey includes making data and AI more accessible to employees throughout AT&T so they can use their deep business expertise to leverage data more easily and rapidly. We always had to balance this data democratization and desire for speed with keeping our data private and secure. We loved the open ecosystem model of Lakehouse that enables data , BI and ML tools to be seamlessly integrated on a single pane arena, it just simplifies the architecture and reduces dependencies between technologies in the cloud. Being clear in our architecture guidelines and patterns was very important to us for our success. We are seeing more interest from our business unit partners and continuing to build the AI as a service capabilities to support more citizen data scientists. To scale up our Lakehouse journey , we built a Databricks center of excellence (CoE) function in AT&T which today has ~1400+ active members , further concentrating existing expertise and resources in ML/AI discipline to collaborate on all things Databricks like technical support, trainings , FAQ’s and best practices to attain and sustain world-class performance and drive business value for AT&T. Join us to learn more about how we process and manage 10+ Petabytes of our network Lakehouse with Delta Lake and Databricks.</td></tr><tr><td>Data Democratization with Lakehouse: An Open Banking Application Case</td><td>\"Banco Bradesco represents one of the largest companies in the financial sector in Latin America. They have more than 99 million customers, 79 years of history, and a legacy of data distributed in hundreds of on-premises systems. With the spread of data-driven approaches and the growth of cloud computing adoption, we needed to innovate and adapt to new trends and enable an analytical environment with democratized data.  We will show how more than 8 business departments have already engaged in using the Lakehouse exploratory environment, with more than 190 use cases mapped and a multi-bank financial manager. Unlike with on-premises, the cost of each process can be isolated and managed in near real-time, allowing quick responses to cost and budget deviations, while increasing the deployment speed of new features 36 times compared to on-premises. The data is now used and shared safely and easily between different areas and companies of the group. Also, the view of dashboards within Databricks allows panels to be efficiently \"\"prototyped\"\" with real data</td></tr><tr><td>Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying</td><td>Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks’ dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia’s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies,  including success stories that resulted in significant cost savings and efficiency gain.</td></tr><tr><td>How RecRoom Processes Billions of Events Per Day With Databricks and RudderStack</td><td>Learn how Rec Room, a fast-growing augmented and virtual reality software startup, is saving 50% of their engineering team's time by using Databricks and RudderStack to power real-time analytics and insights for their 85 million gaming customers. In this session, you will walk through a step-by-step explanation of how Rec Room set up efficient processes for ingestion into their data lakehouse, transformation, reverse-ETL and product analytics. You will also see how Rec Room is using incremental materialization of tables to save costs and establish an uptime of close to 100%.</td></tr><tr><td>Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?</td><td>Rust is a unique language whose traits make it very appealing for data engineering. In this session, we'll walk through the different aspects of the language that make it such a good fit for big data processing including: how it improves performance and how it provides greater safety guarantees and compatibility with a wide range of existing tools that make it well positioned to become a major building block for the future of analytics. \n",
       " \n",
       " We will also take a hands-on look through real code examples at a few emerging technologies built on top of Rust that utilize these capabilities, and learn how to apply them to our modern lakehouse architecture.</td></tr><tr><td>Making the Shift to Application-Driven Intelligence</td><td>In the digital economy, application-driven intelligence delivered against live, real-time data will become a core capability of successful enterprises. It has the potential to improve the experience that you provide to your customers and deepen their engagement. But to make application-driven intelligence a reality, you can no longer rely only on copying live application data out of operational systems into analytics stores.\n",
       " Rather, it takes the unique real-time application-serving layer of a MongoDB database combined with the scale and real-time capabilities of a Databricks Lakehouse to automate and operationalize complex and AI-enhanced applications at scale. In this session we will show how it can be seamless for developers and data scientists to automate decisioning and actions on fresh application data and we'll deliver a practical demonstration on how operational data can be integrated in real time to run complex Machine Learning pipelines.</td></tr><tr><td>Making Travel More Accessible for Customers Bringing Mobility Devices</td><td>American Airlines takes great pride in caring for customers travel, and recognize the importance of supporting the dignity and independence of everyone who travels with us. As we work to improve the customer experience, we're committed to making our airline more accessible to everyone. Our work to ensure that travel that is accessible to all is well underway. We have been particularly focused on making the journey smoother for customers who rely on wheelchairs or other mobility devices. We have implemented the use of a bag tag specifically for wheelchairs and scooters that gives team members more information, like the mobility device’s weight and battery type, or whether it needs to be returned to a customer before a connecting flight.\n",
       " \n",
       " As a data engineering and analytics team, we at American Airlines are building a passenger service request data product that will provide timely insights on expected mobility device traffic at each airport so that the front-line team members can provide seamless travel experience to the passengers.</td></tr><tr><td>Improve Apache Spark DS V2 Query Planning Using Column Stats</td><td>When doing the TPC-DS benchmark using external V2 data source, we have observed that for several of the queries, DS V1 has better join plans than Spark. The main reason is that DS V1 uses column stats, especially number of distinct values (NDV) for query optimization. Currently, Spark DS V2 only has interfaces for data sources to report table statistics such as size in bytes and number of rows. In order to use column stats in DS V2, we have added new interfaces to allow external data sources to report column stats to Spark. For a data source with huge data, it’s always challenging to get the column stats, especially the NDV. We plan to calculate NDV using Apache DataSketches Theta sketch and save the serialized compact sketch in the statistics file. The NDV and other column stats will be reported to Spark for query plan optimization.</td></tr><tr><td>Databricks Powered Enterprise Data Platform for Faster Insights with Optimal Cost</td><td>A leading Australian Bank engaged Deloitte to review architecture of current enterprise data platform enabled using Databricks. Bank’s vision is to limit proliferation of diverse tools/technology while enabling “best of breed” capabilities in the bank’s next-gen data platform. To realize this vision, the enterprise data platform will use various Databricks capabilities: DLT (Delta Live tables), Unity Catalog, Delta Sharing, Databricks Dashboards and Databricks Alerts. Using of these Databricks high performance and scalable capabilities allows the bank to avoid proliferation of disparate tools and reduce a need for custom development or integration. \n",
       " \n",
       " During this session, Deloitte, Databricks and Australian Bank will present how Databricks can be leveraged to develop a robust, resilient and scalable data platform which allows an organization to achieve scaled data sharing, reducing lead time to generate insights via reports, dashboards, AI/ML and Data Sharing.</td></tr><tr><td>Using Databricks to Power Insights and Visualizations on the S&P Global Marketplace</td><td>In this session, we will explain the visualizations that serve to shorten the time to insight for our prospects and encourage potential buyers to take the next step and request more information from our commercial team. The S&P Global Marketplace is a discovery and exploration platform that enables prospective buyers and clients to easily search fundamental and alternative datasets from across S&P Global and curated third-party providers. It serves as a digital storefront that provides transparency into data coverage and use cases, reducing the time and effort for clients to find data for their needs. A key feature of Marketplace is our interactive data visualizations that provide insight into the coverage of a dataset and demonstrate how the dataset can be used to make more informed decisions.   \n",
       " \n",
       "  \n",
       " \n",
       " The S&P Global Marketplace’s interactive visualizations are displayed in Tableau and are powered by Databricks. The Databricks platform allows for easy integration of S&P Global data and provides a collaborative environment where our team of product managers and data engineers can develop the code to generate each visualization. The team utilizes the web interface to develop the queries that perform the heavy lifting of data transformation instead of performing these tasks in Tableau. The final notebook output is saved into a custom datamart (“golden table”) which is the source for Tableau. We also developed an automated process that refreshes the whole process to ensure Marketplace has up to date visualizations.</td></tr><tr><td>Self-Service Geospatial Analysis Leveraging Databricks, Apache Sedona, and R</td><td>Geospatial data analysis is critical to understanding the impact of agricultural operations on environmental sustainability with respect to water quality, soil health, greenhouse gasses, and more. Outside of a few specialized software products, however, support for spatial data types is often limited or missing from analytics and visualization platforms. In this session, we show how Truterra is using Databricks, Apache Sedona, and R to analyze spatial data at scale. Additionally, learn how Truterra uses spatial insights to educate and promote practices that optimize profitability, sustainability, and stewardship outcomes at the farm. In this session, you will see how Databricks and Apache Sedona are used to process large spatial datasets including field, watershed, and hydrologic boundaries. You will see dynamic widgets, SQL and R used in tandem to generate map visuals, display them, and enable download all from a Databricks notebook.</td></tr><tr><td>Processing Delta Lake Tables on AWS Using AWS Glue, Amazon Athena, and Amazon Redshift</td><td>Delta Lake is an open-source project that helps implement modern data lake architectures commonly built on cloud storages. With Delta Lake, you can achieve ACID transactions, time travel queries, CDC, and other common use cases on the cloud. There are a lot of use cases of Delta tables on AWS. AWS has invested a lot in this technology, and now Delta Lake is available with multiple AWS services, such as AWS Glue Spark jobs, Amazon EMR, Amazon Athena, and Amazon Redshift Spectrum. AWS Glue is a serverless, scalable data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources. With AWS Glue, you can easily ingest data from multiple data sources such as on-prem databases, Amazon RDS, DynamoDB, MongoDB into Delta Lake on Amazon S3 even without expertise in coding.\n",
       " \n",
       " This session will demonstrate how to get started with processing Delta Lake tables on Amazon S3 using AWS Glue, and querying from Amazon Athena, and Amazon Redshift. The session also covers recent AWS service updates related to Delta Lake.</td></tr><tr><td>A Simple SQL Execution API for the Databricks Lakehouse Architecture</td><td>Given the widespread support of REST architectures, data managed by the Databricks Lakehouse Platform can now be accessed from anywhere, making it easy to build data applications tailored to your needs. You can use the SQL Execution API to build web services-based applications, integrate with various applications and computing devices, create generic integration layers for enterprise services, or create custom client libraries for your programming language of choice.\n",
       " \n",
       " In this talk we are going to deep dive into the design and implementation of the SQL Execution API with a focus on:\n",
       " 1. The key API features which can be used to develop data applications.\n",
       " 2. The underlying Databricks architecture such as Cloud Fetch that we developed to enable this API.</td></tr><tr><td>Best Exploration of Columnar Shuffle Design</td><td>To significantly improve the performance of Spark SQL, there is a trend to offload Spark SQL execution to highly optimized native libraries or accelerators in past several years, like Photon from Databricks, Nvidia's Rapids plug-in, and Intel & Kyligence's initiated open source Gluten project. By the multi-fold performance improvement from these solutions, more and more Spark users have started to adopt the new technology. One characteristics of native libraries is that they all use columnar data format as the basic data format. It's because the columnar data format has the intrinsic affinity to vectorized data processing using SIMD instructions. While vanilla Spark's shuffle is based on spark's internal row data format. The high overhead of the columnar to row and row to columnar conversion during the shuffle makes reusing current shuffle not possible. Due to the importance of shuffle service in Spark, we have to implement an efficient columnar shuffle, which brings couple of new challenges, like the split of columnar data, or the dictionary support during shuffle.\n",
       " \n",
       " In this session, we will share 1) the exploration process of the columnar shuffle design during our Gazelle and Gluten development, and best practices for implementing the columnar shuffle service. We will also share how we learned from the development of vanilla Spark's shuffle, for example, how to address the small files issue then we will propose the new shuffle solution. We will show the performance comparison between Columnar shuffle and vanilla Spark's row-based shuffle. Finally, we will share how the new built-in accelerators like QAT and IAA in the latest Intel processor are used in our columnar shuffle service and boost the performance.</td></tr><tr><td>What Happens When Curious and Knowledgeable Business People are Provided Access to the Right Data and Equipped With Simple Tools and Supported by Guidance From In-House Technical Experts</td><td>Too often business decisions in large organizations are based on time consuming and labor-intensive data extracts, fragile excel or access sheets that require significant manual intervention. The teams that prepare these manual reports have invaluable heuristic knowledge that, when combined with meaningful data and tools, can make smart business decisions. \n",
       " \n",
       " \n",
       " Imagine a world where these business teams are empowered with tools that help them build meaningful reports despite their limited technical expertise.\n",
       " \n",
       " \n",
       " In this talk we will discuss : \n",
       " \n",
       " The value derived from investing in developing citizen data personas within a business organization\n",
       " \n",
       " How we successfully built a citizen data analytics culture within Michelin \n",
       " \n",
       " Real examples of the impact of this initiative on - the business and on the people themselves.\n",
       " \n",
       " The audience will walk away with some convincing arguments for building a citizen data culture in their organization and a how-to cookbook that they can use to cultivate citizen data personas.\n",
       " \n",
       " Finally, they will also have the opportunity to interactively uncover key success factors in the case of Michelin that can help drive a similar initiative in their respective companies.</td></tr><tr><td>Modularized Approach to Scale Suite of Advanced Omnichannel Analytics Products</td><td>GSK started the journey of Omnichannel vision a few years ago using static business rules and customer targeting. We kicked off 2022 with conceptualization of a suite of monitoring, measurement and planning products that would help GSK establish a comprehensive omnichannel measurement strategy and plan across its global markets. One of the key guiding principles during conceptualization was deeply and seamlessly embedding these products within GSK’s data transformation and storage architecture built on Azure Databricks. We built the platform powered extensively by Databricks, GitHub and open-source technologies like Python. In this session, we will cover the following aspects of our journey along with some best practices that we have learned over time.</td></tr><tr><td>AI to FI with Databricks</td><td>Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\n",
       " 1. Challenges of common pipeline design to support AI-FI \n",
       " 2. Challenges of leveraging data acquired over many acquisitions\n",
       " 3. How is the Lakehouse paradigm lending itself to solving these challenges?\n",
       " 4. Some cool results we can show the world.\n",
       " \n",
       " To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \n",
       " \n",
       " To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying.</td></tr><tr><td>How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse</td><td>Don't miss this session where we demonstrate how the Texas Rangers baseball team organized their predictive models by using MLFlow and the MLRegistry inside Databricks. They started using Databricks as a simple solution to centralizing our development on the cloud. This helped lessen the issue of siloed development in our team, and allowed us to leverage the benefits of distributed cloud computing. But we quickly found that Databricks was a perfect solution to another problem that we faced in our data engineering stack. Specifically, cost, complexity, and scalability issues hampered our data architecture development for years, and we decided we needed to modernize our stack by migrating to a lakehouse. With our lakehouse, ad-hoc-analytics, ETL operations, and MLOps all living within Databricks, development at scale has never been easier for our team. Going forward, we hope to fully eliminate the silos of development, and remove the disconnect between our analytics and data engineering teams. From computer vision, pose analytics, and player tracking, to pitch design, base stealing likelihood, and more, come see how the Texas Rangers are using innovative cloud technologies to create action-driven reports from the current sea of Big Data. </td></tr><tr><td>Improving Hospital Operations With Streaming Data and Real Time AI/ML</td><td>Over the past two years, Providence has developed a robust streaming data platform (SDP) leveraging Databricks in Azure. The SDP enables us to ingest and process real-time data reflecting clinical operations across our 52 hospitals and roughly 1000 ambulatory clinics. HL7 messages generated by Epic are parsed using Databricks in our secure cloud environment and used to generate an up-to-the minute picture of exactly what is happening at the point of care. We are already leveraging this information to minimize hospital overcrowding and have been actively integrating AI/ML to accurately forecast future conditions (e.g., arrivals, length of stay, acuity, and discharge requirements). This allows us to both improve resource utilization (e.g., nurse staffing levels) and to optimize patient throughput. The result is both improved patient care and operational efficiency. In this session we will share how these outcomes are only possible with the power and elegance afforded by our investments in Azure, Databricks, and increasingly Lakehouse. We will demonstrate Providence's blueprint for enabling real-time analytics which can be generalized to other healthcare providers.</td></tr><tr><td>AI Regulation is Coming: The EU AI Act and How Databricks Can Help with Compliance</td><td>With the heightened attention on LLMs and what they can do, and the widening impact of AI on day-to-day life, the push by regulators across the globe to regulate AI is intensifying. As with GDPR in the privacy realm, the EU is leading the way with the EU Artificial Intelligence Act (AIA). Regulators everywhere will be looking to the AIA as precedent, and understanding the requirements imposed by the AIA is important for all players in the AI channel. Although not finalized, the basic framework regarding how the AIA will work is becoming clearer. The impact on developers and deployers of AI (‘providers’ and ‘users’ under the AIA) will be substantial. Although the AIA will probably not go into effect until early 2025, AI applications developed today will likely be affected, and design and development decisions made now should take the future regulations into account.\n",
       "\n",
       "In this session, Matteo Quattrocchi, Brussels-based Director, Policy – EMEA, for BSA (the Software Alliance – the leading advocacy organization representing the enterprise software sector), will present an overview of the current proposed requirements under the AIA and give an update on the ongoing deliberations and likely timing for enactment. We will also highlight some of the ways the Lakehouse platform, including Managed MLflow, can help providers and users of ML-based applications meet the requirements of the AIA and other upcoming AI regulations.\n",
       "</td></tr><tr><td>Using Databricks to Develop Statistical and Mathematical Models to Forecast the Monkeypox Outbreak in Washington State</td><td>In the spring and summer of 2022, monkeypox was detected in the United States and quickly spread throughout the country. To contain and mitigate the spread of the disease in Washington State, the Washington Department of Health data science team used the Databricks platform to develop a modeling pipeline that employed statistical and mathematical techniques to forecast the course of the monkeypox outbreak throughout the state. These models provided actionable information that helped inform decision making and guide the public health response to the outbreak. \n",
       " \n",
       " We used contact-tracing data, standard line-lists, and published parameters to train a variety of time-series forecasting models, including an ARIMA model, a Poisson regression, and an SEIR compartmental model. We also calculated the daily R-effective rate as an additional output. The compartmental model best fit the reported cases when tested out of sample, but the statistical models were quicker and easier to deploy and helped inform initial decision-making. The R-effective rate was particularly useful throughout the effort. \n",
       " \n",
       " Overall, these efforts highlighted the importance of rapidly deployable and scalable infectious disease modeling pipelines. Public health data science is still a nascent field, however, so common best practices in other industries are often-times novel approaches in public health. The need for stable, generalizable pipelines is crucial. Using the Databricks platform has allowed us to more quickly scale and iteratively improve our modeling pipelines to include other infectious diseases, such as influenza and RSV. Further development of scalable and standardized approaches to disease forecasting at the state and local level is vital to better informing future public health response efforts.</td></tr><tr><td>Accelerating the Development of Viewership Personas With a Unified Feature Store</td><td>With the proliferation of video content and flourishing consumer demand, there is an enormous opportunity for customer-centric video entertainment companies to use data & analytics to understand what their viewers want and deliver more of the content that that meets their needs.\n",
       " \n",
       " At DIRECTV, our Data Science Center of Excellence is constantly looking to push the boundary of innovation in how we can better and more quickly understand the needs of our customers and leverage those actionable insights to deliver business impact. One way in which we do so is through the development of Viewership Personas with cluster analysis at scale to group our customers by the types of content they enjoy watching. This process is significantly accelerated by a unified feature store which contain a wide array of features that captures key information on viewing preferences.\n",
       " \n",
       " This presentation will focus on how the DIRECTV Data Science team utilizes Databricks to help: (1) develop a unified feature store and (2) how we leverage the feature store to accelerate the process of running machine learning algorithms to find meaningful viewership clusters.</td></tr><tr><td>How You Can Audit A Language Model</td><td>Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\n",
       " \n",
       " Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\n",
       " \n",
       " * Studying past incidents using the AI Incident Database and using this information to guide debugging.\n",
       " \n",
       " * Finding & fixing common data quality issues.\n",
       " \n",
       " * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\n",
       " \n",
       " * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\n",
       " \n",
       " * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\n",
       " \n",
       " * Testing performance, sentiment & toxicity across different & less common languages.\n",
       " \n",
       " * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\n",
       " \n",
       " * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\n",
       " \n",
       " * Engaging stakeholders to help find problems system designers & developers cannot see.\n",
       " \n",
       " It's now time to figure out how to live with AI, and that means audits, risk management & regulation.</td></tr><tr><td>JetBlue’s real-time AI & ML digital twin journey using Databricks</td><td>JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \n",
       " \n",
       " The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code.</td></tr><tr><td>Intermittent Service Part Demand Forecasting at John Deere</td><td>John Deere customers demand reliability and uptime for their equipment and John Deere dealers must fulfill that need by stocking the right mix of service parts in the right quantities to keep them up and running. John Deere dealers operate on six continents with geographically dispersed customers, products, and equipment. This presents a significant operations and logistics challenge to maintain best-in-industry service levels. Improved demand forecasting enables better retail stocking decisions and inventory planning, however, the variety of equipment serviced, diversity of customer segments, and total number of SKUs at dealer retail locations presents a complex demand forecasting challenge.\n",
       " \n",
       " The John Deere Aftermarket Analytics team took on this opportunity and developed forecasts and stocking recommendations for difficult low volume and intermittent demand service parts that comprise a large percentage of dealers' part sales. Building and iterating the solution required development of data pipelines, exploratory analysis and investigation, model training with multiple R and Python packages, experiment tracking in MLFlow, and model deployment. All of this was enabled with John Deere's Enterprise Data Lake paired with Databricks.\n",
       " \n",
       " In this session, learn about:\n",
       " \n",
       " • Practical approaches for intermittent demand forecasting \n",
       " • Leveraging Workspaces and Repos for efficient team collaboration\n",
       " • Developing scalable data pipelines and features for ML models with Delta Lake\n",
       " • Rapid model experimentation with AutoML and MLFlow\n",
       "  • Job orchestration using Workflows</td></tr><tr><td>Scaling AI Applications With Databricks, HuggingFace and Pinecone</td><td>The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\n",
       " In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution. </td></tr><tr><td>Hyperparameter Tuning Via Apache Spark and Ray</td><td>Model selection through hyperparameter tuning (HPT) is a computationally intensive task. Development of specialized hardware, distributed computation, and modern model selection algorithms has allowed a wide range of potential solutions to be evaluated by the data science community. In this session, you will learn more about how Marks and Spencer tackled this task. In examining the tools, you see that Spark UDF and Ray have received considerable attention from the data science community. Spark UDF allows parallelized execution of custom workloads on distributed datasets in a fault tolerant and scalable manner. Ray is an open-source framework for distributed model selection. It features a well-designed, narrow waist API that seamlessly integrates a wide range of hyperparameter search algorithms. Ray creates trials and controls their execution with the help of a scheduler equipped with well-known HPT algorithms allowing Bayesian optimization and Population-based training. </td></tr><tr><td>Large Scale Multi-Task Learning Recommender Service at Verizon</td><td>Millions of customers visit the Verizon company website and stores monthly to shop, from plans to products and accessories. Numerous task-specific recommender models have been built at Verizon to enhance customer experience. Motivated by the latest development in deep learning, multi-task learning architectures can better understand customer behavior therefore greatly improve model performance, and also save time to learn new tasks. However, it’s challenging to build such a model that digests heterogeneous data from purchases, online clicks, store visits to customer services and many more, while also  accomplishing various tasks where some of them might contradict with each other. Motivated by the Pathways vision, we built a novel multi-task learning recommender service called Pathways Recommender Service (PaRS), which directly handles multiple abstract forms of data and is able to generalize across multiple recommendation tasks at Verizon. You will see the that we explored built-in bias mitigation in PaRS to promote diverse, relevant and fair recommendations.</td></tr><tr><td>Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases</td><td>Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases.</td></tr><tr><td>International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale</td><td>International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \n",
       "In this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.</td></tr><tr><td>How MLOps on Databricks helped adidas to gain speed in productionising ML projects</td><td>MLOps enables Data science teams to bring ML models into production environments in an efficient manner while making sure that the model pipeline is maintained and model is monitored continuously. When drift is detected, models are retrained promptly and teams are notified with the availability of new version of model. Our team at adidas has created a ML template that uses Databricks, Jenkins and MLflow, which facilitates in the faster deployment process for our ML models in production. We employ a hybrid deployment strategy to minimize compute costs for large models. This presentation will demonstrate how the template was developed, how our teams are utilizing it for model deployment and how it helped us to gain speed to bring or use cases live.</td></tr><tr><td>How We Built a Unified Talent Solution Using Databricks Machine Learning</td><td>Using Databricks, we built a “Unified Talent Solution”, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta’s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data</td></tr><tr><td>Meet LOLA: The Innovation Engine Brewing Models at Scale for AB-InBev</td><td>Today the world's largest brewer, AB-InBev, is disrupting the industry with BEES and LOLA. LOLA is our machine learning platform that enables us to brew models at scale. The foundation of LOLA rests on Databricks Lakehouse's platform and is now being extended globally with Unity Catalog. However, three years ago, things were different. Back then, LOLA was just a simple recommender with less than 60% account coverage in the US. Now it is evolving into an ML platform that supports more models, like our new recommendation engine DEAR, with 95%+ account coverage and integration with BEES in the US.\n",
       "As LOLA evolves, the foundation laid by Lakehouse paved the way for the cornerstone of our platform, our platinum data layer, the Feature Store. The Feature Store is the hub for all feature engineering in LOLA, using Databricks Workflows to support over 250+ production features. But, more importantly, it has become the abstraction layer that helps build out our growing number of ML pipelines.\n",
       "  \n",
       " </td></tr><tr><td>Scaling MLOps for a demand forecasting across multiple markets for a large CPG</td><td>In this session, we look at how one of the world’s largest CPG company setup a scalable MLOps pipeline for a Demand Forecasting use case that predicted demand at 100k+ DFUs (demand forecasting units) on a weekly basis across 20+ markets. This implementation resulted in significant cost savings in terms of improved productivity, reduced cloud usage and faster time to value amongst other benefits.\n",
       " The attendees of this session will leave this session with a clearer picture on the following:\n",
       " 1. Best practices in scaling mlops with Databricks and Azure for a demand forecasting use case with a multi-market and multi-region roll-out.\n",
       " 2. Best practices related to model re-factoring and setting up standard CI-CD pipelines for MLOps\n",
       " 3. What are some of the pitfalls to avoid in such scenarios?</td></tr><tr><td>MLOps at Gucci: From Zero to Hero</td><td>In recent years, similar principles to those of DevOps in software development have been applied to Machine Learning projects with the goal of productionizing automated solutions. However, Machine Learning Operations (MLOps) practices have proven to be of difficult implementation, as they often require putting together several different tools in a complex architecture. In this session, we introduce MLOps concepts and describe how we implement MLOps principles in our projects at Gucci by leveraging Databricks functionalities. A use case of deploying a data science tool for supporting media budget allocation decisions is presented. After the development of a POC to experiment and effectively address the business problem, the code was versioned and refactored. Code reviews were executed to ensure quality and readability. Within Databricks, we rapidly moved the project to the production stage by achieving an automated solution including unit tests, environment configuration, registration and versioning of models, monitoring of model performances as well as model serving and a dashboard to visualize results. This template is being successfully replicated for other projects and is allowing our new Data Science team to quickly bring value to different business areas of the company. These best practices and insights can be used as an example of how this can be beneficial to you or other practitioners.</td></tr><tr><td>Streamlining API Deployment for ML Models Across Multiple Brands: Ahold Delhaize's Experience on Serverless</td><td>At Ahold Delhaize, we have 19 local brands. Most of our brands have common goals, such as providing personalized offers to their customers, a better search engine on e-commerce websites, and forecasting models to reduce food waste and ensure availability. As a central team, our goal is to standardize the way of working across all of these brands, including the deployment of machine learning models. To this end, we have adopted Databricks as our standard platform for our batch inference models. \n",
       "\n",
       "However, API deployment for real time inference models remained challenging due to the varying capabilities of our brands. Our attempts to standardize API deployments with different tools failed due to complexity of our organization. Fortunately, Databricks has recently introduced a new feature: serverless API deployment. Since all our brands already use Databricks, this feature was easy to adopt. It allows us to easily reuse API deployment across all of our brands, significantly reducing time to market (from 6-12 months to 1 month), increasing efficiency, and reducing the costs. In this session, you will see the solution architecture, sample use case specifically used to cross-sell model deployed to 4 different brands, and API deployment using Databricks Serverless API with custom model.</td></tr><tr><td>Monetizing Data Assets: Sharing Data, Models and Features</td><td>Exec Summary:\n",
       " \n",
       " - Data is an asset and selling/sharing data has (largely) been solved\n",
       " - Hosted models exist (example: ChatGPT) but moving sensitive data across the public internet or across clouds is problematic\n",
       " - Sharing features (the result of feature engineering) can be monetized for new potential revenue streams\n",
       " - Sharing models can also be monetized while avoiding the transfer of sensitive data\n",
       " - This presentation will walk through a few examples of how to share models and features to generate new revenue streams using Delta Sharing, MLflow, and Databricks</td></tr><tr><td>An API for DL Inferencing on Spark</td><td>Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization.\n",
       " \n",
       " In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models.</td></tr><tr><td>DEIMOS - Optimizing Time-Series Modeling at Scale With Applications in CPG</td><td>At the beginning of a forecasting project, data scientists must make several decisions, each of which impacts the success of a project. Should the model be univariate or multivariate? What features warrant the best model inputs? What model type should you choose? Are the best hyperparameters selected? All of these parts of the modeling process can be quite costly in terms of both time and computational resources. In this session, you will see solution we've created called DEIMOS (Deep Experimental Integrated Multivariate Optimized Series). DEIMOS is built to handle many of the critical problems in time-series modeling automatically and at scale. This package is designed to be simple and straightforward to use, with easily interpretable results to both technical and non-technical audiences. Moreover, we’ve designed it to be flexible so that it can be easily applied to many use cases.</td></tr><tr><td>Comparing Databricks and Snowflake for Machine Learning</td><td>Snowflake and Databricks both aim to provide data science toolkits for machine learning workflows, albeit with different approaches and resources. While developing ML models is technically possible using either platform, the Hitachi Solutions Empower team tested which solution will be easier, faster, and cheaper to work with in terms of both user experience and business outcomes for our customers. To do this, we designed and conducted a series of experiments with use cases from the TPCx-AI benchmark standard. We developed both single-node and multi-node versions of these experiments, which sometimes required us to set up separate compute infrastructure outside of the platform, in the case of Snowflake. We also built datasets of various sizes (1GB, 10GB, and 100GB), to assess how each platform/node setup handles scale. Based on our findings. On the average, Databricks is faster, cheaper, and easier to use for developing machine learning models, and we use it exclusively for data science on the Empower platform. Snowflake’s reliance on third party resources for distributed training is a major drawback, and the need to use multiple compute environments to scale up training is complex and, in our view, an unnecessary complication to achieve best results.</td></tr><tr><td>Testing Generative AI Models - What You Need to Know</td><td>Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\n",
       " \n",
       " But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\n",
       " \n",
       " In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.</td></tr><tr><td>Simplifying Real-Time Machine Learning: A Look at Feature Platforms and Modern Real-time ML Architectures Using MLflow & Tecton</td><td>Are you struggling to keep up with the demands of real-time machine learning? Like most organizations building real-time ML, you’re probably looking for a better way to: Manage the lifecycle of ML models and features, Implement batch, streaming, and real-time data pipelines, Generate accurate training datasets and Serve models and data online with strict SLAs, supporting millisecond latencies and high query volumes. Look no further! In this session, we will unveil a modern technical architecture that simplifies the process of managing real-time ML models and features. Using MLFlow and Tecton, we’ll show you how to build a robust MLOps platform on Databricks that can easily handle the unique challenges of real-time data processing. Join us to discover how to streamline the lifecycle of ML models and features, implement data pipelines with ease, and generate accurate training datasets with minimal effort. See how to serve models and data online with mission-critical speed and reliability, supporting millisecond latencies and high query volumes. Take a firsthand look at how FanDuel uses this solution to power their real-time ML applications, from responsible gaming to content recommendations and marketing optimization. See for yourself how this system can be used to define features, train models, process streaming data, and serve both models and features online for real-time inference with a live demo. Join us to learn how to build a modern MLOps platform for your real-time ML use cases.</td></tr><tr><td>Building a Real-Time Model Monitoring Pipeline on Databricks</td><td>Model deployment is almost never the final step in any ML lifecycle. ML models can degrade over time due to a variety of influencing factors. In this technical deep dive, we will build a real-time ML model monitoring pipeline on Databricks. We need to own and monitor our models for drifts like feature drift, concept drift, distribution drift, and so on. We must constantly monitor the models and issue alerts or trigger retraining when necessary. With so many open source tools and frameworks available, it can be difficult to figure out how to make everything work. In this tutorial, we will create a high-quality Model Monitoring Pipeline. Everything will be built from the ground up using Spark on Databricks. In this session we will introduce a use case in which we set up a model serving pipeline and log the predictions to a stream in real time. We will then configure a model metric monitoring pipeline to consume from the stream and aggregate over specific time windows. Then, to see these metrics live on dashboards, we will integrate a model monitoring visualizing pipeline.</td></tr><tr><td>Using NLP to evaluate 100 Million global webpages daily to contextually target consumers</td><td>This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\n",
       "\n",
       "TTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\n",
       "\n",
       "The Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\n",
       "</td></tr><tr><td>How Office Leverages Deep Graph Learning to Improve Productivity Products</td><td>We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale.</td></tr><tr><td>Stable Diffusion: The Future of Generative AI</td><td>Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering.</td></tr><tr><td>Vector Data Lakes</td><td>Vector databases such as ElasticSearch and Pinecone offer fast ingestion and querying on vector embeddings with ANNs. However, they typically do not decouple compute and storage, making them hard to integrate in production data stacks. Because data storage in these databases is expensive and not easily accessible, data teams typically maintain ETL pipelines to offload historical embedding data to blob stores. When that data needs to be queried, they get loaded back into the vector database in another ETL process. This is reminiscent of loading data from OLTP database to cloud storage, then loading said data into an OLAP warehouse for offline analytics. Recently, “lakehouse” offerings allow direct OLAP querying on cloud storage, removing the need for the second ETL step. The same could be done for embedding data. While embedding storage in blob stores cannot satisfy the high TPS requirements in online settings, we argue it’s sufficient for offline analytics use cases like slicing and dicing data based on embedding clusters. Instead of loading the embedding data back into the vector database for offline analytics, we propose direct processing on embeddings stored in Parquet files in Delta Lake. You will see that offline embedding workloads typically touch a large portion of the stored embeddings without the need for random access. As a result, the workload is entirely bound by network throughput instead of latency, making it quite suitable for blob storage backends. On a test 1 billion vector dataset, ETL into cloud storage takes around 1 hour on a dedicated GPU instance, while batched nearest neighbor search can be done in under one minute with four CPU instances. We believe future “lakehouses” will ship with native support for these embedding workloads.</td></tr><tr><td>JoinBoost: In-DB ML for Tree-Models</td><td>Data and Machine Learning (ML) are crucial for enterprise operations. Enterprises store data in databases for management and use ML to gain business insights. However, there is a mismatch between the way ML expects data to be organized (a single table) and the way data is organized in databases (a join graph of multiple tables) and leads to inefficiencies when joining and materializing tables. In this session, you will see how we successfully address this issue. We introduce JoinBoost, a lightweight python library that trains tree models (such as random forests and gradient boosting) for join graphs in databases. JoinBoost acts as a query rewriting layer that is compatible with cloud databases, and eliminates the need for costly join materialization.</td></tr><tr><td>Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP</td><td>In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.</td></tr><tr><td>Data Caching Strategies for Data Analytics and AI</td><td>The increasing popularity of data analytics and artificial intelligence (AI) has led to a dramatic increase in the volume of data being used in these fields, creating a growing need for an enhanced computational capability. Cache plays a crucial role as an accelerator for data and AI computations, but it is important to note that these domains have different data access patterns, requiring different cache strategies. In this session, you will see our observations on data access patterns in the analytical SQL and AI training domains based on practical experience with large-scale systems. We will discuss the evaluation results of various caching strategies for analytical SQL and AI and provide caching recommendations for different use cases. Over the years, we have learned some best practices from big internet companies about the following aspects of our journey: 1. Traffic pattern for analytical SQL and cache strategy recommendation, 2. Traffic pattern for AI training and how we can measure the cache efficiency for different AI training process, 3. Cache capacity planning based on real-time metrics of the working set, and 4. Adaptive caching admission and eviction for uncertain traffic patterns</td></tr><tr><td>If a Duck Quacks in the Forrest and Everyone Hears, Should you Care?</td><td>\"YES! \"\"Duck posting\"\" has become an internet meme for praising DuckDB on twitter. Nearly every quack using DuckDB has done it once or twice. But</td></tr><tr><td>Python with Spark Connect</td><td>PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python.\n",
       " \n",
       " With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today’s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example.\n",
       " \n",
       " In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications.\n",
       " \n",
       " In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what’s next beyond Apache Spark 3.4.</td></tr><tr><td>Databricks Connect powered by Spark Connect: Develop and Debug Spark from any developer tool</td><td>Spark developers want to develop and debug their code using their tools of choice and development best practices while ensuring high-production fidelity on the target remote cluster. However, Spark's driver architecture is monolithic, with no built-in capability to directly connect to a remote Spark cluster from languages other than SQL. This makes it hard to enable such interactive developer experiences from a user’s local IDE of choice. Spark Connect’s decoupled client-server architecture introduces remote connectivity to Spark clusters and with that, enables interactive development experience - Spark and its open ecosystem can be leveraged from everywhere! \n",
       " \n",
       " In this talk, we show how we leverage Spark Connect to build a completely redesigned version of Databricks Connect, a first-class IDE-based developer experience that offers interactive debugging from any IDE. We show how developers can easily ensure consistency between their local and remote environments. We walk the audience through real-live examples of how to locally debug code running on Databrick. We also show how Databricks Connect integrates into the Databricks Visual Studio Code extension for an even better developer experience.</td></tr><tr><td>Ray on Spark</td><td>Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale.\n",
       " \n",
       " This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. \n",
       " \n",
       " In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. \n",
       " We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. \n",
       " During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change!\n",
       " We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases.</td></tr><tr><td>Why Delta Lake is the best storage format for pandas analyses</td><td>pandas analyses are often limited by file formats like CSV and Parquet. CSV doesn't allow for column pruning, which is an important performance optimization. Parquet doesn't allow for critical features like ACID transactions, time travel, and schema enforcement. This talk discusses why Delta Lake is the fastest file format for pandas users and how it provides users with great features.</td></tr><tr><td>Fine tuning & scaling Hugging Face with Ray AIR</td><td>Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n",
       " \n",
       " The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\n",
       " \n",
       " In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models’ training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n",
       " \n",
       " Key Takeaways: \n",
       " * Python developers and machine learning engineers can use Transformers and scale their language models\n",
       " * Get exposed to Ray AIR’s Python APIs for end-to-end Hugging Face and ML workflow\n",
       " * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads</td></tr><tr><td>Breaking Barriers with Databricks Lakehouse - How Blackberry is Revolutionizing Cybersecurity Services Worldwide. AI-Driven Cybersecurity that Works Smarter, Not Harder.\n",
       "</td><td>Cybersecurity incidents are costly, and using an Endpoint Detection and Response (EDR) solution enables the detection of cybersecurity incidents as quickly as possible. To effectively detect cybersecurity incidences requires the collection of millions of data points, and the storing/querying of endpoints data presents considerable engineering challenges. This includes quickly moving local data from endpoints to a single table in the cloud and enabling performant querying against it. The need to avoid internal data siloing within BlackBerry was paramount as multiple teams required access to the data to deliver an effective EDR solution for the present and the future. Databricks tooling enabled us to break down our data silos and iteratively improve our EDR pipeline to ingest data faster and reduce querying latency by more than 20% while reducing costs by more than 30%.\n",
       "In this session, BlackBerry Engineers Srinivasa Kanamatha and Justin Lai will share the journey, lessons learned, and the future for collecting, storing, governing, and sharing data from endpoints in Databricks. The result of building EDR using Databricks helped us accelerate the deployment of our data platform.\"\"</td></tr><tr><td>Scaling Deep Learning using Delta Lake storage format on Databricks</td><td>Delta Lake is an open-source storage format that can be ideally used for storing large-scale datasets, which can be used for single-node and distributed training of deep learning models. Delta Lake storage format gives deep learning practitioners unique data management capabilities for working with their datasets. \n",
       " The challenge is that, as of now, it’s not possible to use Delta Lake to train PyTorch models directly. PyTorch community has recently introduced a Torchdata library for efficient data loading. This library supports many formats out of the box, but not Delta Lake. This talk will demonstrate using the Delta Lake storage format for single-node and distributed PyTorch training using the torchdata framework and standalone delta-rs Delta Lake implementation.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse",
         "Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\n \n Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \n \n These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\n - Modular and composable code\n - Unit testing framework\n - Incremental event processing with spark structured streaming \n - Granular resource tuning for better performance and cost efficacy\n \n Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse."
        ],
        [
         "Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System",
         "Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community."
        ],
        [
         "From Snowflake to Enterprise-Scale Apache Spark™",
         "Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers.\n \n The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™  solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs.\n \n In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to:\n \n * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation\n * Handle X100 queries per second on a single Spark application with sub-second query latency\n * Protect Spark application from misbehaving users \n * Optimize SQL-based queries"
        ],
        [
         "The Future of Data Orchestration: Asset-Based Orchestration",
         "Data orchestration is a core component for any batch data processing platform and we’ve been using patterns that haven't changed since the 1980s. \n \n In this session, I’ll be introducing a new pattern and way of thinking for data orchestration known as asset-based orchestration, with data freshness sensors to trigger pipelines. I will demo this new pattern using popular tools of the modern data stack - dbt, airbyte, dagster, and databricks."
        ],
        [
         "Photon for Dummies: How Does this New Execution Engine Actually Work?",
         "Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it.\n \n If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. \n \n Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as:\n - Why is a Java Virtual Machine the current bottleneck for spark enhancements?\n - What does vectorized even mean? And how was it done before?\n - Why is the relationship status between Spark and Photon 'It’s complicated'?\n \n In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. \n \n I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to. \n \n "
        ],
        [
         "Monitoring Delta Live Tables",
         "In this session we will share how Volvo Group monitors Databricks jobs and DLTs to stay ahead of the curve. Volvo Group uses Databricks for - amongst others - material planning. Based on live input from warehouse systems, the system predicts which orders to place with the suppliers to ensure the availability of spare parts and keep the trucks running. Since this is a critical component, with reverse ETL feeding the recommendation back into the warehouse systems, strict monitoring to avoid pipeline congestion is required. In this talk I will:\n * Explain a vision of data quality (deliver trustworthy data in time); which metrics to set up to monitor data quality\n * How it helps Volvo Group; make the SLA, jointly use with FinOps to improve ingestion pipelines, avoid congestion\n * Setup; use Jobs API and Databricks SQL workspace to build and consult the monitoring dashboard\n * Demo\n * Get notebook from GitHub"
        ],
        [
         "Data Quality: Fast and Slow",
         "Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \n \n In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\n \n This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical."
        ],
        [
         "Taking Your Cloud Vendor to the Next Level: Solving Complex Challenges with Azure Databricks",
         "Akamai's content delivery network (CDN) processes about 30% of the internet's daily traffic, resulting in a massive amount of data that presents engineering challenges, both internally and with cloud vendors. In this session, we will discuss the barriers faced while building a data infrastructure on Azure, Databricks, and Kafka to meet strict SLAs, hitting the limits of some of our cloud vendors’ services. We will describe the iterative process of re-architecting a massive-scale data platform using the aforementioned technologies. We will also delve into how today, Akamai is able to quickly ingest and make available to customers TBs of data, as well as efficiently query PBs of data and return results within 10 seconds for most queries. This discussion will provide valuable insights for attendees and organizations seeking to effectively process and analyze large amounts of data."
        ],
        [
         "Unleashing the Power of Interactive Analytics at Scale with Databricks and Delta Lake: Lessons Learned from Building Akamai's Web Security Analytics Product",
         "Akamai is a leading content delivery network (CDN) and cybersecurity company, operating hundreds of thousands of servers in more than 135 countries worldwide.\n In this talk, we will share our experiences and lessons learned from building and maintaining the Web Security Analytics (WSA) product, an interactive analytics platform powered by Databricks and Delta Lake, that enables customers to efficiently analyze and take informed action on a high volume of streaming security events. \n \n The WSA platform must be able to serve hundreds of queries per minute, scanning hundreds of terabytes of data from a six petabyte data lake, with most queries returning results within ten seconds (for both aggregation queries and needle in a haystack queries).\n The talk will cover how to use Databricks SQL warehouses and job clusters cost-effectively, and how to improve query performance through the use of tools and techniques such as Delta Lake, Databricks Photon, and partitioning. \n This talk will be valuable for anyone looking to build and operate a high-performance analytics platform."
        ],
        [
         "ABN Story: Migrating to Future Proof Data Platform",
         "ABN AMRO Bank is one of the top leading banks in the Netherlands; it is the 3rd largest bank in the Netherlands by revenue and number when it comes to mortgages within the Netherlands. We have an objective to become a fully data-driven bank and this goal is well supported by our top management.\n \n ABN AMRO started its data journey almost seven years ago and has built a data platform off-premises with Hadoop technologies. This data platform has been used by more than 200 data providers, 150 data consumers, and overall more than 3000 Datasets.\n \n To become a fully digital bank and address the limitation of the on-premises platform, we needed a future-proof data platform DIAL (digital integration and access layer.) ABN AMRO decided to build an Azure cloud-native data platform with the help of Microsoft and Databricks. Last year this cloud-native platform was ready for our data providers and data consumers. Six months ago we started the journey of migrating all the content from the on-premises data platform to the Azure data platform, this was a very large-scale migration and was achieved in 6 months.\n \n In this session, we will focus on two things :\n 1. Share our strategy for migration from on-premises to a cloud-native platform. \n 2. Share how we used Databricks products in our data platform and how the Databricks team helped us in the overall migration.\n \n "
        ],
        [
         "Simon + Denny Live: Ask Us Anything",
         "Simon and Denny have been discussing and debating all things Delta, Lakehouse and Apache Spark™ on their regular webshow. Whether you want advice on lake structures, want to hear their opinions on the latest trends and hype in the data world, or you simply have a tech implementation question to throw at two seasoned experts, these two will have something to say on the matter. In their previous shows, Simon and Denny focused on building out a sample lakehouse architecture, refactoring and tinkering as new features came out, but now we're throwing the doors open for any and every question you might have.\n \n So if you've had a niggling question and these two can help, this is the session for you. There will be a question submission form shared prior to the event, so the team will be prepped with a whole bunch of topics to talk through. Simon and Denny want to hear YOUR questions, which they can field from a wealth of industry experience, wide ranging community engagement and their differing perspectives as external consultant and internal Databricks respectively. There's also a chance they'll get distracted and go way off track talking about coffee, sci-fi, nerdery or the English weather. It happens."
        ],
        [
         "Sometimes Less is More: A Deep Dive into the Evolution of a Large, Complex, Real-Time Data Pipeline",
         "This session will dive into technical detail around one of my complex cybersecurity endpoint detection and response (EDR) data pipelines - which spans multiple industries - that is being joined with an indicator of compromise (IoC) feed. I'll detail the stages we went through in the initial evolution of the pipeline and why it ultimately made more sense to apply less schema structure to it and focus on using a non-obvious partitioning strategy when working with this data source. All code is in Python/Pyspark and uses Delta Live Tables."
        ],
        [
         "Use Apache Spark™  from Anywhere: Remote connectivity with Spark Connect",
         "Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark™. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\n \n However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\n \n Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages.\n \n This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere."
        ],
        [
         "Seven Things You Didn't Know You Can Do with Databricks Workflows",
         "Databricks workflows has come a long way since the initial days of orchestrating simple notebooks and jar/wheel files. Now we can orchestrate multi-task jobs and create a chain of tasks with lineage and DAG with either fan-in or fan-out among multiple other patterns or even run another Databricks job directly inside another job. Databricks workflows takes its tag: “orchestrate anything anywhere” pretty seriously and is a truly fully-managed, cloud-native orchestrator to orchestrate diverse workloads like Delta Live Tables, SQL, Notebooks, Jars, Python Wheels, dbt, SQL, Apache Spark™, ML pipelines with excellent monitoring, alerting and observability capabilities as well. Basically it is a one-stop product for all orchestration needs for an efficient Lakehouse. And what is even better is, it gives full flexibility of running your jobs in a cloud-agnostic and cloud-independent way and is available across AWS, Azure and GCP. In this talk we will discuss and deep dive on some of the very interesting features and will showcase end-to-end demos of the features which will allow you to take full advantage of Databricks workflows for orchestrating the Lakehouse."
        ],
        [
         "Using DMS and DLT for Change Data Capture ",
         "[Abstract TBD]"
        ],
        [
         "Deep Dive into the New Features of Apache Spark™  3.4",
         "In 2022, Apache Spark™ was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. \n \n We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling."
        ],
        [
         "Delta Live Tables: A Modern Approach to Data Pipelines",
         "Data engineers have the difficult task of cleansing complex, diverse data, and transforming it into a usable source to drive data analytics, data science, and machine learning. They need to know the data infrastructure platform in depth, build complex queries in various languages and stitch them together for production. Join this session to learn how Delta Live Tables (DLT) simplifies the complexity of data transformation and ETL. DLT is the first ETL framework to use modern software engineering practices to deliver reliable and trusted data pipelines at any scale. Discover how analysts and data engineers can innovate rapidly with simple pipeline development and maintenance, how to remove operational complexity by automating administrative tasks and gaining visibility into pipeline operations, how built-in quality controls and monitoring ensure accurate BI, data science, and ML, and how simplified batch and streaming can be implemented with self-optimizing and auto-scaling data pipelines."
        ],
        [
         "Journey Towards Uniting Metastores",
         "This session will provide a brief overview about Nationwide’s journey towards implementing Unity Catalog at an Enterprise Level. We will cover the following topics.\n - Identity management structure\n - Compute framework\n - Naming standards and usage best practices\n - And a little bit about how Delta Sharing will help us ingest 3rd party data\n Unity Catalog has been a core feature towards strengthening Data Lakehouse architecture for multiple business units."
        ],
        [
         "How CIBC Integrated Purview and Unity Catalog for Our Data Governance Within a Data Mesh Architecture Leveraging the Lakehouse",
         "This session will focus on our journey to integrate Purview and Unity Catalog for our data governance within data mesh architecture, while leveraging the lakehouse. Our approach is to build four foundational pillars of data mesh that will uphold data management practices based on consumer-driven and domain-centric data sources. \n Principle 1: decentralized ownership\n Principle 2: federated governance\n Principle 3: data as a product\n Principle 4: self-service infrastructure\n Native access controls within the self-service infrastructure are enhanced by pushing policy defined access to unity to enable fine-grain control across the data landscape."
        ],
        [
         "The Future of Data Access Control: Booz Allen Hamilton’s Approach to Securing our Databricks Lakehouse with Immuta",
         "In this session, I’ll review how we utilize Attribute-Based Access Control (ABAC) to enforce policy via Immuta. I’ll discuss the differences between the ABAC and legacy Role-Based Access Control (RBAC) approaches to control access and how the RBAC approach is not sufficient to keep up with today’s growing big data market. With so much data available, there also comes substantial risk. Data can contain many sensitive data elements, including PII and PHI. Industry leaders like Databricks are pushing the boundaries of data technology, which leads to constantly evolving data use cases. And that’s a good thing! However, the RBAC approach is struggling to keep up with those advancements.\n So what is RBAC? It’s an approach to data access that permits system access based on the end-user’s role. For legacy systems, it’s meant as a simple but effective approach to securing data. Are you a manager? Then you’ll get access to data meant for managers. This is great for small deployments with clearly defined roles. Here at Booz Allen, we invested in Databricks because we have an environment of over 30k users and billions of rows of data.\n To mitigate this problem and align with our forward-thinking company standard, we introduced Immuta into our stack. Immuta uses ABAC to allow for dynamic data access control. Users are automatically assigned certain attributes, and access is based on those attributes instead of just their role. This allows for more flexibility and allows data access control to easily scale without the need to constantly map a user to their role. Using attributes, we can write policies in one place and have them applied across all of our data platforms. This makes for a truly holistic data governance approach and provides immediate ROI and time savings for the company."
        ],
        [
         "Increasing Trust in Your Data: Enabling a Data Governance Program on Databricks Using Unity Catalog and ML-Driven MDM",
         "As part of Comcast Effectv’s transformation into a completely digital advertising agency, it was key to develop an approach to manage and remediate data quality issues related to customer data so that the sales organization is using reliable data to enable data-driven decision making. Like many organizations, Effectv's customer lifecycle processes are spread across many systems utilizing various integrations between them. This results in key challenges like duplicate and redundant customer data that requires rationalization and remediation. Data is at the core of Effectv’s modernization journey with the intended result of winning more business, accelerating order fulfillment, reducing make-goods and identifying revenue.\n  \nIn partnership with Slalom Consulting, Comcast Effectv built a traditional lakehouse on Databricks to ingest data from all of these systems but with a twist; they anchored every engineering decision in how it will enable their data governance program. \n  \nIn this talk, we will touch upon the data transformation journey at Effectv and dive deeper into the implementation of data governance leveraging Databricks solutions such as Delta Lake, Unity Catalog and DB SQL. Key focus areas include how we baked master data management into our pipelines by automating the matching and survivorship process, and bringing it all together for the data consumer via DBSQL to use our certified assets in bronze, silver and gold layers.\n  \nBy making thoughtful decisions about structuring data in Unity Catalog and baking MDM into ETL pipelines, you can greatly increase the quality, reliability, and adoption of single-source-of-truth data so your business users can stop spending cycles on wrangling data and spend more time developing actionable insights for your business."
        ],
        [
         "Activate Your Lakehouse with Unity Catalog",
         "Building a Lakehouse is straightforward today thanks to many open-source technologies and Databricks. However, it can be taxing to extract value from Lakehouses as they grow in size without robust data operations. Join us to learn how YipitData uses the Unity Catalog to streamline data operations and discover best practices to scale your own Lakehouse.\n \n At YipitData, our 15+ petabyte Lakehouse is a self-service data platform built with Databricks and AWS, supporting analytics for a 250+ data team. We will share how leveraging Unity Catalog accelerates our mission to help financial institutions and corporations leverage alternative data by:\n \n - Enabling clients to universally access our data through a spectrum of channels, including Sigma, Delta Sharing, and multiple clouds \n - Fostering collaboration across internal teams using a data mesh paradigm that yields rich insights\n - Strengthening the integrity and security of data assets through ACLs, data lineage, audit logs, and further isolation of AWS resources\n - Reducing the cost of large tables without downtime through automated data expiration and ETL optimizations on managed delta tables\n \n Through our migration to Unity Catalog, we have gained tactics and philosophies to seamlessly flow our data assets internally and externally. Data platforms need to be value-generating, secure, and cost-effective in today's world. We are excited to share how Unity Catalog delivers on this and helps you get the most out of your Lakehouse."
        ],
        [
         "Self Service Data Analytics and governance at enterprise scale with unity catalog",
         "This session focuses on one of the first Unity Catalog implementations for a large-scale enterprise. In this scenario, a cloud scale analytics platform with 7500 active users based on the lakehouse approach is used. In addition, there is potential for 1500 further users who are subject to special governance rules. They are consuming 600+TB of data stored in Delta Lake - continuously growing at more than 1TB per day. This might grow due to local country data.\n Therefore, the existing data platform must be extended to enable users to combine global and local data from their countries. A new data management was required, which reflects the strict information security rules at a need to know base. Core requirements are: read only from global data, write into local and share the results.\n Due to a very pronounced information security awareness and a lack of the technological possibilities it was not possible to interdisciplinary analyze and exchange data so easy or at all so far. Therefore, a lot of business potential and gains could not be identified and realized.\n With the new developments in the technology used and the basis of the Lakehouse approach, thanks to Unity Catalog, we were able to develop a solution that could meet high requirements for security and process. And enables globally secured interdisciplinary data exchange and analysis at scale.\n This solution enables the democratization of the data. This results not only in the ability to gain better insights for business management, but also to generate entirely new business cases or products that require a higher degree of data integration and encourage the culture to change.\n We highlight technical challenges and solutions, present best practices and point out benefits of implementing Unity catalog for enterprises."
        ],
        [
         "PII Detection at Scale on the Lakehouse",
         "SEEK is Australia’s largest online employment marketplace and a market leader spanning ten countries across Asia Pacific and Latin America. SEEK provides employment opportunities for roughly 16 million monthly active users and process 25 million candidate applications to listings.\n \n Processing millions of resumès involves handling and managing highly sensitive candidate information, usually inputted in a highly unstructured format. With recent high-profile data leaks in Australia, personally identifiable information (PII) protection has become a major focus area for large digital organizations. The first step is detection, and SEEK has developed a custom framework built using HuggingFace transformers fine tuned with nuances around employment. For example, “Software Engineer at Databricks” is not PII, but “CEO at Databricks” is PII. \n \n After identifying and anonymizing PII in stream and batch data, SEEK uses Unity Catalog’s data lineage to track PII through their reporting, ETL, and other downstream ML use-cases and govern access control achieving an organization-wide data management capability driven by deep learning and enforcement using Databricks."
        ],
        [
         "Essential Data Security Strategies for the Modern Enterprise Data Architecture",
         "Balancing critical data requirements is a 24-7 task for enterprise-level organizations that must straddle the need to open specific gates to enable self-service data access while closing other access points to maintain internal and external compliance. Data breaches can cost U.S. businesses an average of $9.4 million per occurrence; ignoring this leaves organizations vulnerable to severe losses and crippling costs. \n \nThe 2022 Gartner Hype Cycle for Data Security reports that more and more enterprises are modernizing their data architecture with cloud and technology partners to help them collect, store and manage business data; a trend that does not appear to be letting up.\n \nAccording to Gartner®, “by 2025, 30% of enterprises will have adopted bDSP (Broad Data Security Platform), up from less than 10% in 2021, due to the pent-up demand for higher levels of data security and the rapid increase in product capabilities.\""
        ],
        [
         " \nMoving to both a modern data architecture and data-driven culture sets enterprises on the right trajectory for growth",
         " but it’s important to keep in mind individual public cloud platforms are not guaranteed to protect and secure data. To solve this"
        ],
        [
         "Using Open-Source Tools to Build Privacy-Conscious Data Systems",
         "With the rapid proliferation of consumer data privacy laws across the world, it is becoming a strict requirement for data organizations to be mindful of data privacy risks. Privacy violation fines are reaching record highs and will only get higher as governments continue to crack down on the runaway abuse of user data. To continue producing value without becoming a liability, data systems must include privacy protections at a foundational level.\n \n The most practical way to do this is to enable privacy as code, shifting privacy left and including it as a foundational part of the organization's software development life cycle. The promise of privacy as code is that data organizations can be liberated from inefficient, manual workflows for producing the compliance deliverables their legal teams need, and instead ship at speed with pre-defined privacy guardrails built into the structure of their preferred workflows.\n \n Despite being an emerging and complex problem, there are already powerful open source tools available designed to help organizations of all sizes achieve this outcome. Fides is an open source privacy as code tool, written in Python and Typescript, that is engineered to tackle a variety of privacy problems throughout the application lifecycle. The most relevant feature for data organizations is the ability to annotate systems and their datasets with data privacy metadata, thus enabling automatic rejection of dangerous or illegal uses. Fides empowers data organizations to be proactive, not reactive, in terms of protecting user privacy and reducing organizational risk. Moving forward data privacy will need to be top of mind for data teams."
        ],
        [
         "Leveraging Unity Catalog for Data Governance for Grab’s Use Case",
         "Grab has been looking for a tool that provides data access to over 2000 users to support our daily operations. This tool should provide granular access control and easily scale to over thousands of users and tables, with data governance and security in mind.\n \n Through various evaluations, Grab concluded that Unity Catalog is a good fit that addresses all the requirements of data governance and security. For example, it provides row-level-security (RLS) and dynamic-data-masking (DDM) capabilities (through dynamic views), centralized governance and security at both table and object level. \n This ensures sensitive PII data is well protected, and we are able to open up tables to a wider audience ensuring only entitled data is shown. More importantly, we are able to meet the required auditing standards.\n \n With Unity catalog, we managed to:\n 1. Build a custom solution that allows users to seamlessly access data with UC.\n 2. Easily interface and integrate into existing Grab’s in-house entitlement system to translate access requirements, unifying the end-to-end user experience for requesting data access permission. \n 3. Extend UC’s capability by leveraging existing UC APIs and Databricks workflows to easily automate administrative tasks. For example, we have fully automated the creation of RLS and DDM views on-the-fly.\n 4. Provide out of the box, comprehensive auditing capabilities easily.\n \n As a result, we managed to achieve this in a short period of time to support over 2000 users to achieve better governance and significant cost savings with an ROI of about 2.7x"
        ],
        [
         "Unity Catalog: Flexibility to Fit Your Organization",
         "Databricks Unity Catalog provides the flexibility to meet the needs of any-sized organization, from startups to companies the size of Nike. \n \n Unity Catalog and lakehouse architecture have been chosen by Nike to store and process all of Nike's data. Unity Catalog has resolved systemic challenges in the Data Lake, resulting in 1) a simpler and transparent Data Lake, and, 2) allowing for scaled data and analytics use cases with greater efficiency. \n \n Designing Unity Catalog for the scale of Nike met the following goals:\n \n * Ownership of data: how all data produced in the lake has clear team ownership, communication channels, and accountability\n * Data environments: accelerated development of pipelines against real Production Data with zero risk\n * Consistent team semantics: all teams on Lakehouse follow the same semantics, allowing for scaled onboarding and consistent development expectations\n * Consistent governed data access: a single simplified, governed, data access process for all data in the Lake\n * Monitoring of unused data and data pipelines: simplified observability through Unity Catalog Data Lineage to quickly identify any unused data or data pipelines, resulting in less storage and compute costs\n \n These capabilities are powering Nike as a data driven organization, allowing the company to quickly make informed decisions."
        ],
        [
         "Automating Sensitive Data (PII/PHI) Detection and Quarantining to a Databricks Clean Room",
         "Healthcare datasets contain both personally identifiable information (PII) and personal health information (PHI) that needs to be de-identified in order to protect patient confidentiality and ensure HIPAA compliance. This privacy data is easily detected when it’s provided in columns  labeled with names such as “SSN,” First Name,” “Full Name,” and “DOB;” however, it is much harder to detect when it is hidden within columns labeled “Doctor Notes,” “Diagnoses,” or “Comments.” HealthVerity, a leader in the HIPAA-compliant exchange of real-world data (RWD) to uncover patient, payer and genomic insights and power innovation for the healthcare industry, ensures healthcare datasets are de-identified from PII and PHI using elaborate privacy procedures. During this presentation, we will demonstrate how to use a low-code/no-code platform to simplify and automate data pipelines that leverage prebuilt ML models to scan data for PHI/PII leakage and quarantine those rows in Unity Catalog when leakage is identified and move them to a Databricks clean room for analysis."
        ],
        [
         "Cross-Platform Data Lineage with OpenLineage",
         "There are more data tools available than ever before, and it's easier to build a pipeline than it's ever been. This has resulted in an explosion of innovation, but it also means that data within today's organizations has become increasingly distributed. It can't be contained within a single brain, a single team, or a single platform. Data lineage can help by tracing the relationships between datasets and providing a map of your entire data universe. OpenLineage provides a standard for lineage collection that spans multiple platforms, including Apache Airflow, Apache Spark™, Flink®, and dbt. This empowers teams to diagnose and address widespread data quality and efficiency issues in real time. In this session, Julien Le Dem from Astronomer will show how to trace data lineage across Apache Spark and Apache Airflow. He will walk through the OpenLineage architecture and provide a live demo of a running pipeline with real-time data lineage."
        ],
        [
         "Ahold Delhaize's Journey to Implementing Unity Catalog at Scale",
         "In this session we will explore the steps we took to implement Unity Catalog at Ahold Delhaize's data platform. This powerful tool has completely transformed the way we access and manage data objects, making it easier than ever for our entire organization to find and use the data we need. The catalog brings us a governance layer that allows us to more securely and easily share restricted or confidential data by providing row and column level security per project. But that's not all, we're also setting up the Unity Catalog as a self-service tool, allowing users to easily customize and set up their own catalogs, schema's, views or user groups using simple YAML configuration files. We will cover topics such as:\n * Architecture where we came from and the improvement Unity Catalog has made\n * Our self-service way of working by showing code snippets\n * Our fully automatic deployments by explaining our flow\n By the end of this talk, you will have a better understanding of the process of implementing Unity Catalog at scale."
        ],
        [
         "Multi-Cloud Data Governance on the Databricks Lakehouse",
         "Across industries, a multi-cloud setup has quickly become the reality for large organizations. Multi-cloud introduces new governance challenges as permissions models often do not translate from one cloud to the other and if they do, are insufficiently granular to accommodate privacy requirements and principles of least privilege. This problem can be especially acute for Data and AI workloads that rely on sharing and aggregating large and diverse data sources across business unit boundaries and where governance models need to incorporate assets such as table rows/columns and ML features and models. \n \n In this session we will provide guidelines on how best to overcome these challenges for companies that have adopted the Databricks Lakehouse as their collaborative space for data teams across the organization, by exploiting some of the unique product features of the Databricks platform.\n We will focus on a common scenario: a data platform team providing data assets to two different ML teams, one using the same cloud and the other one using a different cloud. We will explain the step-by-step setup of a unified governance model by leveraging the following components and conventions:\n \n - Unity Catalog for implementing fine-grained access control across all data assets: files in cloud storage, rows and columns in tables and ML features and models\n - The Databricks Terraform provider to automatically enforce guardrails and permissions across clouds \n - Account level SSO Integration and identity federation to centralize administer access across workspaces\n - Delta sharing to seamlessly propagate changes in provider data sets to consumers in near real-time\n - Centralized audit logging for a unified view on what asset was accessed by whom"
        ],
        [
         "How Comcast Effectv Drives Data Observability with Databricks and Monte Carlo",
         "Comcast Effectv, the 2,000-employee advertising wing of Comcast, America’s largest telecommunications company, provides custom video ad solutions powered by aggregated viewership data. As a global technology and media company connecting millions of customers to personalized experiences and processing billions of transactions, Comcast Effectv was challenged with handling massive loads of data, monitoring hundreds of data pipelines, and managing timely coordination across data teams. In this talk, Robinson Creighton, Sr. DataOps Lead of Advanced Analytics at Effectv, and Lior Gavish, CTO & co-founder at Monte Carlo, will discuss Comcast Effectv’s journey to building a more scalable, reliable lakehouse and driving data observability at scale with Monte Carlo. This has enabled Effectv to have a single pane of glass view of their entire data environment to ensure consumer data trust across their entire AWS, Databricks, and Looker environment."
        ],
        [
         "Unity Catalog and Marketplace: Door to a Data Driven Organization",
         "Being data-driven can help organizations make better decisions, improve efficiency, increase competitiveness, and reduce compliance risks.\n \n Organizations expect the workforce to embrace data but at the same time want the process to be governance-driven, auditable, traceable and cost-center based for back charging.\n \n Unity Catalog is a tool that helps organizations manage and organize their data assets in a unified manner. By using Unity Catalog, you can create a central repository for all of your data assets, including structured and unstructured data, and provide access to this repository to authorized users within your organization. This can help you build a data-driven organization by enabling easy access to data for analysis and decision-making, and by promoting data governance and data management best practices.\n When Unity Catalog is made the front door for an organization’s data estate then it enables the organization to unlock the following:\n Governance-driven data discovery\n Governance-driven data access\n Governance-driven ephemeral workspace access\n Cost center based back charging\n Auditable and traceable\n \n Internal marketplace will help organizations publish and provide access to data within the organization’s workspaces via an out-of-the-box lightweight solution powered by Unity Catalog. \n \n Depending on the level of complexity and back charging, isolation requirements, organizations can choose to use the out-of-the-box internal marketplace or build the end-to-end process of publishing, provisioning, destroying, back charging process via Unity Catalog plus Workspace APIs."
        ],
        [
         "Data Mesh Realization on Databricks: Making Data Engineering and Consumption Self-Service Driven for Data Platforms",
         "\"Our client, a consulting giant, as part of the famous \"\"Big 4\"\" is creating tax"
        ],
        [
         "Distributing Data Governance: How Unity Catalog Allows for a Collaborative Approach",
         "As one of the world’s largest content delivery network (CDN) and security solutions provider, Akamai owns thousands of data assets of various shapes and sizes, some even go up to multiple PBs.\n Several departments within the company leverage Databricks for their data and AI workloads, which means we have over a hundred Databricks workspaces within a single Databricks account, where some of the assets are shared across products, and some are product-specific.\n  \n In this presentation, we will describe how to use the capabilities of Unity Catalog to distribute the administration burden between departments, while still maintaining a unified governance model.\n \n We will also share the benefits we’ve found in using Unity Catalog, beyond just access management, such as: \n * Visibility into which data assets we have in the organization\n * Ability to identify and potentially eliminate duplicate data workloads between departments \n * Removing boilerplate code for accessing external sources\n * Increasing innovation of product teams by exposing the data assets in a better, more efficient way"
        ],
        [
         "Unity Catalog at Scale in Retail Data Engineering and Data Science",
         "Retail data science, insights, and media company 84.51° helps Kroger and other organizations create more personalized and valuable experiences for shoppers. Powered by cutting edge science, we leverage first-party retail data from nearly one out of two US households totaling over 2B transactions a year. Databricks is integral to the data science and machine learning work at 84.51° and manages 35+ PB of first-party customer data across 150+ Databricks workspaces. Before Unity Catalog, this data was structured in a decentralized data mesh which had issues including problematic data sharing, minimal data governance, and no auditing of data extracts. In this session, we will discuss how 84.51° incrementally rolled out Unity Catalog company-wide in a way that involved minimal disruption of existing processes and how UC-enabled solutions solved several real-world problems including:\n • Being able to migrate an on-prem solution to a Databricks SQL Warehouse with HIPAA auditing and data governance which resulted in reduced on-prem costs and enabled new solutions/products to bring significant new revenue sources. \n • Leveraging Delta Sharing to eliminate duplication of data and processing and enabling new insights to provide a deeper, more personal approach to customer engagement.\nDatabricks has also been key to the 84.51° Collaborative Cloud, which provides our clients' data scientists a platform that provides clean and ready-to-use transaction and UPC-level data from 60 million households empowering companies to get value out of data science, regardless of where they sit on the readiness spectrum. Unity Catalog will be key to extend the collaborative cloud functionality and cleanroom technology with improved security and governance."
        ],
        [
         "Lakehouse as a FAIR Platform",
         "FAIR (findable, accessible, interoperable, reusable) data and data platforms are becoming more and more important in public sector. Lakehouse platform is strongly aligned with these principles. Lakehouse provides tools required to both adhere to FAIR but also to FAIRify data that isn't FAIR compliant. In this session we will cover parts of the lakehouse that enable end users to FAIRify data products, how to build good robust data products and which parts of Lakehouse align to which principles in FAIR. We'll demonstrate how DLT is crucial for data transformations on nonFAIR data, how Unity Catalog unlocks discoverability (F) and governed data access (A), and how marketplace, cleanrooms and Delta Sharing unlock interoperability and data exchange (I and R). All of these concepts are massive enablers for highly regulated industries such as Public Sector. It undeniably important to align Lakehouse to standards that are widely adopted by standards and policy makers and regulators. These principles transcend all industries and all use cases."
        ],
        [
         "Engineers Shouldn't Write Data Governance Policies",
         "Controlling permissions for accessing data assets can be messy, time consuming, and usually a combination of both. The teams responsible for creating the business rules that govern who should have access to what data are usually different from the teams responsible for administering the grants to achieve that access. On the other side of the equation, the end user who needs access to a data asset may be left waiting for grants to be made as the decision is passed between teams. That is, if they even know the correct path to getting access in the first place.\n  \n Separating the concerns of managing data governance at a business level and implementing data governance at an engineering level is the best way to clarify data access permissions. In practice, this involves building systems to enable data governance enforcement based on business rules, with little to no understanding of the individual system where the data lives. \n \n In practice, with a concrete business rule, such as “only users from the finance team should have access to critical financial data,” we want a system that deals only with those constituent concepts. For example, “the data is marked as critical financial” and “the user is a part of the finance team”). By abstracting away any source system components, such as “the tables in the finance schema” and “someone who’s a member of the finance databricks group,” the access policies applied will then model the business rules as closely as possible.\n \n This talk will focus on how to establish and align the processes, policies, and stakeholders involved in making this type of system work seamlessly. Sharing the experience and learnings of our team at Instacart, we will aim to help attendees streamline and simplify their data security and access strategies."
        ],
        [
         "Managing Data Encryption in Apache Spark™",
         "Sensitive data sets can be encrypted directly by new Apache Spark™  versions (3.2 and higher). Setting a number of configuration parameters and dataframe options will trigger the Apache Parquet modular encryption mechanism that protects select columns with column-specific keys. The upcoming Spark 3.4 version will also support uniform encryption, where all dataframe columns are encrypted with the same key.\n \n Spark data encryption is already leveraged by a number of companies to protect personal or business confidential data in their production environments. The main integration effort is focused on key access control and on building a Spark/Parquet plug-in code that can interact with company’s key management service (KMS).\n \n In this talk, we will briefly cover the basics of Spark/Parquet encryption usage, and dive into the details of encryption key management that will help in integrating this Spark data protection mechanism in your deployment. You will learn how to run a HelloWorld encryption sample, and how to extend it into a real world production code integrated with your organization’s KMS and access control policies. We will talk about the standard envelope encryption approach to big data protection, the performance-vs-security trade-offs between single and double envelope wrapping, internal and external key metadata storage. We will see a demo, and discuss the new features such as uniform encryption and two-tier management of encryption keys."
        ],
        [
         "Enabling Data Governance at Enterprise Scale Using Unity Catalog",
         "For some time, Amgen has been building multiple enterprise platforms on the latest technology with a key focus on tech rationalization, data democratization, overall user experience, increase reusability and cost-effectiveness. One of these platforms is the enterprise data fabric platform. This platform specifically focuses on pulling in data across functions into a single platform building capabilities around the connectedness of data, and access governance.\n For a while, we have been trying to setup robust data governance capabilities which are simple, yet easy to manage through Databricks. As part of this process we evaluated products like privacera, Immuta, and Okera. While these tools could solve a few immediate needs like managing tables and database level, for the use cases like maintaining governance on highly restricted data domains like EDW(Finance) and Workday(HR), we felt that for the long term we will require a more Databricks native solution because of below reasons:\n - There were ways to bypass these tools on databricks cluster\n - Unsupported DBR runtime\n - Complexity of fine-grained security\n - Policy management – AWS IAM + Intool policies\n  To address these challenges, and for large-scale enterprise adoption of our governance capability, we started working on UC integration with our governance processes. Following tech benefits we realized:\n - Independent of Databricks runtime\n - Easy fine-grained access control\n - Eliminated management of IAM roles\n - Dynamic access control using UC and dynamic views\n As a result of these technical benefits, we were able to implement fine-grained and complex governance policies around the restricted data set of Amgen including Finance and Workday. At this point we have around 100K objects mapped in the Unity Catalog and growing rapidly."
        ],
        [
         "Lineage System Table in Unity Catalog",
         "Unity Catalog provides fully automated data lineage for all workloads in SQL, R, Python, Scala and across all asset types at Databricks. The aggregated view has been available to end users through data explorer and API. In this session, we are excited to share that lineage is available via delta table in their UC metastore. It stores full history of recent lineage records and it is near real time. Additionally, customers can query it through standard SQL interface. With that, customers can get significant operational insights about their workload for impact analysis, troubleshooting, quality assurance, data discovery, and data governance. Together with the system table platform effort, which provides query history, job run operational data, audit logs and more, lineage table will be a critical piece to link all the data asset and entity asset together. It will provide better Lakehouse observability and unification to customers."
        ],
        [
         "Combining Privacy Solutions to Solve Data Access at Scale",
         "The trend that has made data easier to collect and analyze, has only aggravated privacy risks. Luckily, a range of privacy technologies have emerged to enable private data management (differential privacy, synthetic data, confidential computing).  In isolation, those technologies have had a limited impact because they did not always bring the 10x improvement expected by data leaders.\n \n Combining these privacy technologies has been the real game changer. We will demonstrate that the right mix of technologies brings the optimal balance of privacy and flexibility at the scale of the data warehouse.  We will illustrate this by real-life applications of Sarus in three domains:\nHealthcare: how to make hospital data available for research at scale in full compliance\nFinance: how to pool data between several banks to fight criminal transactions\nMarketing: how to build insights on combined data from partners and distributors\n \n The examples will be illustrated using data stored in Databricks and queried using Sarus differential privacy engine."
        ],
        [
         "Map Your Lakehouse Content with DiscoverX",
         "An enterprise lakehouse contains many different datasets which are related to different sources and might belong to different business units. \n These datasets can span across hundreds of tables, and each table has a different schema, and those schemas evolve over time. The Cyber security domain is a good example where datasets come from many different source systems and land in the lakehouse.\n With such a complex dataset ecosystem, answers to simple questions like “Have we ever detected this IP address?” or “Which columns contain IP addresses?” can become impractical and expensive.\n DiscoverX can automate the discovery of all columns that might contain specific patterns, (e.g. IP addresses, MAC addresses, fully qualified domain names, etc.) and automatically generate search and indexing queries that span across multiple tables and columns."
        ],
        [
         "Post-Merger: Implementing Unity Catalog Across Multiple Accounts",
         "Warner Media and Discovery have recently merged to form Warner Bros Discovery. Owning two Databricks accounts and wanting to maintain their separation, our data governance team has successfully implemented Unity Catalog as our data governance solution across both accounts, allowing our teams to collaboratively and securely use the data assets of two organizations. This session is aimed at sharing that success story, including initial challenges, our approach, our architecture, the actual implementation, and user success post-implementation."
        ],
        [
         "Advanced Governance with Collibra on Databricks",
         "Define security policies in Collibra that are seamlessly enforced on Databricks"
        ],
        [
         "How Nestle is Leveraging Unity Catalog for Governance At Scale",
         "Governance with Unity Catalog across multiple regions, markets, teams and environments at Nestle. Establish data isolation at all levels"
        ],
        [
         "Wrangling Your Security Data: Cybersecurity as a Data Management Problem",
         "Today, the average Fortune 500 company manages 100+ cybersecurity vendors. The sheer number of enterprise cybersecurity toolsets, each capturing and generating its own data, creates significant data management and data engineering challenges for all enterprises. Because each of these security tools has separate logs, outputs, and databases, it prevents any security team from meaningfully understanding their overall security posture and using their own data effectively. We believe that cybersecurity is fundamentally a data management problem. In this presentation, we will show how treating cybersecurity as a data management problem, and subsequently opening disparate cybersecurity data to queries can help organizations secure their systems and data more effectively. In this presentation, practitioners will walk away understanding the value of modeled and organized security on a data warehouse. We will leverage a real-world use case that shows how Databricks can help solve vulnerability management and end-to-end workflows for security practitioners."
        ],
        [
         "Lakehouse Architecture to Advance Security Analytics at the Department of State",
         "In 2023, the Department of State surged forward on implementing a Lakehouse architecture to get faster, smarter, and more effective on cybersecurity log monitoring and incident response. In addition to getting us ahead of federal mandates, this approach promises to enable advanced analytics and machine learning across our highly federated global IT environment while minimizing costs associated with data retention and aggregation. \n \n This session will include a high-level overview of the technical and policy challenge and a technical deeper dive on the tactical implementation choices made. We’ll share lessons learned related to governance and securing organizational support, connecting between multiple cloud environments, and standardizing data to make it useful for analytics. \n \n And finally, we’ll discuss how the Lakehouse leverages Databricks in multi-cloud environments to promote decentralized ownership of data while enabling strong, centralized data governance practices."
        ],
        [
         "Engineering Data Lakehouse Systems for the Next Ten Years of Growth",
         "For most of my professional life, I've dealt with data. As a data practitioner, I developed algorithms to solve real-world problems leveraging machine learning techniques. As an engineer, I led the direction that brought the value of my hands-on machine learning experience into our products and services by building upon cutting-edge and emerging technologies. This experience has taught me that scaling data systems is harder than you might think. \n  \n Supporting the operations of scalable data environments poses a challenge greater than the known application-level support due to the complexity of managing data together with the application. Why is data adding so much complexity? Well, data is big, so all systems are now becoming distributed. Data changes and evolves, and it's hard to create repeatable, automated pipelines. Plus, technology is advancing at an alarming rate and changes are messy.\n  \n Interested in learning about all the most recent updates in the data space? How does that impact our data systems? In this talk, you will learn about the challenges of product quality, delivery velocity, production monitoring, and outage recovery, see how those can be met using best practices in the tech stack, and develop empathy for those who manage scalable data environments."
        ],
        [
         "Optimize Your Delta Lake",
         "Delta tables in Databricks just work. They're so easy to setup to deliver value in your organization. What about all those features and optimization that you can use to super charge your delta lakehouse? \n \n In this lighting talk we will discuss structuring, partitioning, optimizing, delta log retention of your Delta Tables all in 15 minutes or less!"
        ],
        [
         "Lakehouses for Data Engineers: What You Need to Consider to Build Efficient ETL Pipelines",
         "ETL pipelines are ubiquitous in data-warehousing and Lakehouse ecosystem to make business decisions by building raw and derived tables from a plethora of fragmented data. As companies are investing in building data applications for richer user experiences, ETL pipelines need to solve some of the modern-day challenges of helping deliver low-latency analytics. \n \n In this lightning talk, we’ll focus on how to build efficient Apache Spark™  ETL pipelines for Lakehouses where you can effectively ingest and utilize streaming data. Will cover details on:\n - leveraging incrementally processing framework to improve compute efficiency\n - index data to deliver low-latency analytics\n - advanced concurrency control mechanisms to improve throughput"
        ],
        [
         "Bringing Data in-House! Behind the New York Jets' Winning Game Plan to Create a Data Lakehouse on Databricks",
         "On the field, the New York Jets are one of the most recognizable brands in professional sports. Off the field, they’re a small business within a finite market with lots of competition. In order to fuel revenue growth, improve operational efficiencies, and drive fan engagement, the Jets brought its data infrastructure in-house and moved to Databricks. \n \n This 40-minute presentation will include a discussion of the New York Jets data journey including its overall business and technical goals, the organization's multiple attempts at building a data stack, what drove the shift to Databricks, its current architectural approach, and the benefits of leveraging Databricks as its cloud data platform. \n \n Benefits to data practitioners:\n  - Learn about the New York Jets data journey and the similarities to other organizations’ data journeys including challenges, build vs. buy, and the decision \n making processes required to implement a modern data stack\n  - Discussion around the Data Lakehouse architectural pattern and the steps required to implement this on Databricks\n  - Identification of the benefits of a customer 360 analysis and process and technologies required to create this record \n  - Understand the benefits of leveraging Databricks as a cloud data platform \n  - Highlight future opportunities to leverage Databricks for ML / AI use cases to improve upon current baseline analytics"
        ],
        [
         "Databricks-As-Code: How to Effectively Automate a Secure Lakehouse Using Terraform for Resource Provisioning",
         "At Rivian, we have automated >95% of our Databricks resource provisioning workflows using an in-house Terraform module, affording us a lean admin team to manage 750+ users. In this talk, we will cover the following elements of our approach and how others can benefit from improved team efficiency.\n \n User and service principal management\n Our permission model on Unity Catalog for data governance\n Workspace and secrets resource management\n Managing internal package dependencies using init scripts\n Facilitating dashboards, SQL queries and their associated permissions\n Scaling source of truth Petabyte scale Delta Lake table ingestion jobs and workflows"
        ],
        [
         "Real-Time Reporting and Analytics for Construction Data Powered by Delta Lake and DBSQL",
         "Procore is a construction project management software that helps construction professionals efficiently manage their projects and collaborate with their teams. Our mission is to connect everyone in construction on a global platform. \n \n Procore is the system of record for all construction projects. Our customers need to access the data in near real-time for construction insights. Enhanced reporting is a self-service operational reporting module that allows quick data access with consistency to thousands of tables and reports.\n \n Procore Data platform rebuilt the module (originally built on the relational database) using Databricks and Delta lake. We used Apache Spark™  streaming to maintain the consistent state on the ingestion side from Kafka and plan to leverage the fully capable functionalities of DBSQL using the serverless SQL warehouse to read the medallion models (built via DBT) in Delta Lake. In addition, the Unity Catalog and the Delta share features helped us share the data across regions seamlessly. This design enabled us to improve the p95 and p99 read time by x% (which were initially timing out).\n \n  To learn about the learnings and experience of building a Data Lakehouse architecture, attend this talk."
        ],
        [
         "Made in Italy: How Barilla Uses Databricks Lakehouse to Optimize its Operations",
         "Barilla Data2Value digital transformation program democratized data usage. The Databricks centered platform provides BI for a supply chain extended in 100 countries in the world, across 20 factories, >50 logistic hubs, 1000 suppliers, >10.000 B2B clients and >100.000 transports every year. By using metadata-driven and event-driven approaches, Barilla is able to meet the different needs of its 1000 data consumers from top management to operation management with about 50 use cases, consuming TB of data daily and execute more than 150 data pipelines. During the presentation, Barilla digital transformation strategy will be discussed, the platform architecture, the expected evolution as well as the lessons learned along the way with EY. Will be explained how Barilla has reduced the time to insight using Unity Catalog and MLFlow. Mariama Kamanda, a data scientist from the Barilla Acceleration Team, will provide testimony on the impact of Databricks Lakehouse daily journey. Will be explained how Barilla saved M$ every year by improving the factory losses through advanced analytics and machine learning leveraging Databricks."
        ],
        [
         "Using Lakehouse to Fight Cancer: Ontada’s Journey to Establish a RWD Platform on Databricks Lakehouse",
         "Ontada, a McKesson business, is an oncology real-world data and evidence, clinical education and provider of technology business dedicated to transforming the fight against cancer. Core to Ontada’s mission is using real-world data (RWD) and evidence generation to improve patient health outcomes and to accelerate life science research. \n \n To support its mission, Ontada embarked on a journey to migrate its enterprise data warehouse (EDW) from an on-premise Oracle database to Databricks Lakehouse. This move allows Ontada to now consume data from any source, including structured and unstructured data from its own EHR and genomics lab results, and realize faster time to insight. In addition, using the Lakehouse has helped Ontada eliminate data silos, enabling the organization to realize the full potential of RWD – from running traditional descriptive analytics to extracting biomarkers from unstructured data. \n \n The presentation will cover the following topics: \n - Oracle to Databricks: migration best practices and lessons learned \n - People, process, and tools: expediting innovation while protecting patient information using Unity Catalog \n - Getting the most out of the Databricks Lakehouse: from BI to genomics, running all analytics under one platform \n - Hyperscale biomarker abstraction: reducing the manual effort needed to extract biomarkers from large unstructured data (medical notes, scanned/faxed documents) using spaCY and John Snow Lab NLP libraries \n \n Join this session to hear how Ontada is transforming RWD to deliver safe and effective cancer treatment."
        ],
        [
         "CPG Lakehouse Analytics: Rapidly Implementing Walmart’s Luminate API at the Hershey Company",
         "Accurate, reliable, and timely data is critical for CPG companies to stay ahead in highly competitive retailer relationships, and for a company like the Hershey Company, the commercial relationship with Walmart is one of the most important. The team at Hershey found themselves with a looming deadline for their legacy analytics services and targeted a migration to the brand new Walmart Luminate API.\n  \n Working in partnership with Advancing Analytics, the Hershey Company leveraged a metadata-driven Lakehouse architecture to rapidly onboard the new Luminate API, helping the category management teams to overhaul how they measure, predict, and plan their business operations.\n  \n In this session, we will discuss the impact Luminate has had on Hershey's business covering key areas such as sales, supply chain, and retail field execution, and the technical building blocks that can be used to rapidly provision business users with the data they need, when they need it. We will discuss how key technologies enable this rapid approach, with Databricks Autoloader ingesting and shaping our data, Delta Streaming processing the data through the Lakehouse and Databricks SQL providing a responsive serving layer.\n  \n The session will be jointly run by Hersheys and Advancing Analytics, with Jordan Donmoyer, manager of Hershey's direct data team and Dan Davies, director of Walmart sales analytics providing the commercial commentary, and Simon Whiteley and Zach Stagers from Advancing Analytics covering the technical journey."
        ],
        [
         "Building a Minimalistic Open Lakehouse Using Open Source Projects Apache Spark™, Project Nessie and Iceberg",
         "A Lakehouse architecture is a combination of various components such as Storage, File format, Table format, and Catalog. What truly makes a lakehouse 'open' is data being stored in open-source table & file formats like Iceberg, Delta & Parquet respectively and the technology being open-sourced for easy & quick adoption by the community. Like any new technology, implementation of a lakehouse may seem daunting at first. However, when we break down the architecture to its open components, this becomes easy to adopt & scale.\n \n Though this session, the idea is to help data engineers getting their leg into the world of data lakehouses, easily learn & implement it. We will go through a Notebook-style presentation to show beginners how to build a minimalistic functional lakehouse using Apache Spark, Project Nessie & Iceberg.\n \n We will talk about:\n - configuring the 3 different components\n - creating tables from raw data files\n - ingesting new data from various sources into the tables, querying it and making updates\n - time travel, compaction, etc capabilities"
        ],
        [
         "Labcorp Data Platform Journey: From Selection to Go-Live in six months",
         "Please join with us to learn about the Labcorp data platform transformation from on-premises Hadoop to AWS Databricks Lakehouse. We will share best practices and lessons learned from cloud-native data platform selection, implementation, and migration from Hadoop (with in 6 months) with Unity Catalog. We will share steps taken to retire several legacy on-premises technologies and leverage Databricks native features like Spark streaming, workflows, job pools, cluster policies and Spark JDBC within Databricks platform. Lessons learned in Implementing Unity Catalog and building a security and governance model that scales across applications. Demos and walkthrough’s of batch frameworks, streaming frameworks, data compare tools used across several applications to improve data quality and speed of delivery. Discover how we have improved operational efficiency, resiliency and reduced TCO, and how we scaled building workspaces and associated cloud infrastructure using Terraform provider."
        ],
        [
         "Deep Dive Into Grammarly's Data Platform",
         "Grammarly helps 30 million people and 50,000 teams to communicate more effectively. Using the Databricks Lakehouse Platform, we are able to rapidly ingest, transform, aggregate, and query complex data sets from an ecosystem of sources, all governed by Unity Catalog. This presentation overviews Grammarly’s data platform and the decisions that shaped the implementation. We will dive deep into some architectural challenges the Grammarly Data Platform team overcame as we developed a self-service framework for incremental event processing.\n \n Our investment in the Lakehouse and Unity Catalog has dramatically improved the speed of our data value chain: making 5 billion events (ingested, aggregated, de-identified, and governed) available to stakeholders (data scientists, business analysts, sales, marketing) and downstream services (feature store, reporting/dashboards, customer support, operations) available within 15 minutes. As a result, we have improved our query cost performance (110% faster at 10% the cost) compared to our legacy system on AWS EMR.\n \n I will share architecture diagrams, their implications at scale, code samples, and problems solved and to be solved in a technology-focused discussion about Grammarly’s iterative Lakehouse Data Platform."
        ],
        [
         "Simplifying Migrations to Lakehouse",
         "Following on from Perenti Global's successful presentation at the ANZ (Syd) Data and AI summit in Nov 2022, they were asked by Bede Hackney if they would be interested in supporting Databricks events on a bigger stage, ramping up their reference-ability etc... they are confirmed as able to support us at this event and are excited to do so. \n Link to client presentation noting tweaks to be made upon confirmation of attendance --> https://docs.google.com/presentation/d/1OHMsyFhV-gq6lcVvXqkp-0LY8wS6C6TS/edit#slide=id.p1\n \n \n High level Synopsis:\n - Challenges with Legacy Platforms \n - Perenti Databricks Migration Journey\n - Reimagining Migrations the Databricks Way\n - The Databricks Migration Methodology & Approach"
        ],
        [
         "Having Your Cake and Eating it Too: How Vizio Built a Next-Generation ACR Data Platform While Lowering TCO",
         "As the top manufacturer of smart TVs, Vizio uses TV data to drive its business and provide customers with best digital experiences. Our company's mission is to continually improve the viewing experience for our customers, which is why we developed our award-winning automatic content recognition (ACR) platform. When we first built our data platform almost ten years ago, there was no single platform to run a data as a service business, so we got creative and built our own by stitching together different AWS services and a data warehouse. As our business needs and data volumes have grown exponentially over the years, we made the strategic decision to replatform on Databricks Lakehouse, as it was the only platform that could satisfy all our needs out-of-the-box such as BI analytics, real-time streaming, and AI/ML. Now the Lakehouse is our sole source of truth for all analytics and machine learning projects. The technical value of the Databricks Lakehouse platform, such as traditional data warehousing low-latency query processing with complex joins thanks to Photon to using Apache Spark™  structured streaming; analytics and model serving, will be covered in this session as we talk about our path to the Lakehouse."
        ],
        [
         "From Insights to Recommendations: How SkyWatch Predicts Demand for Satellite Imagery Using Databricks",
         "SkyWatch is on a mission to democratize earth observation data and make it simple for anyone to use.\n \n In this session, you will learn about how SkyWatch aggregates demand signals for EO market and turns them into monetizable recommendations for satellite operators.\n Skywatch’s head of data engineering, Vincent, and data engineer, Aayush, will share how the team built a serverless architecture that synthesizes customer requests for satellite images and identifies geographic locations with high demand, helping satellite operators maximize revenue and satisfying a broad range of EO data hungry consumers.\n \n This session will cover the usage of\n \n - Databricks in-built H3 functions \n - Open Source Mosaic library to process geospatial data\n - Delta Lake to efficiently store data leveraging optimization techniques like Z-Ordering\n - Databricks Serverless SQL endpoint to build REST API with AWS Lambda and step functions.\n - Databricks with AWS serverless architecture (Lambda and step functions)"
        ],
        [
         "Determining When to Use GPU for Your ETL Pipelines at Scale",
         "Assuming you have hundreds of jobs and/or clusters in your Databricks workspace, what is the best way to determine if those pipelines can take advantage of GPU for speed and/or cost saving? We are presenting NVIDIA GPU Qualification Tool applied at scale to project potential cost saving for your entire workspace."
        ],
        [
         "Massive Data Processing in Adobe Using Delta Lake:  A Year In",
         "At Adobe Experience Platform, we ingest TBs of data everyday and manage PBs of data for our customers as part of the unified profile offering. At the heart of this is a bunch of complex ingestion of a mix of normalized and denormalized data with various linkage scenarios power by a central identity linking graph. This helps power various marketing scenarios that are activated in multiple platforms and channels like email, advertisements etc. We will go over how we built a cost effective and scalable data pipeline using Apache Spark™  and Delta Lake and share our experiences after one year in production.\n \n - What are we storing?\n - Multi-source, multi-channel problem\n - Access pattern to optimize for\n - Custom high performance query engine\n - Data representation and nested schema evolution\n - Performance trade-offs with various formats\n - Go over anti-patterns used \n - String FTW\n - Data manipulation using UDFs \n - Writer worries and how to wipe them away\n  - Locking for contention mechanism\n - Gotchas\n - Concurrency\n - Column size\n - Update frequency\n - Transaction management for a healthy state\n - Staging tables FTW \n - Why we can't live without them\n - Datalake replication lag tracking\n - Instrumentation of the data pipeline gives more confidence to the reade\n - Maintenance jobs\n - Go over essentials of compaction and vacuuming\n - Performance time!\n - What scale are we operating at?\n - Settings like autoCompact and optimizeWrite\n - Timings with and without delta\n - Cost \n  - Lesson learned and burnt"
        ],
        [
         "DHL eCommerce US – Building a scalable and robust Cloud Data Platform as enabler for Enterprise Analytics",
         "DHL eCommerce Solutions Americas is a high volume B2C domestic and international parcel delivery business and that is on mission to become a data-driven organization to by democratizing the use of data and to maximizing the business value.\n  \n Learn how the DHL eCommerce Solutions Americas data team built a centralized Enterprise grade scalable cloud data platform processing 100s of millions of events daily. This single source of truth powering all analytics use cases helps DHL ecommerce US team to deliver business insights 5x faster.\n We will dive into how we solved challenging key requirements including secure multi-tenant central data platform, scalable data ingestion and processing, cost effective big data repository, fast and regular data refreshes, diverse analytics workload needs.\n \n Finally we will share some example usecases operated on this platform and the business value those usecases are generating along various lines of the business including operations, commercial, customer service, sales, product management, etc."
        ],
        [
         "Learnings From the Field: Migration From Oracle DW and IBM DataStage to Databricks on AWS",
         "Legacy data warehouses are costly to maintain, unscalable and cannot deliver on data science, ML and real-time analytics use cases. Migrating from your enterprise data warehouse to Databricks lets you scale as your business needs grow and accelerate innovation by running all your data, analytics and AI workloads on a single unified data platform.\n \n In the first part of this talk we will guide you through the well-designed process and tools that will help you from the assessment phase to the actual implementation of an EDW migration project. Also, we will address ways to convert PL/SQL proprietary code to an open standard python code and take advantage of PySpark for ETL workloads and Databricks SQL’s data analytics workload power.\n \n The second part of this talk will be based on an EDW migration project of SNCF (French national railways); one of the major enterprise customers of Databricks in France. Databricks partnered with SNCF to migrate its real estate entity from Oracle DW and IBM DataStage to Databricks on AWS. We will walk you through the customer context, urgency to migration, challenges, target architecture, nitty-gritty details of implementation, best practices, recommendations, and learnings in order to execute a successful migration project in a very accelerated time frame."
        ],
        [
         "Why a Major Japanese Financial Institution Chose Databricks to Accelerate its Data and AI-Driven Journey",
         "In this session, we will introduce a case study of migrating the Japanese largest data analysis platform to Databricks.\n \nNTT DATA is one of the largest system integrators in Japan. In the Japanese market, many companies are working on BI, and we are now in the phase of using AI. Our team provides solutions that provide data analytics infrastructure to drive the democratization of data and AI for leading Japanese companies.\n \n The customer in this case study is the largest insurance company in Japan. This project has the following characteristics:\n  - As a financial institution, security requirements are very strict.\n  - Since it is used company-wide, including group companies, it is necessary to support various use cases.\n \n We started operating a data analysis platform on AWS in 2017. Over the next five years, we leveraged AWS-managed services such as Amazon EMR, Amazon Athena, and Amazon Sage Maker to modernize our architecture.\n \n In the near future, in order to promote the use cases of AI as well as BI, we have begun to consider upgrading to a platform that realizes both BI and AI.\n \n This session will cover:\n  - Challenges in developing AI on a DWH-based data analysis platform and why a data lake house is the best choice\n  - Examining the architecture of a platform that supports both AI and BI use cases. In this case study, we will introduce the results of a comparative study of a proposal based on Databricks, a proposal based on Snowflake, and a proposal combining Snowflake and Databricks.\n \n This session is recommended for those who want to accelerate their business by utilizing AI as well as BI.\n \n \n"
        ],
        [
         "Eliminating Shuffles in Delete Update, and Merge",
         "If you’ve ever had to delete a set of records for regulatory compliance, update a set of records to fix an issue in the ingestion pipeline, or apply changes in a transaction log to a fact table, you know that row-level operations are becoming critical for modern data lake workflows. Even though the industry has seen a tremendous amount of innovation in this area, row-level operations can be still fairly expensive if the underlying data has to be shuffled. This talk will explain how Apache Spark can completely avoid shuffles during row-level operations by leveraging storage-partitioned joins, a key to efficiently modify data at PB scale."
        ],
        [
         "When Self-Service Meets Earnings Calls: A Journey Towards FinOps",
         "At Disney Streaming, we extensively use Databricks and the delta lake as an operational and analytical tool. But as we shift focus from growth to profit, our data platform needed to pivot from self-service and ease use and towards automatic tracking, auditing, and continuous efficiency chasing. \n \n In this talk, I'll explain our FinOps journey; how we democratized cost data, increased transparency, tightened policies, and got dozens of analytical and engineering teams to fit cost savings into their roadmap.\n"
        ],
        [
         "Delta Lake Migration At Scale",
         "Adobe Experience Platform (AEP) serves a large number Adobe digital experience customers with tens of thousands of datasets in non-Delta format. To leverage Delta Lake, we migrate existing tables to Delta format after resolving the following challenges: 1). Short downtime to minimize customer impact; 2). Adding new system metadata; 3). Migration automation for scalability; and 4). Error detection and catastrophic rollback. This talk will cover the solutions to the above challenges and lessons learned from migration. Finally, scalable migration service becomes a core capability of the data platform."
        ],
        [
         "Databricks Cost Management",
         "With Databricks you only pay for the compute resources you use. With growing usage it is advisable to have a cost management strategy to avoid surprises at the end of the month. It is also useful to monitor and break down costs for budgeting purposes or to calculate the return on investment (ROI) for specific projects.\n \n In this talk I will:\n - Describe the Databricks pricing model\n - Show how to analyze and break down costs\n - Discuss best practises for cost optimization"
        ],
        [
         "Data Asset Bundles: A Standard, Unified Approach to Deploying Data Products on Databricks",
         "In this talk we will introduce data asset bundles, provide a demonstration of how they work for a variety of data products, and how to fit them into an overall CICD strategy for the well-architected Lakehouse.\n \n Data teams produce a variety of assets; datasets, reports and dashboards, ML models, and business applications. These assets depend upon code (notebooks, repos, queries, pipelines), infrastructure (clusters, SQL warehouses, serverless endpoints), and supporting services/resources like Unity Catalog, Databricks Workflows, and DBSQL dashboards. Today, each organization must figure out a deployment strategy for the variety of data products they build on Databricks as there is no consistent way to describe the infrastructure and services associated with project code.\n \n Data asset bundles is a new capability on Databricks that standardizes and unifies the deployment strategy for all data products developed on the platform. It allows developers to describe the infrastructure and resources of their project through a YAML or JSON configuration file, regardless of whether they are producing a report, dashboard, online ML model, or Delta Live Tables pipeline. Behind the scenes, these configuration files use Terraform to manage resources in a Databricks workspace, but knowledge of Terraform is not required to use Data Asset Bundles."
        ],
        [
         "Lakehouses: The Best Start to Your Graph Data and Analytics Journey",
         "Data architects and IT executives are continually looking for the best ways to integrate graph data and analytics into their organizations to improve business outcomes. This presentation outlines how the Data Lakehouse provides the perfect starting point for a successful journey. We will explore how the Data Lakehouse offer the unique combination of scalability, flexibility, and speed to quickly and effectively ingest, pre-process, curate, and analyze graph data to create powerful analytics. Additionally, this talk will discuss the benefits of using the Data Lakehouse over traditional graph databases and how it can help improve time to insight, time to production and overall satisfaction. \n \nAt the end of this presentation, attendees will: \n - Understand the benefits of using a Data Lakehouse for graph data and analytics \n - Learn how to get started with a successful Lakehouse implementation (demo)\n - Discover the advantages of using a Data Lakehouse over graph databases\n - Learn specifically where graph databases integrate and perform better together\n \n Key Takeaways: \n - Data Lakehouses provide the perfect starting point for a successful graph data and analytics journey \n - Data Lakehouses offer scalability, flexibility, and speed to quickly and effectively analyze graph data \n - The Data Lakehouse is a cost-effective alternative to traditional graph database shortening your time to insight and de-risk your project.\n \n Presentation Components:\n The customer journey\n Access patterns\n Reference architecture (logical, physical)\n Customer stories\n Demo (notebook + live graph visualization)"
        ],
        [
         "Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse",
         "As you embark on a lakehouse project or evolve your existing data lake, you may want to improve your security posture and take advantage of new security features—there may even be a security team at your company that demands it! Databricks has worked with thousands of customers to securely deploy the Databricks Platform to meet their architecture and security requirements. While many organizations deploy security differently, we have found a common set of guidelines and features among organizations that require a high level of security. In this session, we will detail the security features and architectural choices frequently used by these organizations and walk through a series of threat models for the risks that most concern security teams. While this session is great for people who already know Databricks—don’t worry—that knowledge isn’t required. \nYou will walk away with a full handbook detailing all of the concepts, configurations, check lists, security analysis tool (SAT), and security reference architecture (SRA) automation scripts  from the session so that you can make immediate progress when you get back to the office. \\\nSecurity can be hard, but we’ve collected the hard work already done by some of the best in the industry, and built tools, to make it easier. Come learn how. See how good looks like via a demo.\n"
        ],
        [
         "Data Globalization at Conde Nast Using Delta Sharing",
         "Databricks has been an essential part of the Conde Nast architecture for the last few years.\n \n Prior to building our centralized data platform, “evergreen”, we had similar challenges as many other organizations; siloed data, duplicated efforts for engineers, and a lack of collaboration between data teams. These problems led to mistrust in data sets and made it difficult to scale to meet the strategic globalization plan we had for Conde Nast.\n \n Over the last few years we have been extremely successful in building a centralized data platform on Databricks in AWS, fully embracing the lakehouse vision from end-to-end. Now, our analysts and marketers can derive the same insights from one dataset and data scientists can use the same datasets for use cases such as personalization, subscriber propensity models, churn models and on-site recommendations for our iconic brands.\n \n In this talk, we’ll discuss how we plan to incorporate Unity Catalog and Delta Sharing as the next phase of our globalization mission. The evergreen platform has become the global standard for data processing and analytics at Conde. In order to manage the worldwide data and comply with GDPR requirements, we need to make sure data is processed in the appropriate region and PII data is handled appropriately. At the same time, we need to have a global view of the data to allow us to make business decisions at the global level. We’ll talk about how delta sharing allows us a simple, secure way to share de-identified datasets across regions in order to make these strategic business decisions, while complying with security requirements. Additionally, we’ll discuss how Unity Catalog allows us to secure, govern and audit these datasets in an easy and scalable manner."
        ],
        [
         "Unlocking the Value of Data Sharing in Financial Services With Lakehouse",
         "The emergence of secure data sharing is already having a tremendous economic impact, in large part due to the increasing ease and safety of sharing financial data. McKinsey predicts that the impact of open financial data will be 1-4.5% of GDP globally by 2030. This indicates there is a narrowing window on a massive opportunity for financial institutions and it is critical that they prioritize data sharing. This session will first address the ways in which Delta Sharing and Unity Catalog on a Databricks Lakehouse architecture provides a simple and open framework for building a Secure Data Sharing platform in the financial services industry. Next we will use a Databricks environment to walk through different use cases for open banking data and secure data sharing, demonstrating how they will be implemented using Delta Sharing, Unity Catalog, and other parts of the Lakehouse platform. The use cases will include examples of new product features such as Databricks to Databricks sharing, change data feed and streaming on Delta Sharing, table/column lineage, and the Delta Sharing Excel plugin to demonstrate state of the art sharing capabilities.\n Spencer Cook, a Sr. Solutions Architect at Databricks focussed on the Financial Services industry will discuss secure data sharing on Databricks Lakehouse and will demonstrate architecture and code for common sharing use cases in the finance industry."
        ],
        [
         "Extending Lakehouse Architecture with Collaborative Identity",
         "Lakehouse architecture has become a valuable solution for unifying data processing for AI, but faces limitations in maximizing data’s full potential. Additional data infrastructure is helpful for strengthening data consolidation and data connectivity with third-party sources, which are necessary for building full data sets for accurate audience modeling.\n\nIn this session, LiveRamp will demonstrate to data and analytics decision-makers how to build on the Lakehouse architecture with extensions for collaborative identity graph construction, including how to simplify and improve data enrichment, data activation and data collaboration. LiveRamp will also introduce a complete data marketplace, which enables easy, pseudonymized data enhancements that widen the attribute set for better behavioral model construction.\n\nWith these techniques and technologies, enterprises across financial services, retail, media, travel and more can safely unlock partner insights and ultimately produce more accurate inputs for personalization engines, and more engaging offers and recommendations for customers.\n"
        ],
        [
         "Writing Data-Sharing Apps Using Node.js and Delta Sharing",
         "Javascript remains the top programming language today, with most code repositories written using JavaScript on GitHub. However, JavaScript is evolving beyond just a language for web application development into a language built for tomorrow. Everyday tasks like data wrangling, data analysis, and predictive analytics are possible today directly from a web browser. For example, many popular data analytics libraries, like Tensorflow.js, now support JavaScript SDKs. Another popular library, Danfo.js, makes it possible to wrangle data using familiar Pandas-like operations, shortening the learning curve and arming the typical data engineer or data scientist with another data tool in their toolbox. In this presentation, we’ll explore using the Node.js connector for Delta Sharing to build a data analytics app that summarizes a Twitter dataset."
        ],
        [
         "Creating the First Sports and Entertainment Data Marketplace Powered by Pumpjack Dataworks, Revelate, Immuta and Databricks",
         "Creating a secure and easily actionable marketplace is no simple task. Add to this governance requirements of privacy frameworks and responsibilities of protecting consumer data, and things get harder. With Pumpjack Dataworks partnering with Databricks, Immuta, and Revelate, we bring secure, privacy-focused data products directly to data consumers. With Delta Share, the service solves redundancy of data purchases, time to action, and lowers cost of data acquisition while providing new revenue streams for rights holders. Its a win-win solution for bringing Data Acquisition and Commercial teams together into the new age of data."
        ],
        [
         "Data Interoperability Across Clouds and Regions",
         "L’Oréal has the largest and richest data repository in the beauty industry, with a 110-year heritage solely dedicated to analyzing, measuring and magnifying the beauty needs and desires of consumers around the world. As L’Oréal’s ambition is to become the leader in beauty tech, a profound IT transformation started three years ago to meet their most strategic priorities: \n - Hyper personalization\n - Consumer experience online and offline \n - Preventive care\n - Integrating artificial intelligence into the business\n \n This IT transformation now needs to reach all of their services and teams worldwide with the same level of security, quality and exacting standards. To do so, data must be unlocked and democratized across the world in order to meet their strategic priorities. This is why L’Oréal and Databricks worked together on the data interoperability value proposition in order to :\n - Facilitate multi-cloud interoperability exchanges thanks to open standards technologies\n - Secure and trace through a centralized governance tool to respond to the growing regulatory environment and the application of other data laws (such as in Vietnam)\n - Optimize and rationalize data exchanges to reduce transported volumes and meet L'Oréal's sustainable development objectives\n - Ensure the quality of the data exchanged through quality and validation checks"
        ],
        [
         "Data Extraction and Sharing Via the Delta Sharing Protocol: Overfetching, Underfetching and Other Lessons and Tips for Development Learned While Building the Delta Sharing Excel Add-in",
         "The Delta Sharing open protocol for secure sharing and distribution of Lakehouse data is designed to reduce friction in getting data to users. Delivering custom data solutions from this protocol further leverages the technical investment committed to your Delta Lake infrastructure.\n \n There are key design and computational concepts unique to Delta Sharing to know when undertaking development. And there are pitfalls and hazards to avoid when delivering modern cloud data to traditional data platforms and users.\n \n In this session we introduce Delta Sharing Protocol development and examine our journey and the lessons learned while creating the Delta Sharing Excel Add-in. We will demonstrate scenarios of overfetching, underfetching, and interpretation of types. We will suggest methods to overcome these development challenges.\n \n The session will combine live demonstrations that exercise the Delta Sharing REST protocol with detailed analysis of the responses. The demonstrations will elaborate on optional capabilities of the protocol’s query mechanism, and how they are used and interpreted in real-life scenarios.\n \n As a reference baseline for data professionals, the Delta Sharing exercises will be framed relative to SQL counterparts. Specific attention will be paid to how they differ, and how Delta Sharing’s Change Data Feed (CDF) can power next-generation data architectures.\n \n The session will conclude with a survey of available integration solutions for getting the most out of your Delta Sharing environment, including frameworks, connectors, and managed services.\n \n Attendees are encouraged to be familiar with REST, JSON, and modern programming concepts. A working knowledge of Delta Lake, the Parquet file format, and the Delta Sharing Protocol are advised."
        ],
        [
         "Data sharing & beyond with Delta Sharing",
         "Stepping into this brave new digital world we are certain that data will be a central product for many organizations. The way to convey their knowledge and their assets will be through data and analytics. Delta Sharing was the world's first open protocol for secure and scalable real-time data sharing. Through our customer conversations, there is a lot of anticipation of how Delta Sharing can be extended to non-tabular assets, such as machine learning experiments and models.\n \n In this talk, we will cover how we extended the Delta Sharing protocol to other sharing workflows, enabling sharing of ML models, arbitrary files and more. The development resulted in Arcuate, a Databricks Labs project with a data sharing flavour. The talk will start with the high-level approach and how it can be extended to cover other similar use cases. It will then move to our implementation and how it integrates seamlessly with Databricks-managed Delta Sharing server and notebooks. We finally conclude with lessons learned, and our visions for a future of data sharing & beyond"
        ],
        [
         "Ad Measurement: From Impressions to Attribution",
         "\"Effectv, the advertising sales division of Comcast, delivers linear and digital advertising to help advertisers reach potential customers. In this talk, Joe Walsh, Director of Attribution & Measurement and Derek Sugden, Ad Measurement Lead, will discuss their team's journey of measuring the impact of advertising campaigns on customer behavior by combining Comcast's household-level exposure data with various types of third-party conversion data sourced externally. They will highlight the challenges of sharing and receiving data securely while preserving customer privacy, and share how they have implemented an internal \"\"clean room\"\" concept to restrict roles and usage of a cluster"
        ],
        [
         "Clean Room Primer: Using Clean Rooms on Databricks to Utilize More and Better Data in your BI, ML, and Beyond",
         "In this session, Habu’s Chief Product Officer, Matt Karasick and Senior Architect, Anil Puliyeril, will discuss the foundational changes in the ecosystem, the implications of data insights on marketing, analytics, and measurement, and how companies are coming together to collaborate through data clean rooms in new and exciting ways to power mutually beneficial value for their businesses while preserving privacy and governance.\n \n We will delve into the concept and key features of clean room technology and how they can be used to access more and better data for business intelligence (BI), machine learning (ML), and other data-driven initiatives. By examining real-world use cases of clean rooms in action, attendees will gain a clear understanding of the benefits they can bring to industries like CPG, retail, and media & entertainment. \n \n In addition, we will unpack the advantages of using Databricks as a clean room platform, specifically showcasing how interoperable clean rooms can be leveraged to enhance BI, ML and other compute scenarios. By the end of this talk, you will be equipped with the knowledge and inspiration to explore how clean rooms can unlock new collaboration opportunities that drive better outcomes for your business and improved experiences for consumers."
        ],
        [
         "Embrace First-Party Data Collaboration to Lower Acquisition Costs with Lookalike Audiences in Media Cleanrooms",
         "Third-party tracking is dead! Yet customer acquisition costs continue to rise to meet the demands of customer attention. Lookalike audience modeling leveraging first-party customer data has offered organizations a cost-effective means to identifying and engaging new customers at scale. However, as we move into a privacy-centric era, collaboration with first-party data across organizations becomes increasingly challenging. So how can we work together to meet both needs? The answer is data clean rooms for the lakehouse, fueled by first-party customer data to build lookalike audience profiles and drive highly personalized ads and experiences in media and commerce.\n \n Using Snowplow's generation of granular first-party customer behavioral data with governance metadata and consent tracking, managed through data clean rooms in the lakehouse, organizations can collaborate securely across their first-party datasets. For example, brands and advertisers can join their first-party onsite data with third-party datasets shared by their agency and advertising partners to boost CTR and ROAS with lookalike audiences.\n \nThis session you focus on:\n - Best practices for creating lookalike audiences with first-party customer data to lower acquisition costs\n - See a live demonstration of how audiences derived through the Databricks Lakehouse can be securely exchanged with customers and partners to foster data-driven innovations creating a customer behavioral profile using Snowplow \n - Data engineering and preparation of Snowplow Behavioral Data with DBT and Databricks\n - Development and training of an ML model using PySpark on Databricks to create a model to drive contextual advertising\n - Implementation of the model for scoring\n - Engage in Q&A"
        ],
        [
         null,
         "S&P merged with IHS Markit unlocking massive market opportunities, but it also created organizational and technology challenges that made it difficult to centralize all their data and make it readily available for analytics and AI. This impacted cross-team collaboration and limited their ability to provide innovative solutions for their clients looking to make smarter, sustainable investment choices. There were infrastructure complexities and they significantly increased data volumes that limited their ability to democratize data and insights. Their legacy data warehouse also struggled with unifying alternative data sources with other data sets. This impacted various use cases across the business including credit risk, ESG, and Marketplace Workbench. The Sustainable1 team has spearheaded getting all their datasets readily available for sharing across all of their internal divisions via Delta Share, and also to share S&P data externally with their customers. "
        ],
        [
         "How to Create and Manage a High-Performance Analytics Team",
         "Data Science and analytics teams are unique. Large and small corporations want to build and manage analytics teams to convert their data and analytic assets into revenue and competitive advantage, but many are failing before they make their first hire. In this presentation, the audience will learn how to structure, hire, manage and grow an analytics team. Organizational structure, project and program portfolios, neurodiversity, developing talent, and more will be discussed. Questions and discussion will be encouraged and engaged in. The audience will leave with a deeper understanding of how to succeed in turning data and analytics into tangible results."
        ],
        [
         "What it Takes to Democratize AI and ML in a Large Company: The Importance of User Enablement and Technical Training",
         "The biggest critical factor to success in a Cloud transformation is people. As such, having a change management process in place to manage the impact of the transformation and user enablement is foundational to any large program. In this session we will dive into how TD bank democratizes data, mobilizes our 2000+ analytics user community and the tactics we used to successfully enable new use cases on Cloud. The session will focus on\n \n To democratize data:\n • Centralize a data platform that is accessible to all employees and allow for easy data sharing \n • Implement privacy and security to protect data and use data ethically\n • Compliance and governance for using data in responsible and compliant way\n • Simplification of processes and procedures to reduce redundancy and faster adoption\n\n To mobilize end users:\n • Increase data literacy: provide training and resources for employees to increase their abilities and skills\n • Foster a culture of collaborationand openness: cross-functional teams to collaborate and share ideas\n • Encourage exploration of innovative ideas that impact the organization's values and customers\n \nTechnical enablement and adoption tactics we've used at TD Bank:\n • Hands-on training for over 1300+ analytics users with emphasis on learn by doing, to relate to real-life situations\n • Online tutorials and documentations to be used as self-paced study \n • Workshops and office hours on specific topics to empower business users\n • Coaching to work with teams on a specific use case/complex issue and provide recommendations for a faster, cost effective solutions\n • Offer certification and encourage continuous education for employees to keep up to date with latest \n • Feedback loop: get user feedback on training and user experience to improve future trainings"
        ],
        [
         "Weaving the Data Mesh in the Department of Defense",
         "The Chief Digital and AI Office (CDAO) was created to lead the strategy and policy on data, analytics, and AI adoption across the Department of Defense. To enable that vision, the Department must achieve new ways to scale and standardize delivery under a global strategy while enabling decentralized workflows that capture the wealth of data and domain expertise. \n \n CDAO’s strategy and goals are aligned with data mesh principles. This alignment starts with providing enterprise-level infrastructure and services to advance the adoption of data, analytics, and AI, creating the self-service data infrastructure as a platform. And it continues through implementing policy for federated computational governance centered around decentralizing data ownership to become domain-oriented but enforcing the quality and trustworthiness of data. CDAO seeks to expand and make enterprise data more accessible through providing data as a product and leveraging a federated data catalog to designate authoritative data and common data models. This results in domain-oriented, decentralized data ownership to empower the business domains across the Department to increase mission and business impact that result in significant cost savings, saving lives, and data serving as a “public good.”\n \n Please join us in our presentation as we discuss how the CDAO leverages modern, innovative implementations that accelerate the delivery of data and AI throughout one of the largest distributed organizations in the world; the Department of Defence.  We will walk through how this enables delivery in various Department of Defence use cases."
        ],
        [
         "The Modern Composable CX Stack: Deconstructing the Data Lifecycle From Infrastructure to Orchestration",
         " In today’s privacy-conscious world, it’s imperative for IT to have ownership and full control of sensitive customer data. But more than ever, business stakeholders are in dogged pursuit of customer data that can drive better customer understanding and more effective campaigns. For IT, maintaining the integrity of data and architecture while driving customer 360 and other customer experience initiatives can become costly, time consuming and a source of friction in the organization.\n \n Hear from ActionIQ’s SVP of Product Justin DeBrabant and Northwestern Mutual’s Chief Data Officer Don Vu as they discuss how Northwestern Mutual is leveraging Databricks and ActionIQ’s CX Hub to bring together business and technical teams around existing data and architecture investments while enabling powerful CX initiatives.\n \n You’ll learn:\n - How to maintain IT choice, agility and control across the stack with composable technology \n - Strategies to optimize and showcase existing data investments of IT while elevating the stack \n - Tactics to drive business impact from data management, governance and modernization initiatives"
        ],
        [
         "Leveraging Product Thinking to Scale a Data Platform to a 400-person (Or More!) Data & Analytics Team",
         "There exists a large and complex set of big data and analytics tools. How do you decide which tools to use? When to use them? In what way? And how to ensure availability and productivity? In this session we'll talk about how we organized the data platform team so that we could provide the best data environment for the 400+ engineers and data scientists at Anheuser-Busch InBev. \n \n Data platforms are generally a set of tools that enable data analysis activities. They are built on many different technologies and processes. How to guarantee reliability and constant evolutions for our users was the problem we started to face during the first months after the launch of our Data Platform. To solve this issue, we used an agile mindset and product management framework, already consolidated in software management teams. As a result, we divided the data platform team into squads, each one having its own OKRs and backlog aimed at improving user productivity. \n \n Having well-defined responsibilities for each squad according to the stages of the data journey within the platform made it possible to establish clear priorities for evolution in terms of benefits generated for users and the company. This also allowed us to have a highly specialized operations team to each part of the architecture, as each squad is responsible for keeping its product up and running. \n \n In this session, we will show the evolution of the team's structure over three years, as well as the challenges encountered. In addition, we will show how the daily work of the squads is organized."
        ],
        [
         "Experience the New Era of Data & AI: Taking Bold Steps",
         "Being an organization operating in >65 countries, PETRONAS has a complex and  growing data landscape across its value chain. It is undoubted that digital transformation is a necessity; however, its success is underpinned by data being a strategic enabler. At PETRONAS, we took bold steps in pioneering new frontiers through the undertaking of several strategic initiatives since 2020 to accelerate the digital transformation in PETRONAS:\n \n 1. Liberalization of data & analytics\n Data & analytics are made available and accessible across PETRONAS through a paradigm shift in ways of working with PETRONAS having 'default access rights' to data.\n  \n 2. Insight velocity via single source of truth data platform for analytics\n Enterprise data hub as a dual cloud-based data platform with integrated capabilities and robust technology stacks, whereby data (internal and external) and analytic products that brings the biggest impact and value is curated for insights.\n \n 3. Analytics for everyone\n Everyone can now step-up in using data and advanced analytics without the need for programming skillsets through advanced analytics in EDH that is low-code and augmented by AI\n \n 4. Line of sight across multiple data platforms and applications\n Data Governance Control Tower to provide the line of sight on areas such as metadata management, data access, security and data quality across all data platforms and applications\n \n 5. Modernized data governance adaptive data governance; a modern approach, with centralized governance and decentralised assurance to achieve a centralized, inclusive and resilient data governance, while removing bottlenecks and breaking down siloes through federated execution across the business\n \n PETRONAS has achieved major milestones with the above. As such, we would like to share our knowledge & expertise."
        ],
        [
         "Advancing Customer Centricity: Mission Data's Data & Analytics Transformation",
         "Overview - \n Mission Data was a three-year transformation focused on leveraging data to better understand our members and engage with them in a personalized way to help them better manage their finances. We’re accomplishing this by transforming Navy Federal into a member-centric, data-driven and AI-powered organization. Leveraging member data in new and innovative ways allows us to create a superior member experience by delivering value back to our members to show them that we know them, and this has always been at the core of our Member Centric strategy. Due to the evolving nature of the broader technological marketplace, consumers have come to expect increasing degrees of personalization across their experiences. Navy Federal members expect the same from their financial institutions and Mission Data is enabling the financial service and guidance our members have come to expect through personalized insights. A personalized insight takes the guess work out of managing finances for members. We show them we know them AND we do it for them…From providing target savings guidance, tracking financial goals, or letting members know they’ve reached a milestone achievement. Delivering these types of insights tailored to our member’s individual needs ultimately deepens their relationship and loyalty with Navy Federal as their trusted financial partner. This is an effort we’re accelerating as we democratize the Mission Data Platform—making member data accessible and actionable across the enterprise. In 2019, we developed a strategy and roadmap for Navy Federal to build the data and analytics capabilities needed to deliver these types of experiences. Key Enablers in the Journey: People, Mindset, Change, Adoption."
        ],
        [
         "The C-Level Guide to Data Strategy Success With the Lakehouse",
         "Join us for a practical session on implementing a data strategy leveraging people, process, and technology to meet the growing demands of your business stakeholders for faster innovation at lower cost. Dael Williamson and Robin Sutara, EMEA Field CTOs at Databricks, will share real-world examples on best practices and things to avoid as you drive your strategy from the board to the business units in your organization"
        ],
        [
         "Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse",
         "Abstract\n How customers implemented Data Mesh on Databricks and how standardizing on delta format enabled Delta-to-Delta Share to non-Databricks consumers.\n \n Presentation Agenda:\n \n 1. Current State of the IT Landscape\n a. Data Silos (problems with organizations not having connected data in the ecosystem)\n b. A look back on why we moved away from Data warehouses and choose cloud in the first place\n c. What caused the data chaos in the cloud (instrumentation and too much stitching together) ~ periodic table list of services of the cloud\n \n 2. How to strike the balance between Autonomy and Centralization\n \n 3. Why Databricks Unity Catalog puts you in the right path to implementing Data Mesh strategy\n \n 4. What are the process and features that enable and end-to-end Implementation of a Data Strategy\n \n 5. How customers were able to successfully implement the Data mesh on out of the box Unity Catalog and Delta Sharing without overwhelming their IT tool Stack\n \n 6. Use-Cases\n a. Delta-to-Delta Data Sharing \n b. Delta-to-Others Data Sharing\n \n 7. How do you navigate when data today is available across regions, across clouds,on-prem and external systems\n a. Change Data Feed to share only “data that has changed”\n \n 8. Data stewardship\n a. Why ABAC is important\n b. How file based access policies and governance play an important role\n \n 9. Future State and its pitfalls\n a. Egress Costs\n b. Data compliances"
        ],
        [
         "Event Driven Real-Time Supply Chain Ecosystem Powered by Lakehouse",
         "As the backbone of Australia’s supply chain, the Australia Rail Track Corporation (ARTC) plays a vital role in the management and monitoring of goods transportation across 8,500km of its rail network throughout Australia. ARTC provides weighbridges along their track which read train weights as they pass at speeds of up to 60 kilometers an hour. This information is highly valuable and is required both by ARTC and their customers to provide accurate haulage weight details, analyze technical equipment, and help ensure wagons have been loaded correctly. \n \n 750 trains run across a network of 8500 km in a day and generate real-time data at approximately 50 sensor platforms. \n \n With the help of structured streaming and Delta Lake, ARTC was able to analyze and store: \n \n 1. Precise train location\n 2. Weight of the train in real-time\n 3. Train crossing time to the second level \n 4. Train speed, temperature, sound frequency, and friction \n 5. Train schedule lookups \n \n Once all the IoT data has been pulled together from an IoT event hub, it is processed in real-time using structured streaming and stored in Delta Lake. To understand the train GPS location API calls are then made per minute per train from the Lakehouse. API calls are made in real-time to another scheduling system to lookup customer info. Once the processed, enriched, data is stored in Delta Lake, an API layer was also created on top of it to expose this data to all consumers. \n \n Outcome:\n \n Increased transparency on weight data as it is now made available to customers. \nA digital data ecosystem that now ARTC’s customers use to meet their KPIs/ planning. \nAbility to determine temporary speed restrictions across the network to improve train scheduling accuracy and also schedule network maintenance based on train schedules/ speed."
        ],
        [
         "Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark™",
         "Apache Spark™’s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important.\n \n Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa’s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data."
        ],
        [
         "Taking Control of Streaming Healthcare Data",
         "Chesapeake Regional Information System for our Patients (CRISP), a nonprofit healthcare information exchange (HIE), initially partnered with Slalom to build a Databricks data lakehouse architecture in response to the analytics demands of the COVID-19 pandemic, since then they have expanded the platform to additional use cases. Recently they have worked together to engineer streaming data pipelines to process healthcare messages, such as HL7, to help CRISP become vendor independent.\n \n Our talk will focus on the improvements CRISP has made to their data lakehouse platform to support streaming use cases and the impact these changes have had for the organization. We will touch on using Databricks Auto Loader to efficiently ingest incoming files, ensuring data quality with Delta Live Tables, and sharing data internally with a SQL warehouse, as well as some of the work CRISP has done to parse and standardize HL7 messages from hundreds of sources. These efforts have allowed CRISP to stream over 4 million messages daily in near real-time with the scalability it needs to continue to onboard new healthcare providers so it can continue to facilitate care and improve health outcomes."
        ],
        [
         "Deploying the Lakehouse to Improve the Viewer Experience on Discovery+",
         "In this session we will discuss how real-time data streaming can be used to gain insights into user behavior and preferences, and how this data is being used to provide personalized content and recommendations on discovery+. We will examine techniques that enables faster decision making and insights on accurate real time data including data masking and data validation. To enable a wide set of data consumers from data engineers to data scientists to data analysts, we will discuss how Unity Catalog is leveraged for secure data access and sharing while still allowing teams flexibility. Operating at this scale requires examining the value being created by the data being processed and optimizing along the way and we will share some of our success in this area."
        ],
        [
         "Apache Spark™ Streaming and Delta Live Tables Accelerates KPMG Clients For Real-Time IoT Insights",
         "Unplanned downtime in manufacturing costs firms up to a trillion dollars annually. Time that materials spend sitting on a production line is lost revenue. Even just 15 hours of downtime a week adds up to over 800 hours of downtime yearly. The use of internet of things or IoT devices can cut this time down by providing details of machine metrics. However, IoT predictive maintenance is challenged by the lack of effective, scalable infrastructure and machine learning solutions. IoT data can be the size of multiple terabytes per day and can come in a variety of formats. Furthermore, without any insights and analysis, this data becomes just another table. \n \n The KPMG Databricks IoT Accelerator is an end-to-end solution enabling manufacturing plant operators to have a bird’s eye view of their machines’ health and empowers proactive machine maintenance across their portfolio of IoT devices. The Databricks Accelerator ingests IoT streaming data at scale and implements the Databricks medallion architecture while leveraging delta live tables to clean and process data. Real time machine learning models are developed from IoT machine measurements and are managed in MLflow. The AI predictions and IoT device readings are compiled in the gold table powering downstream dashboards like Tableau. Dashboards inform machine operators of not only machines’ ailments, but action they can take to mitigate issues before they arise. Operators can see fault history to aid in understanding failure trends, and can filter dashboards by fault type, machine, or specific sensor reading."
        ],
        [
         "Optimizing batch and streaming aggregations",
         "I've got a client recently who asked me to optimize their batch and streaming workloads. It happened to be aggregations using DataFrame.groupBy operation with a custom Scala UDAF over a data stream from Kafka. Just a single simple-looking request that turned itself up into a a-few-month-long hunt to find a more performant query execution planning than ObjectHashAggregateExec that kept falling back to a sort-based aggregation (i.e. the worst possible aggregation runtime performance). It quickly taught us that an aggregation using a custom Scala UDAF cannot be planned other than ObjectHashAggregateExec but at least tasks don't always have to fall back. And that's just batch workloads. When you throw in streaming semantics and think of the different output modes, windowing and streaming watermark optimizing aggregation can take a long time to do right.\n \n During the talk you'll learn the different execution strategies of aggregation (groupBy) in Apache Spark and what issues I faced while hunting down the tiniest performance improvements. You will learn why an aggregation with a Scala UDAF cannot be planned using HashAggregateExec and what problems you may come across while using streaming aggregation over the built-in Kafka data source. Lots of optimization tips and tricks with quite a few looks at the internals of aggregation query execution."
        ],
        [
         "Structured Streaming: Demystifying Arbitrary Stateful Operations",
         "Let’s face it -- data is messy. And your company’s business requirements? Even messier! You’re staring at your screen, knowing there is a tool that will let you give your business partners the information they need as quickly as they need it. There’s even a Python version of it now. But…it looks kinda scary. You’ve never used it before, and you don’t know where to start. Yes, we’re talking about the dreaded flatMapGroupsWithState! But fear not - we’ve got you covered. In this session we’ll take a real-word use case and use it to show you how to break down flatMapGroupsWithState into its basic building blocks. We’ll explain each piece in both Scala and the newly-released Python, and at the end we’ll illustrate how it all comes together to enable the implementation of arbitrary stateful operations with Spark Structured Streaming."
        ],
        [
         "How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience",
         "Disney+ uses Amazon Kinesis and Databricks Apache Spark™ streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers."
        ],
        [
         "The Future is Open: Data Streaming in an Omni-Cloud Reality",
         "An assortment of low-code connectors boasts the ability to make data available for analytics in real time. However, many such turn-key solutions face challenges: cost-inefficient EDW targets; inability to evolve schema; forbiddingly expensive data exports due to cloud and vendor lock-in. The alternative: an open data lake that unifies batch and streaming workloads. Increasingly, we must adapt to a multi-cloud reality. Delta Lake decouples data storage from proprietary formats, dramatically reducing data extraction costs. Exporting data from delta tables eliminates the compute and storage API costs required by EDW. Bronze landing zones in Delta format build the foundation for a medallion architecture. Apache Spark™ Structured Streaming provides a unified ingestion strategy. Streaming triggers allow us to switch back and forth between batch and stream with one-line code changes. Streaming aggregation enables us to incrementally compute on data that arrives near each other.\n It is a misconception that streaming requires always-on clusters and thus is prohibitively expensive. Streaming means the ability to automatically handle new data and avoid reprocessing of historical data; real time is optional depending on use cases. Autoloader is a technique to discover newly arrived data and ensure exactly once, incremental processing. DLT further simplifies streaming jobs, accelerating the development cycle.\n Workflows bring it all together and the open source dbx project applies SWE best practices. Workflows can orchestrate tasks such as DLT pipelines, spark jobs, SQL scripts, as well as dbt transformations. Developers can define compute resources, scheduling, and dependencies in descriptive yaml format. Both task definitions and job code are versioned with popular source control systems."
        ],
        [
         "Getting Insight From Anything: Gathering Data With IoT Devices and Delta Live",
         "The drive to make data-driven decisions shapes the strategy of many companies. However, this dream has a problem; just because you need to make a decision does not mean you have the data to make it! Often these decisions require data from sources typically unconnected to the internet. This could be an architect determined to understand how the construction of her building is progressing, or a manufacturer wanting to live their best Industry 4.0 life. It's not uncommon for companies to find this journey to become “smart” is a long one; with difficult ethical ground to cover, or prohibitive costs associated with procuring the necessary equipment. However, the development of cheap and accessible IoT devices allows for the gathering of data from anything to be more within reach than ever.\n \n This talk will look at efforts to build proof-of-concept IoT systems. We will discuss our (continuing) journey of using and managing hardware and the problems we faced along the way. The main focus however will be the software stack. Azure IoT Hub is used for device management, with Power BI supporting the front end. However, the centre-piece is Delta Live Tables - allowing us a method of near real-time analysis of our data. We will discuss the advantages of using Delta Live for this, and how we wish to extend our IoT systems in the future."
        ],
        [
         "Practical Pipelines: A Houseplant Alerting System With ksqlDB",
         "Houseplants can be hard; in many cases, over-watering and under-watering can have the same symptoms. Take away the guesswork involved in caring for your houseplants while also gaining valuable experience in building a practical, event-driven pipeline in your own home! This session explores the process of building a houseplant monitoring and alerting system using a Raspberry Pi and Apache Kafka. \n \n Moisture and temperature readings are captured from sensors in the soil and streamed into Kafka. From here, we’ll use stream processing to transform the data, create a summary view of the current state, and drive real-time push alerts through Telegram. In this session, I’ll talk about how I ingest the data followed by the tools – including ksqlDB and Kafka Connect – that help transform the raw data into useful information, and finally, I’ll show how to use Kafka Producers and Consumers to make the entire application more interactive.\n \n By the end of this session you’ll have everything you need to start building practical streaming pipelines in your own home. Roll up your sleeves – let’s get our hands dirty!"
        ],
        [
         "Disaster Recovery Strategies for Structured Streams",
         "In recent years, many businesses have adopted real-time streaming applications to enable faster decision making, quicker predictions and improved customer experiences. Few of these applications are driving critical business use cases like financial fraud detection, loan application processing, personalized offers, etc. These business critical applications need robust disaster recovery strategies to recover from the catastrophic events to reduce the lost uptime. However, most organizations find it hard to set up disaster recovery for streaming applications as it involves continuous data flow. Streaming state and temporal behavior of data brings additional complexities to the DR strategy.\n \n A reliable disaster recovery strategy includes backup, failover and failback approaches for the streaming application. Unlike the batch applications, these steps include many moving elements and need a very sophisticated approach to ensure that the services are failing over the DR region and meet the set RTO and RPO requirements. In this session, we will cover following topics with a FINSERV use case demo:\n 1. Backup strategy: backup of delta tables, message bus services and checkpoint including offsets\n 2. Failover strategy: failover strategy to disable services in the primary region and start the services in the secondary region with minimum data loss \n 3. Failback strategy: failback strategy to restart the services in the primary region once all the services are restored \n 4. Common challenges and best practices for backup"
        ],
        [
         "Near Real-Time Data Linkage and Entity Resolution for High Volumes of Continuous and Unsynchronized Data Streams",
         "Citrix Analytics is a cloud-based service that provides analytical insights from data across Citrix product portfolio. It collates data from various sources and proactively detects security threats.\n \n The data consists of information from various entities like users, devices, applications, network, files and shares, along with their correlation over a period. This data is analyzed by various ML algorithms to detect any suspicious activity. The ML models need correlated data in real-time so vulnerabilities can be detected instantly and remediation actions can be taken.\n \n The data pipeline comprises of multiple event sources, each one of them producing high volumes of continuous data. This data generated from different sources need to be correlated using common identifiers to produce final stream of data with low latency and high data quality.\n \n The proposed solution is to build scalable, extensible, fault-tolerant system for stateful joining of two or more high volume event streams that are not fully synchronised, event ordering is not guaranteed, and certain events arrive a bit late. The system also ensures to combine the events/link so the data is in near real-time with low latency so that there is no impact on the downstream applications like ML models in determining the suspicious behavior. Apart from combining events the system also ensures to propagate the needed entities to other product streams to help in entity resolution. If any of the needed data is yet to arrive, we can configure few parameters based on use cases, so as to achieve the required eventual or attribute consistency.\n \n The architecture is an agnostic streaming framework, but our implementation leverages Spark Streaming and Kafka. The solution is implemented and is processing billions of events per day."
        ],
        [
         "Realtime ML in Marketplace @ Lyft",
         "Lyft is a ride-sharing company which is a two-sided marketplace; balancing supply and demand using various levers (passenger pricing, driver incentive etc.) to maintain an efficient system. Lyft has built a real-time optimization platform that helps to build the product faster. This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. This infrastructure consumes a massive number of events from different sources to make real-time product decisions.\n Rakesh discusses how Lyft organically evolved and scaled the streaming platform that provides a consistent view of the marketplace to aid an individual team independently run their optimization. The platform offers online and offline feature access that helps teams to back test their model in the future. It provides various other powerful capabilities such as replaying the production ML feature in PyNotebook, feature validation, near-realtime model training, executing multi-layer of models in a DAG, etc. \n He is also going to elaborate things that helped him scale the systems to process millions of events per minute and power T0 products with tighter latency SLA."
        ],
        [
         "Streaming Data Analytics With Power BI and Databricks",
         "In this session we will present a series of end-to-end technical demos illustrating the synergy between Databricks and Power BI for streaming use cases. \n  - Scenario 1: DLT + Power BI Direct Query and Auto Refresh\n  - Scenario 2: Structured Streaming + Power BI streaming datasets \n  - Scenario 3: DLT + Power BI composite datasets\n  - Considerations when to choose which scenario"
        ],
        [
         "How Coinbase Built and Optimized SOON, a Streaming Ingestion Framework",
         "Data with low latency is important for real-time incident analysis and metrics. Though we have up-to-date data in OLTP databases, they cannot support those scenarios. Data need to be replicated to a data warehouse to serve queries doing group-bys, and joins across tables from different systems. At Coinbase, we designed SOON (Spark cOntinuOus iNgestion) based on Kafka, Kafka Connect, and Spark as an incremental table replication solution to replicate tables of any size from any database to Delta Lake in a timely manner. It also supports Kafka events ingestion naturally.\n \n SOON incrementally ingests Kafka events as appends, updates, and deletes to an existing table on Delta Lake. The events are grouped into two categories: CDC (change data capture) events generated by Kafka Connect source connectors, and non-CDC events by the frontend or backend services. Both types can be appended or merged into the Delta Lake. Non-CDC events can be in any format, but CDC events must be in the standard SOON CDC schema. We implemented Kafka Connect SMTs to transform raw CDC events into this standardized format. SOON unifies all streaming ingestion scenarios such that users only need to learn one onboarding experience and the team only needs to maintain one framework.\n \n We care about the ingestion performance. The biggest append-only table onboarded has ingress traffic at hundreds of thousands events per second; the biggest CDC-merge table onboarded has a snapshot size of a few TBs and CDC update traffic at hundreds of events per second. A lot of innovative ideas are incorporated in SOON to improve its performance, such as min-max range merge optimization, KMeans merge optimization, no-update merge for deduplication, generated columns as partitions, etc."
        ],
        [
         "High Volume Intelligent Streaming with Sub-Minute SLA for Near Real-Time Data Replication",
         "Come and learn an innovative solution built around Databricks structured streaming and Delta Live Tables (DLT) to replicate thousands of tables from on-premisis to cloud-based relational databases. A highly desirable pattern for many enterprises across the industries to replicate on-premises data to cloud-based data lakes and data stores in near real time for consumption. This powerful architecture can offload legacy platform workloads and accelerate cloud journey. The intelligent cost-efficient solution leverages thread-pools, multi-task jobs, Kafka, spark structured streaming and DLT. This session will go into detail about problems, solutions, lessons-learned and best practices."
        ],
        [
         "Using Cisco Spaces Firehose API as a Stream of Data for Real-Time Occupancy Modelling",
         "Honeywell manages the control of equipment for hundreds of thousands of buildings worldwide. Many of our outcomes relating to energy and comfort rely on knowing where people are in the building at any one time. This is so we can target health and comfort conditions more suitably to areas where are more densely populated. Many of these buildings have Cisco IT infrastructure in them. Using their WIFI points and the RSSI signal strength from people’s laptops and phones, Cisco can calculate the number of people in each area of the building. Cisco Spaces offer this data up as a real-time streaming source. Honeywell HBT has utilised this stream of data by writing delta live table pipelines to consume this data source.\n Honeywell buildings can now receive this firehose data from hundred of concurrent customers and provide this occupancy data as a service to our vertical offerings in commercial, health, real estate and education. We will discuss the benefits of using DLT to handle this sort of incoming stream data, and illustrate the pain points we had and the resolutions we undertook in successfully receiving the stream of Cisco data. We will illustrate how our DLT pipeline was designed, and how it scaled to deal with huge quantities of real time streaming data."
        ],
        [
         "Leveraging IoT Data at Scale to Mitigate Global Water Risks Using Apache Spark Streaming and Delta",
         "Every year, billions of dollars are lost due to water risks from storms, floods, and droughts. Water data scarcity and excess are issues that risk models cannot overcome, creating a world of uncertainty. Divirod is building a platform of water data by normalizing diverse data sources of varying velocity into one unified data asset. In addition to publicly available third-party datasets, we are rapidly deploying our own IoT sensors. \n \n These sensors ingest signals at a rate of about 100,000 messages per hour into preprocessing, signal-processing, analytics, and postprocessing workloads in one spark-streaming pipeline to enable critical real-time decision-making processes. By leveraging streaming architecture, we were able to reduce end-to-end latency from 10s of minutes to just a few seconds. \n \n We are leveraging Delta to provide a single query interface across multiple tables of this continuously changing data. This enables data science and analytics workloads to always use the most current and comprehensive information available. In addition to the obvious schema transformations, we implement data-quality metrics and datum conversions to provide a trustworthy unified dataset."
        ],
        [
         "Unlocking Near Real-Time Data Replication with CDC, Apache Spark Streaming, and Delta Lake",
         "Tune into DoorDash's journey to migrate from a flaky ETL system with 24-hour data delays, to standardizing a CDC streaming pattern across 150+ databases to produce near real-time data in a scalable, configurable, and reliable manner. During this journey, understand how we use Delta Lake to build a self-serve, read-optimized data lake with data latencies of 15 minutes, whilst reducing operational overhead. Furthermore, understand how certain tradeoffs like conceding to a non-real-time system allow for multiple optimizations but still permit for OLTP query use-cases, and the benefits it provides."
        ],
        [
         "Streaming Schema Drift Discovery and Controlled Mitigation",
         "\"When creating streaming work loads with Databricks, it can sometimes be difficult to capture and understand the current structure of your source data. For example, what happens if you are ingesting JSON events from a vendor, and the keys are very sparsely populated, or contain dynamic content? Ideally, data engineers want to \"\"lock in\"\" a target schema in order to minimize complexity and maximize performance for known access patterns. What do you do when your data sources just don't cooperate with that vision?"
        ],
        [
         " \n The first step is to quantify how far your current source data is drifting from your established Delta table. But how? In this talk I will demonstrate a way to capture and visual drift across all of your streaming tables. The next question is",
         " \"\"Now that I see all of the data I'm missing"
        ],
        [
         "Real-Time Streaming Solution for a Call Center Analytics: Business Challenges and Technical Enablement",
         "A large international client with a business footprint in North America, Europe and Africa, reached out to us with an interest in having a real-time streaming solution designed and implemented for its call center handling incoming and outgoing client calls. The client had a previous bad experience with another vendor, who overpromised and underdelivered on the latency of the streaming solution. The previous vendor delivered an over-complex streaming data pipeline resulting in the data taking over 5 minutes to reach a visualization layer. The client felt that architecture was too complex and involved many services integrated together. Our immediate challenges involved gaining the client's trust and proving that our design and implementation quality would supercede a previous experience. To resolve an immediate challenge of the overly complicated pipeline design, we deployed a Databricks Lakehouse architecture with Azure Databricks at the center of the solution. Our reference archiitecture integrated Genesys Cloud -> App Services -> Event Hub -> Databricks <-> Data Lake -> Power BI. The streaming solution proved to be low latency (seconds) during the POV stage, which led to subsequent productionalization of the pipeline with deployment of jobs, DLTs pipeline, including multi-notebook workflow and business and performance metrics dashboarding relied on by the call center staff for a day-to-day performance monitoring and improvements."
        ],
        [
         "Top Mistakes to Avoid in Streaming Applications",
         "Nowadays, streaming use cases become very important in almost all companies. When you think of streaming, Apache Spark™ Structured Streaming is at the top of the list in the technology. As we are dealing with many customer streaming use cases, we learned many do’s and don’t of the streaming application. I will be discussing the top mistakes to avoid when working with streaming applications with regard to different sources and sinks like DLT, Kafka, Delta, and so on. If you are avoiding these mistakes while architecting/running/restarting the application, then you will avoid the costly mistakes at a later point in time."
        ],
        [
         "Embracing the Future of Data Engineering: The Serverless, Real-Time Lakehouse in Action",
         "As we venture into the future of data engineering, streaming and serverless technologies take center stage. In this fun, hands-on, in-depth and interactive session you can learn about the essence of future data engineering today. In this session we will tackle the challenge of processing streaming events created by hundreds of sensors in the conference room from a serverless web app (bring your phone and be a part of it!). The focus is on the system architecture and the involved products. Which Databricks product, capability and settings will be most useful for our scenario? What does streaming really mean and why does it make our life easier? What are the exact benefits of serverless and how much serverless is a particular solution?\nLeveraging the power of the Databricks Lakehouse Platform, I will demonstrate how to create a streaming data pipeline with delta live tables ingesting data from AWS Kinesis. Further, I’ll utilize advanced Databricks workflows triggers for efficient orchestration and real-time alerts feeding into a real-time dashboard. And since I don’t want you to leave with empty hands - I will use Delta Sharing to share the results of the demo we built with every participant in the room.\nJoin me in this hands-on exploration of cutting-edge data engineering techniques and witness the future in action!"
        ],
        [
         "Building and Managing Data Platform for 13+ PB Delta Lake and 1000’s of Users:  AT&T'S Story",
         "Data runs AT&T’s business, just like it runs most businesses these days. Data can lead to a greater understanding of a business and when translated correctly into information can provide human and business systems valuable insights to make better decisions. Unique to AT&T is the volume of data we support, how much of our work that is driven by AI and the scale at which data and AI drive value for our customers and stakeholders. Our Cloud migration journey includes making data and AI more accessible to employees throughout AT&T so they can use their deep business expertise to leverage data more easily and rapidly. We always had to balance this data democratization and desire for speed with keeping our data private and secure. We loved the open ecosystem model of Lakehouse that enables data , BI and ML tools to be seamlessly integrated on a single pane arena, it just simplifies the architecture and reduces dependencies between technologies in the cloud. Being clear in our architecture guidelines and patterns was very important to us for our success. We are seeing more interest from our business unit partners and continuing to build the AI as a service capabilities to support more citizen data scientists. To scale up our Lakehouse journey , we built a Databricks center of excellence (CoE) function in AT&T which today has ~1400+ active members , further concentrating existing expertise and resources in ML/AI discipline to collaborate on all things Databricks like technical support, trainings , FAQ’s and best practices to attain and sustain world-class performance and drive business value for AT&T. Join us to learn more about how we process and manage 10+ Petabytes of our network Lakehouse with Delta Lake and Databricks."
        ],
        [
         "Data Democratization with Lakehouse: An Open Banking Application Case",
         "\"Banco Bradesco represents one of the largest companies in the financial sector in Latin America. They have more than 99 million customers, 79 years of history, and a legacy of data distributed in hundreds of on-premises systems. With the spread of data-driven approaches and the growth of cloud computing adoption, we needed to innovate and adapt to new trends and enable an analytical environment with democratized data.  We will show how more than 8 business departments have already engaged in using the Lakehouse exploratory environment, with more than 190 use cases mapped and a multi-bank financial manager. Unlike with on-premises, the cost of each process can be isolated and managed in near real-time, allowing quick responses to cost and budget deviations, while increasing the deployment speed of new features 36 times compared to on-premises. The data is now used and shared safely and easily between different areas and companies of the group. Also, the view of dashboards within Databricks allows panels to be efficiently \"\"prototyped\"\" with real data"
        ],
        [
         "Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying",
         "Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks’ dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia’s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies,  including success stories that resulted in significant cost savings and efficiency gain."
        ],
        [
         "How RecRoom Processes Billions of Events Per Day With Databricks and RudderStack",
         "Learn how Rec Room, a fast-growing augmented and virtual reality software startup, is saving 50% of their engineering team's time by using Databricks and RudderStack to power real-time analytics and insights for their 85 million gaming customers. In this session, you will walk through a step-by-step explanation of how Rec Room set up efficient processes for ingestion into their data lakehouse, transformation, reverse-ETL and product analytics. You will also see how Rec Room is using incremental materialization of tables to save costs and establish an uptime of close to 100%."
        ],
        [
         "Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?",
         "Rust is a unique language whose traits make it very appealing for data engineering. In this session, we'll walk through the different aspects of the language that make it such a good fit for big data processing including: how it improves performance and how it provides greater safety guarantees and compatibility with a wide range of existing tools that make it well positioned to become a major building block for the future of analytics. \n \n We will also take a hands-on look through real code examples at a few emerging technologies built on top of Rust that utilize these capabilities, and learn how to apply them to our modern lakehouse architecture."
        ],
        [
         "Making the Shift to Application-Driven Intelligence",
         "In the digital economy, application-driven intelligence delivered against live, real-time data will become a core capability of successful enterprises. It has the potential to improve the experience that you provide to your customers and deepen their engagement. But to make application-driven intelligence a reality, you can no longer rely only on copying live application data out of operational systems into analytics stores.\n Rather, it takes the unique real-time application-serving layer of a MongoDB database combined with the scale and real-time capabilities of a Databricks Lakehouse to automate and operationalize complex and AI-enhanced applications at scale. In this session we will show how it can be seamless for developers and data scientists to automate decisioning and actions on fresh application data and we'll deliver a practical demonstration on how operational data can be integrated in real time to run complex Machine Learning pipelines."
        ],
        [
         "Making Travel More Accessible for Customers Bringing Mobility Devices",
         "American Airlines takes great pride in caring for customers travel, and recognize the importance of supporting the dignity and independence of everyone who travels with us. As we work to improve the customer experience, we're committed to making our airline more accessible to everyone. Our work to ensure that travel that is accessible to all is well underway. We have been particularly focused on making the journey smoother for customers who rely on wheelchairs or other mobility devices. We have implemented the use of a bag tag specifically for wheelchairs and scooters that gives team members more information, like the mobility device’s weight and battery type, or whether it needs to be returned to a customer before a connecting flight.\n \n As a data engineering and analytics team, we at American Airlines are building a passenger service request data product that will provide timely insights on expected mobility device traffic at each airport so that the front-line team members can provide seamless travel experience to the passengers."
        ],
        [
         "Improve Apache Spark DS V2 Query Planning Using Column Stats",
         "When doing the TPC-DS benchmark using external V2 data source, we have observed that for several of the queries, DS V1 has better join plans than Spark. The main reason is that DS V1 uses column stats, especially number of distinct values (NDV) for query optimization. Currently, Spark DS V2 only has interfaces for data sources to report table statistics such as size in bytes and number of rows. In order to use column stats in DS V2, we have added new interfaces to allow external data sources to report column stats to Spark. For a data source with huge data, it’s always challenging to get the column stats, especially the NDV. We plan to calculate NDV using Apache DataSketches Theta sketch and save the serialized compact sketch in the statistics file. The NDV and other column stats will be reported to Spark for query plan optimization."
        ],
        [
         "Databricks Powered Enterprise Data Platform for Faster Insights with Optimal Cost",
         "A leading Australian Bank engaged Deloitte to review architecture of current enterprise data platform enabled using Databricks. Bank’s vision is to limit proliferation of diverse tools/technology while enabling “best of breed” capabilities in the bank’s next-gen data platform. To realize this vision, the enterprise data platform will use various Databricks capabilities: DLT (Delta Live tables), Unity Catalog, Delta Sharing, Databricks Dashboards and Databricks Alerts. Using of these Databricks high performance and scalable capabilities allows the bank to avoid proliferation of disparate tools and reduce a need for custom development or integration. \n \n During this session, Deloitte, Databricks and Australian Bank will present how Databricks can be leveraged to develop a robust, resilient and scalable data platform which allows an organization to achieve scaled data sharing, reducing lead time to generate insights via reports, dashboards, AI/ML and Data Sharing."
        ],
        [
         "Using Databricks to Power Insights and Visualizations on the S&P Global Marketplace",
         "In this session, we will explain the visualizations that serve to shorten the time to insight for our prospects and encourage potential buyers to take the next step and request more information from our commercial team. The S&P Global Marketplace is a discovery and exploration platform that enables prospective buyers and clients to easily search fundamental and alternative datasets from across S&P Global and curated third-party providers. It serves as a digital storefront that provides transparency into data coverage and use cases, reducing the time and effort for clients to find data for their needs. A key feature of Marketplace is our interactive data visualizations that provide insight into the coverage of a dataset and demonstrate how the dataset can be used to make more informed decisions.   \n \n  \n \n The S&P Global Marketplace’s interactive visualizations are displayed in Tableau and are powered by Databricks. The Databricks platform allows for easy integration of S&P Global data and provides a collaborative environment where our team of product managers and data engineers can develop the code to generate each visualization. The team utilizes the web interface to develop the queries that perform the heavy lifting of data transformation instead of performing these tasks in Tableau. The final notebook output is saved into a custom datamart (“golden table”) which is the source for Tableau. We also developed an automated process that refreshes the whole process to ensure Marketplace has up to date visualizations."
        ],
        [
         "Self-Service Geospatial Analysis Leveraging Databricks, Apache Sedona, and R",
         "Geospatial data analysis is critical to understanding the impact of agricultural operations on environmental sustainability with respect to water quality, soil health, greenhouse gasses, and more. Outside of a few specialized software products, however, support for spatial data types is often limited or missing from analytics and visualization platforms. In this session, we show how Truterra is using Databricks, Apache Sedona, and R to analyze spatial data at scale. Additionally, learn how Truterra uses spatial insights to educate and promote practices that optimize profitability, sustainability, and stewardship outcomes at the farm. In this session, you will see how Databricks and Apache Sedona are used to process large spatial datasets including field, watershed, and hydrologic boundaries. You will see dynamic widgets, SQL and R used in tandem to generate map visuals, display them, and enable download all from a Databricks notebook."
        ],
        [
         "Processing Delta Lake Tables on AWS Using AWS Glue, Amazon Athena, and Amazon Redshift",
         "Delta Lake is an open-source project that helps implement modern data lake architectures commonly built on cloud storages. With Delta Lake, you can achieve ACID transactions, time travel queries, CDC, and other common use cases on the cloud. There are a lot of use cases of Delta tables on AWS. AWS has invested a lot in this technology, and now Delta Lake is available with multiple AWS services, such as AWS Glue Spark jobs, Amazon EMR, Amazon Athena, and Amazon Redshift Spectrum. AWS Glue is a serverless, scalable data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources. With AWS Glue, you can easily ingest data from multiple data sources such as on-prem databases, Amazon RDS, DynamoDB, MongoDB into Delta Lake on Amazon S3 even without expertise in coding.\n \n This session will demonstrate how to get started with processing Delta Lake tables on Amazon S3 using AWS Glue, and querying from Amazon Athena, and Amazon Redshift. The session also covers recent AWS service updates related to Delta Lake."
        ],
        [
         "A Simple SQL Execution API for the Databricks Lakehouse Architecture",
         "Given the widespread support of REST architectures, data managed by the Databricks Lakehouse Platform can now be accessed from anywhere, making it easy to build data applications tailored to your needs. You can use the SQL Execution API to build web services-based applications, integrate with various applications and computing devices, create generic integration layers for enterprise services, or create custom client libraries for your programming language of choice.\n \n In this talk we are going to deep dive into the design and implementation of the SQL Execution API with a focus on:\n 1. The key API features which can be used to develop data applications.\n 2. The underlying Databricks architecture such as Cloud Fetch that we developed to enable this API."
        ],
        [
         "Best Exploration of Columnar Shuffle Design",
         "To significantly improve the performance of Spark SQL, there is a trend to offload Spark SQL execution to highly optimized native libraries or accelerators in past several years, like Photon from Databricks, Nvidia's Rapids plug-in, and Intel & Kyligence's initiated open source Gluten project. By the multi-fold performance improvement from these solutions, more and more Spark users have started to adopt the new technology. One characteristics of native libraries is that they all use columnar data format as the basic data format. It's because the columnar data format has the intrinsic affinity to vectorized data processing using SIMD instructions. While vanilla Spark's shuffle is based on spark's internal row data format. The high overhead of the columnar to row and row to columnar conversion during the shuffle makes reusing current shuffle not possible. Due to the importance of shuffle service in Spark, we have to implement an efficient columnar shuffle, which brings couple of new challenges, like the split of columnar data, or the dictionary support during shuffle.\n \n In this session, we will share 1) the exploration process of the columnar shuffle design during our Gazelle and Gluten development, and best practices for implementing the columnar shuffle service. We will also share how we learned from the development of vanilla Spark's shuffle, for example, how to address the small files issue then we will propose the new shuffle solution. We will show the performance comparison between Columnar shuffle and vanilla Spark's row-based shuffle. Finally, we will share how the new built-in accelerators like QAT and IAA in the latest Intel processor are used in our columnar shuffle service and boost the performance."
        ],
        [
         "What Happens When Curious and Knowledgeable Business People are Provided Access to the Right Data and Equipped With Simple Tools and Supported by Guidance From In-House Technical Experts",
         "Too often business decisions in large organizations are based on time consuming and labor-intensive data extracts, fragile excel or access sheets that require significant manual intervention. The teams that prepare these manual reports have invaluable heuristic knowledge that, when combined with meaningful data and tools, can make smart business decisions. \n \n \n Imagine a world where these business teams are empowered with tools that help them build meaningful reports despite their limited technical expertise.\n \n \n In this talk we will discuss : \n \n The value derived from investing in developing citizen data personas within a business organization\n \n How we successfully built a citizen data analytics culture within Michelin \n \n Real examples of the impact of this initiative on - the business and on the people themselves.\n \n The audience will walk away with some convincing arguments for building a citizen data culture in their organization and a how-to cookbook that they can use to cultivate citizen data personas.\n \n Finally, they will also have the opportunity to interactively uncover key success factors in the case of Michelin that can help drive a similar initiative in their respective companies."
        ],
        [
         "Modularized Approach to Scale Suite of Advanced Omnichannel Analytics Products",
         "GSK started the journey of Omnichannel vision a few years ago using static business rules and customer targeting. We kicked off 2022 with conceptualization of a suite of monitoring, measurement and planning products that would help GSK establish a comprehensive omnichannel measurement strategy and plan across its global markets. One of the key guiding principles during conceptualization was deeply and seamlessly embedding these products within GSK’s data transformation and storage architecture built on Azure Databricks. We built the platform powered extensively by Databricks, GitHub and open-source technologies like Python. In this session, we will cover the following aspects of our journey along with some best practices that we have learned over time."
        ],
        [
         "AI to FI with Databricks",
         "Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\n 1. Challenges of common pipeline design to support AI-FI \n 2. Challenges of leveraging data acquired over many acquisitions\n 3. How is the Lakehouse paradigm lending itself to solving these challenges?\n 4. Some cool results we can show the world.\n \n To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \n \n To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying."
        ],
        [
         "How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse",
         "Don't miss this session where we demonstrate how the Texas Rangers baseball team organized their predictive models by using MLFlow and the MLRegistry inside Databricks. They started using Databricks as a simple solution to centralizing our development on the cloud. This helped lessen the issue of siloed development in our team, and allowed us to leverage the benefits of distributed cloud computing. But we quickly found that Databricks was a perfect solution to another problem that we faced in our data engineering stack. Specifically, cost, complexity, and scalability issues hampered our data architecture development for years, and we decided we needed to modernize our stack by migrating to a lakehouse. With our lakehouse, ad-hoc-analytics, ETL operations, and MLOps all living within Databricks, development at scale has never been easier for our team. Going forward, we hope to fully eliminate the silos of development, and remove the disconnect between our analytics and data engineering teams. From computer vision, pose analytics, and player tracking, to pitch design, base stealing likelihood, and more, come see how the Texas Rangers are using innovative cloud technologies to create action-driven reports from the current sea of Big Data. "
        ],
        [
         "Improving Hospital Operations With Streaming Data and Real Time AI/ML",
         "Over the past two years, Providence has developed a robust streaming data platform (SDP) leveraging Databricks in Azure. The SDP enables us to ingest and process real-time data reflecting clinical operations across our 52 hospitals and roughly 1000 ambulatory clinics. HL7 messages generated by Epic are parsed using Databricks in our secure cloud environment and used to generate an up-to-the minute picture of exactly what is happening at the point of care. We are already leveraging this information to minimize hospital overcrowding and have been actively integrating AI/ML to accurately forecast future conditions (e.g., arrivals, length of stay, acuity, and discharge requirements). This allows us to both improve resource utilization (e.g., nurse staffing levels) and to optimize patient throughput. The result is both improved patient care and operational efficiency. In this session we will share how these outcomes are only possible with the power and elegance afforded by our investments in Azure, Databricks, and increasingly Lakehouse. We will demonstrate Providence's blueprint for enabling real-time analytics which can be generalized to other healthcare providers."
        ],
        [
         "AI Regulation is Coming: The EU AI Act and How Databricks Can Help with Compliance",
         "With the heightened attention on LLMs and what they can do, and the widening impact of AI on day-to-day life, the push by regulators across the globe to regulate AI is intensifying. As with GDPR in the privacy realm, the EU is leading the way with the EU Artificial Intelligence Act (AIA). Regulators everywhere will be looking to the AIA as precedent, and understanding the requirements imposed by the AIA is important for all players in the AI channel. Although not finalized, the basic framework regarding how the AIA will work is becoming clearer. The impact on developers and deployers of AI (‘providers’ and ‘users’ under the AIA) will be substantial. Although the AIA will probably not go into effect until early 2025, AI applications developed today will likely be affected, and design and development decisions made now should take the future regulations into account.\n\nIn this session, Matteo Quattrocchi, Brussels-based Director, Policy – EMEA, for BSA (the Software Alliance – the leading advocacy organization representing the enterprise software sector), will present an overview of the current proposed requirements under the AIA and give an update on the ongoing deliberations and likely timing for enactment. We will also highlight some of the ways the Lakehouse platform, including Managed MLflow, can help providers and users of ML-based applications meet the requirements of the AIA and other upcoming AI regulations.\n"
        ],
        [
         "Using Databricks to Develop Statistical and Mathematical Models to Forecast the Monkeypox Outbreak in Washington State",
         "In the spring and summer of 2022, monkeypox was detected in the United States and quickly spread throughout the country. To contain and mitigate the spread of the disease in Washington State, the Washington Department of Health data science team used the Databricks platform to develop a modeling pipeline that employed statistical and mathematical techniques to forecast the course of the monkeypox outbreak throughout the state. These models provided actionable information that helped inform decision making and guide the public health response to the outbreak. \n \n We used contact-tracing data, standard line-lists, and published parameters to train a variety of time-series forecasting models, including an ARIMA model, a Poisson regression, and an SEIR compartmental model. We also calculated the daily R-effective rate as an additional output. The compartmental model best fit the reported cases when tested out of sample, but the statistical models were quicker and easier to deploy and helped inform initial decision-making. The R-effective rate was particularly useful throughout the effort. \n \n Overall, these efforts highlighted the importance of rapidly deployable and scalable infectious disease modeling pipelines. Public health data science is still a nascent field, however, so common best practices in other industries are often-times novel approaches in public health. The need for stable, generalizable pipelines is crucial. Using the Databricks platform has allowed us to more quickly scale and iteratively improve our modeling pipelines to include other infectious diseases, such as influenza and RSV. Further development of scalable and standardized approaches to disease forecasting at the state and local level is vital to better informing future public health response efforts."
        ],
        [
         "Accelerating the Development of Viewership Personas With a Unified Feature Store",
         "With the proliferation of video content and flourishing consumer demand, there is an enormous opportunity for customer-centric video entertainment companies to use data & analytics to understand what their viewers want and deliver more of the content that that meets their needs.\n \n At DIRECTV, our Data Science Center of Excellence is constantly looking to push the boundary of innovation in how we can better and more quickly understand the needs of our customers and leverage those actionable insights to deliver business impact. One way in which we do so is through the development of Viewership Personas with cluster analysis at scale to group our customers by the types of content they enjoy watching. This process is significantly accelerated by a unified feature store which contain a wide array of features that captures key information on viewing preferences.\n \n This presentation will focus on how the DIRECTV Data Science team utilizes Databricks to help: (1) develop a unified feature store and (2) how we leverage the feature store to accelerate the process of running machine learning algorithms to find meaningful viewership clusters."
        ],
        [
         "How You Can Audit A Language Model",
         "Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\n \n Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\n \n * Studying past incidents using the AI Incident Database and using this information to guide debugging.\n \n * Finding & fixing common data quality issues.\n \n * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\n \n * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\n \n * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\n \n * Testing performance, sentiment & toxicity across different & less common languages.\n \n * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\n \n * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\n \n * Engaging stakeholders to help find problems system designers & developers cannot see.\n \n It's now time to figure out how to live with AI, and that means audits, risk management & regulation."
        ],
        [
         "JetBlue’s real-time AI & ML digital twin journey using Databricks",
         "JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \n \n The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code."
        ],
        [
         "Intermittent Service Part Demand Forecasting at John Deere",
         "John Deere customers demand reliability and uptime for their equipment and John Deere dealers must fulfill that need by stocking the right mix of service parts in the right quantities to keep them up and running. John Deere dealers operate on six continents with geographically dispersed customers, products, and equipment. This presents a significant operations and logistics challenge to maintain best-in-industry service levels. Improved demand forecasting enables better retail stocking decisions and inventory planning, however, the variety of equipment serviced, diversity of customer segments, and total number of SKUs at dealer retail locations presents a complex demand forecasting challenge.\n \n The John Deere Aftermarket Analytics team took on this opportunity and developed forecasts and stocking recommendations for difficult low volume and intermittent demand service parts that comprise a large percentage of dealers' part sales. Building and iterating the solution required development of data pipelines, exploratory analysis and investigation, model training with multiple R and Python packages, experiment tracking in MLFlow, and model deployment. All of this was enabled with John Deere's Enterprise Data Lake paired with Databricks.\n \n In this session, learn about:\n \n • Practical approaches for intermittent demand forecasting \n • Leveraging Workspaces and Repos for efficient team collaboration\n • Developing scalable data pipelines and features for ML models with Delta Lake\n • Rapid model experimentation with AutoML and MLFlow\n  • Job orchestration using Workflows"
        ],
        [
         "Scaling AI Applications With Databricks, HuggingFace and Pinecone",
         "The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\n In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution. "
        ],
        [
         "Hyperparameter Tuning Via Apache Spark and Ray",
         "Model selection through hyperparameter tuning (HPT) is a computationally intensive task. Development of specialized hardware, distributed computation, and modern model selection algorithms has allowed a wide range of potential solutions to be evaluated by the data science community. In this session, you will learn more about how Marks and Spencer tackled this task. In examining the tools, you see that Spark UDF and Ray have received considerable attention from the data science community. Spark UDF allows parallelized execution of custom workloads on distributed datasets in a fault tolerant and scalable manner. Ray is an open-source framework for distributed model selection. It features a well-designed, narrow waist API that seamlessly integrates a wide range of hyperparameter search algorithms. Ray creates trials and controls their execution with the help of a scheduler equipped with well-known HPT algorithms allowing Bayesian optimization and Population-based training. "
        ],
        [
         "Large Scale Multi-Task Learning Recommender Service at Verizon",
         "Millions of customers visit the Verizon company website and stores monthly to shop, from plans to products and accessories. Numerous task-specific recommender models have been built at Verizon to enhance customer experience. Motivated by the latest development in deep learning, multi-task learning architectures can better understand customer behavior therefore greatly improve model performance, and also save time to learn new tasks. However, it’s challenging to build such a model that digests heterogeneous data from purchases, online clicks, store visits to customer services and many more, while also  accomplishing various tasks where some of them might contradict with each other. Motivated by the Pathways vision, we built a novel multi-task learning recommender service called Pathways Recommender Service (PaRS), which directly handles multiple abstract forms of data and is able to generalize across multiple recommendation tasks at Verizon. You will see the that we explored built-in bias mitigation in PaRS to promote diverse, relevant and fair recommendations."
        ],
        [
         "Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases",
         "Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases."
        ],
        [
         "International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale",
         "International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \nIn this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps."
        ],
        [
         "How MLOps on Databricks helped adidas to gain speed in productionising ML projects",
         "MLOps enables Data science teams to bring ML models into production environments in an efficient manner while making sure that the model pipeline is maintained and model is monitored continuously. When drift is detected, models are retrained promptly and teams are notified with the availability of new version of model. Our team at adidas has created a ML template that uses Databricks, Jenkins and MLflow, which facilitates in the faster deployment process for our ML models in production. We employ a hybrid deployment strategy to minimize compute costs for large models. This presentation will demonstrate how the template was developed, how our teams are utilizing it for model deployment and how it helped us to gain speed to bring or use cases live."
        ],
        [
         "How We Built a Unified Talent Solution Using Databricks Machine Learning",
         "Using Databricks, we built a “Unified Talent Solution”, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta’s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data"
        ],
        [
         "Meet LOLA: The Innovation Engine Brewing Models at Scale for AB-InBev",
         "Today the world's largest brewer, AB-InBev, is disrupting the industry with BEES and LOLA. LOLA is our machine learning platform that enables us to brew models at scale. The foundation of LOLA rests on Databricks Lakehouse's platform and is now being extended globally with Unity Catalog. However, three years ago, things were different. Back then, LOLA was just a simple recommender with less than 60% account coverage in the US. Now it is evolving into an ML platform that supports more models, like our new recommendation engine DEAR, with 95%+ account coverage and integration with BEES in the US.\nAs LOLA evolves, the foundation laid by Lakehouse paved the way for the cornerstone of our platform, our platinum data layer, the Feature Store. The Feature Store is the hub for all feature engineering in LOLA, using Databricks Workflows to support over 250+ production features. But, more importantly, it has become the abstraction layer that helps build out our growing number of ML pipelines.\n  \n "
        ],
        [
         "Scaling MLOps for a demand forecasting across multiple markets for a large CPG",
         "In this session, we look at how one of the world’s largest CPG company setup a scalable MLOps pipeline for a Demand Forecasting use case that predicted demand at 100k+ DFUs (demand forecasting units) on a weekly basis across 20+ markets. This implementation resulted in significant cost savings in terms of improved productivity, reduced cloud usage and faster time to value amongst other benefits.\n The attendees of this session will leave this session with a clearer picture on the following:\n 1. Best practices in scaling mlops with Databricks and Azure for a demand forecasting use case with a multi-market and multi-region roll-out.\n 2. Best practices related to model re-factoring and setting up standard CI-CD pipelines for MLOps\n 3. What are some of the pitfalls to avoid in such scenarios?"
        ],
        [
         "MLOps at Gucci: From Zero to Hero",
         "In recent years, similar principles to those of DevOps in software development have been applied to Machine Learning projects with the goal of productionizing automated solutions. However, Machine Learning Operations (MLOps) practices have proven to be of difficult implementation, as they often require putting together several different tools in a complex architecture. In this session, we introduce MLOps concepts and describe how we implement MLOps principles in our projects at Gucci by leveraging Databricks functionalities. A use case of deploying a data science tool for supporting media budget allocation decisions is presented. After the development of a POC to experiment and effectively address the business problem, the code was versioned and refactored. Code reviews were executed to ensure quality and readability. Within Databricks, we rapidly moved the project to the production stage by achieving an automated solution including unit tests, environment configuration, registration and versioning of models, monitoring of model performances as well as model serving and a dashboard to visualize results. This template is being successfully replicated for other projects and is allowing our new Data Science team to quickly bring value to different business areas of the company. These best practices and insights can be used as an example of how this can be beneficial to you or other practitioners."
        ],
        [
         "Streamlining API Deployment for ML Models Across Multiple Brands: Ahold Delhaize's Experience on Serverless",
         "At Ahold Delhaize, we have 19 local brands. Most of our brands have common goals, such as providing personalized offers to their customers, a better search engine on e-commerce websites, and forecasting models to reduce food waste and ensure availability. As a central team, our goal is to standardize the way of working across all of these brands, including the deployment of machine learning models. To this end, we have adopted Databricks as our standard platform for our batch inference models. \n\nHowever, API deployment for real time inference models remained challenging due to the varying capabilities of our brands. Our attempts to standardize API deployments with different tools failed due to complexity of our organization. Fortunately, Databricks has recently introduced a new feature: serverless API deployment. Since all our brands already use Databricks, this feature was easy to adopt. It allows us to easily reuse API deployment across all of our brands, significantly reducing time to market (from 6-12 months to 1 month), increasing efficiency, and reducing the costs. In this session, you will see the solution architecture, sample use case specifically used to cross-sell model deployed to 4 different brands, and API deployment using Databricks Serverless API with custom model."
        ],
        [
         "Monetizing Data Assets: Sharing Data, Models and Features",
         "Exec Summary:\n \n - Data is an asset and selling/sharing data has (largely) been solved\n - Hosted models exist (example: ChatGPT) but moving sensitive data across the public internet or across clouds is problematic\n - Sharing features (the result of feature engineering) can be monetized for new potential revenue streams\n - Sharing models can also be monetized while avoiding the transfer of sensitive data\n - This presentation will walk through a few examples of how to share models and features to generate new revenue streams using Delta Sharing, MLflow, and Databricks"
        ],
        [
         "An API for DL Inferencing on Spark",
         "Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization.\n \n In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models."
        ],
        [
         "DEIMOS - Optimizing Time-Series Modeling at Scale With Applications in CPG",
         "At the beginning of a forecasting project, data scientists must make several decisions, each of which impacts the success of a project. Should the model be univariate or multivariate? What features warrant the best model inputs? What model type should you choose? Are the best hyperparameters selected? All of these parts of the modeling process can be quite costly in terms of both time and computational resources. In this session, you will see solution we've created called DEIMOS (Deep Experimental Integrated Multivariate Optimized Series). DEIMOS is built to handle many of the critical problems in time-series modeling automatically and at scale. This package is designed to be simple and straightforward to use, with easily interpretable results to both technical and non-technical audiences. Moreover, we’ve designed it to be flexible so that it can be easily applied to many use cases."
        ],
        [
         "Comparing Databricks and Snowflake for Machine Learning",
         "Snowflake and Databricks both aim to provide data science toolkits for machine learning workflows, albeit with different approaches and resources. While developing ML models is technically possible using either platform, the Hitachi Solutions Empower team tested which solution will be easier, faster, and cheaper to work with in terms of both user experience and business outcomes for our customers. To do this, we designed and conducted a series of experiments with use cases from the TPCx-AI benchmark standard. We developed both single-node and multi-node versions of these experiments, which sometimes required us to set up separate compute infrastructure outside of the platform, in the case of Snowflake. We also built datasets of various sizes (1GB, 10GB, and 100GB), to assess how each platform/node setup handles scale. Based on our findings. On the average, Databricks is faster, cheaper, and easier to use for developing machine learning models, and we use it exclusively for data science on the Empower platform. Snowflake’s reliance on third party resources for distributed training is a major drawback, and the need to use multiple compute environments to scale up training is complex and, in our view, an unnecessary complication to achieve best results."
        ],
        [
         "Testing Generative AI Models - What You Need to Know",
         "Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\n \n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\n \n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk."
        ],
        [
         "Simplifying Real-Time Machine Learning: A Look at Feature Platforms and Modern Real-time ML Architectures Using MLflow & Tecton",
         "Are you struggling to keep up with the demands of real-time machine learning? Like most organizations building real-time ML, you’re probably looking for a better way to: Manage the lifecycle of ML models and features, Implement batch, streaming, and real-time data pipelines, Generate accurate training datasets and Serve models and data online with strict SLAs, supporting millisecond latencies and high query volumes. Look no further! In this session, we will unveil a modern technical architecture that simplifies the process of managing real-time ML models and features. Using MLFlow and Tecton, we’ll show you how to build a robust MLOps platform on Databricks that can easily handle the unique challenges of real-time data processing. Join us to discover how to streamline the lifecycle of ML models and features, implement data pipelines with ease, and generate accurate training datasets with minimal effort. See how to serve models and data online with mission-critical speed and reliability, supporting millisecond latencies and high query volumes. Take a firsthand look at how FanDuel uses this solution to power their real-time ML applications, from responsible gaming to content recommendations and marketing optimization. See for yourself how this system can be used to define features, train models, process streaming data, and serve both models and features online for real-time inference with a live demo. Join us to learn how to build a modern MLOps platform for your real-time ML use cases."
        ],
        [
         "Building a Real-Time Model Monitoring Pipeline on Databricks",
         "Model deployment is almost never the final step in any ML lifecycle. ML models can degrade over time due to a variety of influencing factors. In this technical deep dive, we will build a real-time ML model monitoring pipeline on Databricks. We need to own and monitor our models for drifts like feature drift, concept drift, distribution drift, and so on. We must constantly monitor the models and issue alerts or trigger retraining when necessary. With so many open source tools and frameworks available, it can be difficult to figure out how to make everything work. In this tutorial, we will create a high-quality Model Monitoring Pipeline. Everything will be built from the ground up using Spark on Databricks. In this session we will introduce a use case in which we set up a model serving pipeline and log the predictions to a stream in real time. We will then configure a model metric monitoring pipeline to consume from the stream and aggregate over specific time windows. Then, to see these metrics live on dashboards, we will integrate a model monitoring visualizing pipeline."
        ],
        [
         "Using NLP to evaluate 100 Million global webpages daily to contextually target consumers",
         "This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\n\nTTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\n\nThe Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\n"
        ],
        [
         "How Office Leverages Deep Graph Learning to Improve Productivity Products",
         "We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale."
        ],
        [
         "Stable Diffusion: The Future of Generative AI",
         "Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering."
        ],
        [
         "Vector Data Lakes",
         "Vector databases such as ElasticSearch and Pinecone offer fast ingestion and querying on vector embeddings with ANNs. However, they typically do not decouple compute and storage, making them hard to integrate in production data stacks. Because data storage in these databases is expensive and not easily accessible, data teams typically maintain ETL pipelines to offload historical embedding data to blob stores. When that data needs to be queried, they get loaded back into the vector database in another ETL process. This is reminiscent of loading data from OLTP database to cloud storage, then loading said data into an OLAP warehouse for offline analytics. Recently, “lakehouse” offerings allow direct OLAP querying on cloud storage, removing the need for the second ETL step. The same could be done for embedding data. While embedding storage in blob stores cannot satisfy the high TPS requirements in online settings, we argue it’s sufficient for offline analytics use cases like slicing and dicing data based on embedding clusters. Instead of loading the embedding data back into the vector database for offline analytics, we propose direct processing on embeddings stored in Parquet files in Delta Lake. You will see that offline embedding workloads typically touch a large portion of the stored embeddings without the need for random access. As a result, the workload is entirely bound by network throughput instead of latency, making it quite suitable for blob storage backends. On a test 1 billion vector dataset, ETL into cloud storage takes around 1 hour on a dedicated GPU instance, while batched nearest neighbor search can be done in under one minute with four CPU instances. We believe future “lakehouses” will ship with native support for these embedding workloads."
        ],
        [
         "JoinBoost: In-DB ML for Tree-Models",
         "Data and Machine Learning (ML) are crucial for enterprise operations. Enterprises store data in databases for management and use ML to gain business insights. However, there is a mismatch between the way ML expects data to be organized (a single table) and the way data is organized in databases (a join graph of multiple tables) and leads to inefficiencies when joining and materializing tables. In this session, you will see how we successfully address this issue. We introduce JoinBoost, a lightweight python library that trains tree models (such as random forests and gradient boosting) for join graphs in databases. JoinBoost acts as a query rewriting layer that is compatible with cloud databases, and eliminates the need for costly join materialization."
        ],
        [
         "Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP",
         "In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively."
        ],
        [
         "Data Caching Strategies for Data Analytics and AI",
         "The increasing popularity of data analytics and artificial intelligence (AI) has led to a dramatic increase in the volume of data being used in these fields, creating a growing need for an enhanced computational capability. Cache plays a crucial role as an accelerator for data and AI computations, but it is important to note that these domains have different data access patterns, requiring different cache strategies. In this session, you will see our observations on data access patterns in the analytical SQL and AI training domains based on practical experience with large-scale systems. We will discuss the evaluation results of various caching strategies for analytical SQL and AI and provide caching recommendations for different use cases. Over the years, we have learned some best practices from big internet companies about the following aspects of our journey: 1. Traffic pattern for analytical SQL and cache strategy recommendation, 2. Traffic pattern for AI training and how we can measure the cache efficiency for different AI training process, 3. Cache capacity planning based on real-time metrics of the working set, and 4. Adaptive caching admission and eviction for uncertain traffic patterns"
        ],
        [
         "If a Duck Quacks in the Forrest and Everyone Hears, Should you Care?",
         "\"YES! \"\"Duck posting\"\" has become an internet meme for praising DuckDB on twitter. Nearly every quack using DuckDB has done it once or twice. But"
        ],
        [
         "Python with Spark Connect",
         "PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python.\n \n With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today’s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example.\n \n In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications.\n \n In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what’s next beyond Apache Spark 3.4."
        ],
        [
         "Databricks Connect powered by Spark Connect: Develop and Debug Spark from any developer tool",
         "Spark developers want to develop and debug their code using their tools of choice and development best practices while ensuring high-production fidelity on the target remote cluster. However, Spark's driver architecture is monolithic, with no built-in capability to directly connect to a remote Spark cluster from languages other than SQL. This makes it hard to enable such interactive developer experiences from a user’s local IDE of choice. Spark Connect’s decoupled client-server architecture introduces remote connectivity to Spark clusters and with that, enables interactive development experience - Spark and its open ecosystem can be leveraged from everywhere! \n \n In this talk, we show how we leverage Spark Connect to build a completely redesigned version of Databricks Connect, a first-class IDE-based developer experience that offers interactive debugging from any IDE. We show how developers can easily ensure consistency between their local and remote environments. We walk the audience through real-live examples of how to locally debug code running on Databrick. We also show how Databricks Connect integrates into the Databricks Visual Studio Code extension for an even better developer experience."
        ],
        [
         "Ray on Spark",
         "Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale.\n \n This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. \n \n In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. \n We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. \n During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change!\n We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases."
        ],
        [
         "Why Delta Lake is the best storage format for pandas analyses",
         "pandas analyses are often limited by file formats like CSV and Parquet. CSV doesn't allow for column pruning, which is an important performance optimization. Parquet doesn't allow for critical features like ACID transactions, time travel, and schema enforcement. This talk discusses why Delta Lake is the fastest file format for pandas users and how it provides users with great features."
        ],
        [
         "Fine tuning & scaling Hugging Face with Ray AIR",
         "Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n \n The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\n \n In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models’ training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n \n Key Takeaways: \n * Python developers and machine learning engineers can use Transformers and scale their language models\n * Get exposed to Ray AIR’s Python APIs for end-to-end Hugging Face and ML workflow\n * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads"
        ],
        [
         "Breaking Barriers with Databricks Lakehouse - How Blackberry is Revolutionizing Cybersecurity Services Worldwide. AI-Driven Cybersecurity that Works Smarter, Not Harder.\n",
         "Cybersecurity incidents are costly, and using an Endpoint Detection and Response (EDR) solution enables the detection of cybersecurity incidents as quickly as possible. To effectively detect cybersecurity incidences requires the collection of millions of data points, and the storing/querying of endpoints data presents considerable engineering challenges. This includes quickly moving local data from endpoints to a single table in the cloud and enabling performant querying against it. The need to avoid internal data siloing within BlackBerry was paramount as multiple teams required access to the data to deliver an effective EDR solution for the present and the future. Databricks tooling enabled us to break down our data silos and iteratively improve our EDR pipeline to ingest data faster and reduce querying latency by more than 20% while reducing costs by more than 30%.\nIn this session, BlackBerry Engineers Srinivasa Kanamatha and Justin Lai will share the journey, lessons learned, and the future for collecting, storing, governing, and sharing data from endpoints in Databricks. The result of building EDR using Databricks helped us accelerate the deployment of our data platform.\"\""
        ],
        [
         "Scaling Deep Learning using Delta Lake storage format on Databricks",
         "Delta Lake is an open-source storage format that can be ideally used for storing large-scale datasets, which can be used for single-node and distributed training of deep learning models. Delta Lake storage format gives deep learning practitioners unique data management capabilities for working with their datasets. \n The challenge is that, as of now, it’s not possible to use Delta Lake to train PyTorch models directly. PyTorch community has recently introduced a Torchdata library for efficient data loading. This library supports many formats out of the box, but not Delta Lake. This talk will demonstrate using the Delta Lake storage format for single-node and distributed PyTorch training using the torchdata framework and standalone delta-rs Delta Lake implementation."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Abstract",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dais_pdf = pd.read_parquet(f\"{DA.paths.datasets}/dais/dais23_talks.parquet\")\n",
    "display(dais_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe082d5-2844-4cef-aeb1-805058ae2229",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse\n                Abstract:  Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\n \n Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \n \n These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\n - Modular and composable code\n - Unit testing framework\n - Incremental event processing with spark structured streaming \n - Granular resource tuning for better performance and cost efficacy\n \n Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse.\n"
     ]
    }
   ],
   "source": [
    "dais_pdf[\"full_text\"] = dais_pdf.apply(\n",
    "    lambda row: f\"\"\"Title: {row[\"Title\"]}\n",
    "                Abstract:  {row[\"Abstract\"]}\"\"\".strip(),\n",
    "    axis=1,\n",
    ")\n",
    "print(dais_pdf.iloc[0][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd485035-b163-4bdf-875e-9797e7fb8345",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System\n                Abstract:  Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community.\n"
     ]
    }
   ],
   "source": [
    "print(dais_pdf.iloc[1][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29dffe86-d192-44c6-8fab-59ce4e969483",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "texts = dais_pdf[\"full_text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c86f7d8a-8b42-43b9-8dc6-5276557b3db6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Title: Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse\\n                Abstract:  Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\\n \\n Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \\n \\n These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\\n - Modular and composable code\\n - Unit testing framework\\n - Incremental event processing with spark structured streaming \\n - Granular resource tuning for better performance and cost efficacy\\n \\n Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse.',\n",
       " 'Title: Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System\\n                Abstract:  Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community.',\n",
       " \"Title: From Snowflake to Enterprise-Scale Apache Spark™\\n                Abstract:  Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers.\\n \\n The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™  solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs.\\n \\n In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to:\\n \\n * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation\\n * Handle X100 queries per second on a single Spark application with sub-second query latency\\n * Protect Spark application from misbehaving users \\n * Optimize SQL-based queries\",\n",
       " \"Title: The Future of Data Orchestration: Asset-Based Orchestration\\n                Abstract:  Data orchestration is a core component for any batch data processing platform and we’ve been using patterns that haven't changed since the 1980s. \\n \\n In this session, I’ll be introducing a new pattern and way of thinking for data orchestration known as asset-based orchestration, with data freshness sensors to trigger pipelines. I will demo this new pattern using popular tools of the modern data stack - dbt, airbyte, dagster, and databricks.\",\n",
       " \"Title: Photon for Dummies: How Does this New Execution Engine Actually Work?\\n                Abstract:  Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it.\\n \\n If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. \\n \\n Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as:\\n - Why is a Java Virtual Machine the current bottleneck for spark enhancements?\\n - What does vectorized even mean? And how was it done before?\\n - Why is the relationship status between Spark and Photon 'It’s complicated'?\\n \\n In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. \\n \\n I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to.\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "068cf2e4-8ac3-4d77-aab6-6d9604de0467",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Title</th><th>Abstract</th><th>full_text</th></tr></thead><tbody><tr><td>Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse</td><td>Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\n",
       " \n",
       " Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \n",
       " \n",
       " These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\n",
       " - Modular and composable code\n",
       " - Unit testing framework\n",
       " - Incremental event processing with spark structured streaming \n",
       " - Granular resource tuning for better performance and cost efficacy\n",
       " \n",
       " Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse.</td><td>Title: Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse\n",
       "                Abstract:  Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\n",
       " \n",
       " Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \n",
       " \n",
       " These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\n",
       " - Modular and composable code\n",
       " - Unit testing framework\n",
       " - Incremental event processing with spark structured streaming \n",
       " - Granular resource tuning for better performance and cost efficacy\n",
       " \n",
       " Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse.</td></tr><tr><td>Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System</td><td>Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community.</td><td>Title: Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System\n",
       "                Abstract:  Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community.</td></tr><tr><td>From Snowflake to Enterprise-Scale Apache Spark™</td><td>Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers.\n",
       " \n",
       " The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™  solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs.\n",
       " \n",
       " In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to:\n",
       " \n",
       " * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation\n",
       " * Handle X100 queries per second on a single Spark application with sub-second query latency\n",
       " * Protect Spark application from misbehaving users \n",
       " * Optimize SQL-based queries</td><td>Title: From Snowflake to Enterprise-Scale Apache Spark™\n",
       "                Abstract:  Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers.\n",
       " \n",
       " The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™  solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs.\n",
       " \n",
       " In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to:\n",
       " \n",
       " * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation\n",
       " * Handle X100 queries per second on a single Spark application with sub-second query latency\n",
       " * Protect Spark application from misbehaving users \n",
       " * Optimize SQL-based queries</td></tr><tr><td>The Future of Data Orchestration: Asset-Based Orchestration</td><td>Data orchestration is a core component for any batch data processing platform and we’ve been using patterns that haven't changed since the 1980s. \n",
       " \n",
       " In this session, I’ll be introducing a new pattern and way of thinking for data orchestration known as asset-based orchestration, with data freshness sensors to trigger pipelines. I will demo this new pattern using popular tools of the modern data stack - dbt, airbyte, dagster, and databricks.</td><td>Title: The Future of Data Orchestration: Asset-Based Orchestration\n",
       "                Abstract:  Data orchestration is a core component for any batch data processing platform and we’ve been using patterns that haven't changed since the 1980s. \n",
       " \n",
       " In this session, I’ll be introducing a new pattern and way of thinking for data orchestration known as asset-based orchestration, with data freshness sensors to trigger pipelines. I will demo this new pattern using popular tools of the modern data stack - dbt, airbyte, dagster, and databricks.</td></tr><tr><td>Photon for Dummies: How Does this New Execution Engine Actually Work?</td><td>Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it.\n",
       " \n",
       " If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. \n",
       " \n",
       " Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as:\n",
       " - Why is a Java Virtual Machine the current bottleneck for spark enhancements?\n",
       " - What does vectorized even mean? And how was it done before?\n",
       " - Why is the relationship status between Spark and Photon 'It’s complicated'?\n",
       " \n",
       " In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. \n",
       " \n",
       " I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to. \n",
       " \n",
       " </td><td>Title: Photon for Dummies: How Does this New Execution Engine Actually Work?\n",
       "                Abstract:  Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it.\n",
       " \n",
       " If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. \n",
       " \n",
       " Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as:\n",
       " - Why is a Java Virtual Machine the current bottleneck for spark enhancements?\n",
       " - What does vectorized even mean? And how was it done before?\n",
       " - Why is the relationship status between Spark and Photon 'It’s complicated'?\n",
       " \n",
       " In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. \n",
       " \n",
       " I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to.</td></tr><tr><td>Monitoring Delta Live Tables</td><td>In this session we will share how Volvo Group monitors Databricks jobs and DLTs to stay ahead of the curve. Volvo Group uses Databricks for - amongst others - material planning. Based on live input from warehouse systems, the system predicts which orders to place with the suppliers to ensure the availability of spare parts and keep the trucks running. Since this is a critical component, with reverse ETL feeding the recommendation back into the warehouse systems, strict monitoring to avoid pipeline congestion is required. In this talk I will:\n",
       " * Explain a vision of data quality (deliver trustworthy data in time); which metrics to set up to monitor data quality\n",
       " * How it helps Volvo Group; make the SLA, jointly use with FinOps to improve ingestion pipelines, avoid congestion\n",
       " * Setup; use Jobs API and Databricks SQL workspace to build and consult the monitoring dashboard\n",
       " * Demo\n",
       " * Get notebook from GitHub</td><td>Title: Monitoring Delta Live Tables\n",
       "                Abstract:  In this session we will share how Volvo Group monitors Databricks jobs and DLTs to stay ahead of the curve. Volvo Group uses Databricks for - amongst others - material planning. Based on live input from warehouse systems, the system predicts which orders to place with the suppliers to ensure the availability of spare parts and keep the trucks running. Since this is a critical component, with reverse ETL feeding the recommendation back into the warehouse systems, strict monitoring to avoid pipeline congestion is required. In this talk I will:\n",
       " * Explain a vision of data quality (deliver trustworthy data in time); which metrics to set up to monitor data quality\n",
       " * How it helps Volvo Group; make the SLA, jointly use with FinOps to improve ingestion pipelines, avoid congestion\n",
       " * Setup; use Jobs API and Databricks SQL workspace to build and consult the monitoring dashboard\n",
       " * Demo\n",
       " * Get notebook from GitHub</td></tr><tr><td>Data Quality: Fast and Slow</td><td>Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \n",
       " \n",
       " In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\n",
       " \n",
       " This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical.</td><td>Title: Data Quality: Fast and Slow\n",
       "                Abstract:  Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \n",
       " \n",
       " In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\n",
       " \n",
       " This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical.</td></tr><tr><td>Taking Your Cloud Vendor to the Next Level: Solving Complex Challenges with Azure Databricks</td><td>Akamai's content delivery network (CDN) processes about 30% of the internet's daily traffic, resulting in a massive amount of data that presents engineering challenges, both internally and with cloud vendors. In this session, we will discuss the barriers faced while building a data infrastructure on Azure, Databricks, and Kafka to meet strict SLAs, hitting the limits of some of our cloud vendors’ services. We will describe the iterative process of re-architecting a massive-scale data platform using the aforementioned technologies. We will also delve into how today, Akamai is able to quickly ingest and make available to customers TBs of data, as well as efficiently query PBs of data and return results within 10 seconds for most queries. This discussion will provide valuable insights for attendees and organizations seeking to effectively process and analyze large amounts of data.</td><td>Title: Taking Your Cloud Vendor to the Next Level: Solving Complex Challenges with Azure Databricks\n",
       "                Abstract:  Akamai's content delivery network (CDN) processes about 30% of the internet's daily traffic, resulting in a massive amount of data that presents engineering challenges, both internally and with cloud vendors. In this session, we will discuss the barriers faced while building a data infrastructure on Azure, Databricks, and Kafka to meet strict SLAs, hitting the limits of some of our cloud vendors’ services. We will describe the iterative process of re-architecting a massive-scale data platform using the aforementioned technologies. We will also delve into how today, Akamai is able to quickly ingest and make available to customers TBs of data, as well as efficiently query PBs of data and return results within 10 seconds for most queries. This discussion will provide valuable insights for attendees and organizations seeking to effectively process and analyze large amounts of data.</td></tr><tr><td>Unleashing the Power of Interactive Analytics at Scale with Databricks and Delta Lake: Lessons Learned from Building Akamai's Web Security Analytics Product</td><td>Akamai is a leading content delivery network (CDN) and cybersecurity company, operating hundreds of thousands of servers in more than 135 countries worldwide.\n",
       " In this talk, we will share our experiences and lessons learned from building and maintaining the Web Security Analytics (WSA) product, an interactive analytics platform powered by Databricks and Delta Lake, that enables customers to efficiently analyze and take informed action on a high volume of streaming security events. \n",
       " \n",
       " The WSA platform must be able to serve hundreds of queries per minute, scanning hundreds of terabytes of data from a six petabyte data lake, with most queries returning results within ten seconds (for both aggregation queries and needle in a haystack queries).\n",
       " The talk will cover how to use Databricks SQL warehouses and job clusters cost-effectively, and how to improve query performance through the use of tools and techniques such as Delta Lake, Databricks Photon, and partitioning. \n",
       " This talk will be valuable for anyone looking to build and operate a high-performance analytics platform.</td><td>Title: Unleashing the Power of Interactive Analytics at Scale with Databricks and Delta Lake: Lessons Learned from Building Akamai's Web Security Analytics Product\n",
       "                Abstract:  Akamai is a leading content delivery network (CDN) and cybersecurity company, operating hundreds of thousands of servers in more than 135 countries worldwide.\n",
       " In this talk, we will share our experiences and lessons learned from building and maintaining the Web Security Analytics (WSA) product, an interactive analytics platform powered by Databricks and Delta Lake, that enables customers to efficiently analyze and take informed action on a high volume of streaming security events. \n",
       " \n",
       " The WSA platform must be able to serve hundreds of queries per minute, scanning hundreds of terabytes of data from a six petabyte data lake, with most queries returning results within ten seconds (for both aggregation queries and needle in a haystack queries).\n",
       " The talk will cover how to use Databricks SQL warehouses and job clusters cost-effectively, and how to improve query performance through the use of tools and techniques such as Delta Lake, Databricks Photon, and partitioning. \n",
       " This talk will be valuable for anyone looking to build and operate a high-performance analytics platform.</td></tr><tr><td>ABN Story: Migrating to Future Proof Data Platform</td><td>ABN AMRO Bank is one of the top leading banks in the Netherlands; it is the 3rd largest bank in the Netherlands by revenue and number when it comes to mortgages within the Netherlands. We have an objective to become a fully data-driven bank and this goal is well supported by our top management.\n",
       " \n",
       " ABN AMRO started its data journey almost seven years ago and has built a data platform off-premises with Hadoop technologies. This data platform has been used by more than 200 data providers, 150 data consumers, and overall more than 3000 Datasets.\n",
       " \n",
       " To become a fully digital bank and address the limitation of the on-premises platform, we needed a future-proof data platform DIAL (digital integration and access layer.) ABN AMRO decided to build an Azure cloud-native data platform with the help of Microsoft and Databricks. Last year this cloud-native platform was ready for our data providers and data consumers. Six months ago we started the journey of migrating all the content from the on-premises data platform to the Azure data platform, this was a very large-scale migration and was achieved in 6 months.\n",
       " \n",
       " In this session, we will focus on two things :\n",
       " 1. Share our strategy for migration from on-premises to a cloud-native platform. \n",
       " 2. Share how we used Databricks products in our data platform and how the Databricks team helped us in the overall migration.\n",
       " \n",
       " </td><td>Title: ABN Story: Migrating to Future Proof Data Platform\n",
       "                Abstract:  ABN AMRO Bank is one of the top leading banks in the Netherlands; it is the 3rd largest bank in the Netherlands by revenue and number when it comes to mortgages within the Netherlands. We have an objective to become a fully data-driven bank and this goal is well supported by our top management.\n",
       " \n",
       " ABN AMRO started its data journey almost seven years ago and has built a data platform off-premises with Hadoop technologies. This data platform has been used by more than 200 data providers, 150 data consumers, and overall more than 3000 Datasets.\n",
       " \n",
       " To become a fully digital bank and address the limitation of the on-premises platform, we needed a future-proof data platform DIAL (digital integration and access layer.) ABN AMRO decided to build an Azure cloud-native data platform with the help of Microsoft and Databricks. Last year this cloud-native platform was ready for our data providers and data consumers. Six months ago we started the journey of migrating all the content from the on-premises data platform to the Azure data platform, this was a very large-scale migration and was achieved in 6 months.\n",
       " \n",
       " In this session, we will focus on two things :\n",
       " 1. Share our strategy for migration from on-premises to a cloud-native platform. \n",
       " 2. Share how we used Databricks products in our data platform and how the Databricks team helped us in the overall migration.</td></tr><tr><td>Simon + Denny Live: Ask Us Anything</td><td>Simon and Denny have been discussing and debating all things Delta, Lakehouse and Apache Spark™ on their regular webshow. Whether you want advice on lake structures, want to hear their opinions on the latest trends and hype in the data world, or you simply have a tech implementation question to throw at two seasoned experts, these two will have something to say on the matter. In their previous shows, Simon and Denny focused on building out a sample lakehouse architecture, refactoring and tinkering as new features came out, but now we're throwing the doors open for any and every question you might have.\n",
       " \n",
       " So if you've had a niggling question and these two can help, this is the session for you. There will be a question submission form shared prior to the event, so the team will be prepped with a whole bunch of topics to talk through. Simon and Denny want to hear YOUR questions, which they can field from a wealth of industry experience, wide ranging community engagement and their differing perspectives as external consultant and internal Databricks respectively. There's also a chance they'll get distracted and go way off track talking about coffee, sci-fi, nerdery or the English weather. It happens.</td><td>Title: Simon + Denny Live: Ask Us Anything\n",
       "                Abstract:  Simon and Denny have been discussing and debating all things Delta, Lakehouse and Apache Spark™ on their regular webshow. Whether you want advice on lake structures, want to hear their opinions on the latest trends and hype in the data world, or you simply have a tech implementation question to throw at two seasoned experts, these two will have something to say on the matter. In their previous shows, Simon and Denny focused on building out a sample lakehouse architecture, refactoring and tinkering as new features came out, but now we're throwing the doors open for any and every question you might have.\n",
       " \n",
       " So if you've had a niggling question and these two can help, this is the session for you. There will be a question submission form shared prior to the event, so the team will be prepped with a whole bunch of topics to talk through. Simon and Denny want to hear YOUR questions, which they can field from a wealth of industry experience, wide ranging community engagement and their differing perspectives as external consultant and internal Databricks respectively. There's also a chance they'll get distracted and go way off track talking about coffee, sci-fi, nerdery or the English weather. It happens.</td></tr><tr><td>Sometimes Less is More: A Deep Dive into the Evolution of a Large, Complex, Real-Time Data Pipeline</td><td>This session will dive into technical detail around one of my complex cybersecurity endpoint detection and response (EDR) data pipelines - which spans multiple industries - that is being joined with an indicator of compromise (IoC) feed. I'll detail the stages we went through in the initial evolution of the pipeline and why it ultimately made more sense to apply less schema structure to it and focus on using a non-obvious partitioning strategy when working with this data source. All code is in Python/Pyspark and uses Delta Live Tables.</td><td>Title: Sometimes Less is More: A Deep Dive into the Evolution of a Large, Complex, Real-Time Data Pipeline\n",
       "                Abstract:  This session will dive into technical detail around one of my complex cybersecurity endpoint detection and response (EDR) data pipelines - which spans multiple industries - that is being joined with an indicator of compromise (IoC) feed. I'll detail the stages we went through in the initial evolution of the pipeline and why it ultimately made more sense to apply less schema structure to it and focus on using a non-obvious partitioning strategy when working with this data source. All code is in Python/Pyspark and uses Delta Live Tables.</td></tr><tr><td>Use Apache Spark™  from Anywhere: Remote connectivity with Spark Connect</td><td>Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark™. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\n",
       " \n",
       " However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\n",
       " \n",
       " Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages.\n",
       " \n",
       " This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere.</td><td>Title: Use Apache Spark™  from Anywhere: Remote connectivity with Spark Connect\n",
       "                Abstract:  Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark™. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\n",
       " \n",
       " However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\n",
       " \n",
       " Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages.\n",
       " \n",
       " This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere.</td></tr><tr><td>Seven Things You Didn't Know You Can Do with Databricks Workflows</td><td>Databricks workflows has come a long way since the initial days of orchestrating simple notebooks and jar/wheel files. Now we can orchestrate multi-task jobs and create a chain of tasks with lineage and DAG with either fan-in or fan-out among multiple other patterns or even run another Databricks job directly inside another job. Databricks workflows takes its tag: “orchestrate anything anywhere” pretty seriously and is a truly fully-managed, cloud-native orchestrator to orchestrate diverse workloads like Delta Live Tables, SQL, Notebooks, Jars, Python Wheels, dbt, SQL, Apache Spark™, ML pipelines with excellent monitoring, alerting and observability capabilities as well. Basically it is a one-stop product for all orchestration needs for an efficient Lakehouse. And what is even better is, it gives full flexibility of running your jobs in a cloud-agnostic and cloud-independent way and is available across AWS, Azure and GCP. In this talk we will discuss and deep dive on some of the very interesting features and will showcase end-to-end demos of the features which will allow you to take full advantage of Databricks workflows for orchestrating the Lakehouse.</td><td>Title: Seven Things You Didn't Know You Can Do with Databricks Workflows\n",
       "                Abstract:  Databricks workflows has come a long way since the initial days of orchestrating simple notebooks and jar/wheel files. Now we can orchestrate multi-task jobs and create a chain of tasks with lineage and DAG with either fan-in or fan-out among multiple other patterns or even run another Databricks job directly inside another job. Databricks workflows takes its tag: “orchestrate anything anywhere” pretty seriously and is a truly fully-managed, cloud-native orchestrator to orchestrate diverse workloads like Delta Live Tables, SQL, Notebooks, Jars, Python Wheels, dbt, SQL, Apache Spark™, ML pipelines with excellent monitoring, alerting and observability capabilities as well. Basically it is a one-stop product for all orchestration needs for an efficient Lakehouse. And what is even better is, it gives full flexibility of running your jobs in a cloud-agnostic and cloud-independent way and is available across AWS, Azure and GCP. In this talk we will discuss and deep dive on some of the very interesting features and will showcase end-to-end demos of the features which will allow you to take full advantage of Databricks workflows for orchestrating the Lakehouse.</td></tr><tr><td>Using DMS and DLT for Change Data Capture </td><td>[Abstract TBD]</td><td>Title: Using DMS and DLT for Change Data Capture \n",
       "                Abstract:  [Abstract TBD]</td></tr><tr><td>Deep Dive into the New Features of Apache Spark™  3.4</td><td>In 2022, Apache Spark™ was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. \n",
       " \n",
       " We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling.</td><td>Title: Deep Dive into the New Features of Apache Spark™  3.4\n",
       "                Abstract:  In 2022, Apache Spark™ was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. \n",
       " \n",
       " We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling.</td></tr><tr><td>Delta Live Tables: A Modern Approach to Data Pipelines</td><td>Data engineers have the difficult task of cleansing complex, diverse data, and transforming it into a usable source to drive data analytics, data science, and machine learning. They need to know the data infrastructure platform in depth, build complex queries in various languages and stitch them together for production. Join this session to learn how Delta Live Tables (DLT) simplifies the complexity of data transformation and ETL. DLT is the first ETL framework to use modern software engineering practices to deliver reliable and trusted data pipelines at any scale. Discover how analysts and data engineers can innovate rapidly with simple pipeline development and maintenance, how to remove operational complexity by automating administrative tasks and gaining visibility into pipeline operations, how built-in quality controls and monitoring ensure accurate BI, data science, and ML, and how simplified batch and streaming can be implemented with self-optimizing and auto-scaling data pipelines.</td><td>Title: Delta Live Tables: A Modern Approach to Data Pipelines\n",
       "                Abstract:  Data engineers have the difficult task of cleansing complex, diverse data, and transforming it into a usable source to drive data analytics, data science, and machine learning. They need to know the data infrastructure platform in depth, build complex queries in various languages and stitch them together for production. Join this session to learn how Delta Live Tables (DLT) simplifies the complexity of data transformation and ETL. DLT is the first ETL framework to use modern software engineering practices to deliver reliable and trusted data pipelines at any scale. Discover how analysts and data engineers can innovate rapidly with simple pipeline development and maintenance, how to remove operational complexity by automating administrative tasks and gaining visibility into pipeline operations, how built-in quality controls and monitoring ensure accurate BI, data science, and ML, and how simplified batch and streaming can be implemented with self-optimizing and auto-scaling data pipelines.</td></tr><tr><td>Journey Towards Uniting Metastores</td><td>This session will provide a brief overview about Nationwide’s journey towards implementing Unity Catalog at an Enterprise Level. We will cover the following topics.\n",
       " - Identity management structure\n",
       " - Compute framework\n",
       " - Naming standards and usage best practices\n",
       " - And a little bit about how Delta Sharing will help us ingest 3rd party data\n",
       " Unity Catalog has been a core feature towards strengthening Data Lakehouse architecture for multiple business units.</td><td>Title: Journey Towards Uniting Metastores\n",
       "                Abstract:  This session will provide a brief overview about Nationwide’s journey towards implementing Unity Catalog at an Enterprise Level. We will cover the following topics.\n",
       " - Identity management structure\n",
       " - Compute framework\n",
       " - Naming standards and usage best practices\n",
       " - And a little bit about how Delta Sharing will help us ingest 3rd party data\n",
       " Unity Catalog has been a core feature towards strengthening Data Lakehouse architecture for multiple business units.</td></tr><tr><td>How CIBC Integrated Purview and Unity Catalog for Our Data Governance Within a Data Mesh Architecture Leveraging the Lakehouse</td><td>This session will focus on our journey to integrate Purview and Unity Catalog for our data governance within data mesh architecture, while leveraging the lakehouse. Our approach is to build four foundational pillars of data mesh that will uphold data management practices based on consumer-driven and domain-centric data sources. \n",
       " Principle 1: decentralized ownership\n",
       " Principle 2: federated governance\n",
       " Principle 3: data as a product\n",
       " Principle 4: self-service infrastructure\n",
       " Native access controls within the self-service infrastructure are enhanced by pushing policy defined access to unity to enable fine-grain control across the data landscape.</td><td>Title: How CIBC Integrated Purview and Unity Catalog for Our Data Governance Within a Data Mesh Architecture Leveraging the Lakehouse\n",
       "                Abstract:  This session will focus on our journey to integrate Purview and Unity Catalog for our data governance within data mesh architecture, while leveraging the lakehouse. Our approach is to build four foundational pillars of data mesh that will uphold data management practices based on consumer-driven and domain-centric data sources. \n",
       " Principle 1: decentralized ownership\n",
       " Principle 2: federated governance\n",
       " Principle 3: data as a product\n",
       " Principle 4: self-service infrastructure\n",
       " Native access controls within the self-service infrastructure are enhanced by pushing policy defined access to unity to enable fine-grain control across the data landscape.</td></tr><tr><td>The Future of Data Access Control: Booz Allen Hamilton’s Approach to Securing our Databricks Lakehouse with Immuta</td><td>In this session, I’ll review how we utilize Attribute-Based Access Control (ABAC) to enforce policy via Immuta. I’ll discuss the differences between the ABAC and legacy Role-Based Access Control (RBAC) approaches to control access and how the RBAC approach is not sufficient to keep up with today’s growing big data market. With so much data available, there also comes substantial risk. Data can contain many sensitive data elements, including PII and PHI. Industry leaders like Databricks are pushing the boundaries of data technology, which leads to constantly evolving data use cases. And that’s a good thing! However, the RBAC approach is struggling to keep up with those advancements.\n",
       " So what is RBAC? It’s an approach to data access that permits system access based on the end-user’s role. For legacy systems, it’s meant as a simple but effective approach to securing data. Are you a manager? Then you’ll get access to data meant for managers. This is great for small deployments with clearly defined roles. Here at Booz Allen, we invested in Databricks because we have an environment of over 30k users and billions of rows of data.\n",
       " To mitigate this problem and align with our forward-thinking company standard, we introduced Immuta into our stack. Immuta uses ABAC to allow for dynamic data access control. Users are automatically assigned certain attributes, and access is based on those attributes instead of just their role. This allows for more flexibility and allows data access control to easily scale without the need to constantly map a user to their role. Using attributes, we can write policies in one place and have them applied across all of our data platforms. This makes for a truly holistic data governance approach and provides immediate ROI and time savings for the company.</td><td>Title: The Future of Data Access Control: Booz Allen Hamilton’s Approach to Securing our Databricks Lakehouse with Immuta\n",
       "                Abstract:  In this session, I’ll review how we utilize Attribute-Based Access Control (ABAC) to enforce policy via Immuta. I’ll discuss the differences between the ABAC and legacy Role-Based Access Control (RBAC) approaches to control access and how the RBAC approach is not sufficient to keep up with today’s growing big data market. With so much data available, there also comes substantial risk. Data can contain many sensitive data elements, including PII and PHI. Industry leaders like Databricks are pushing the boundaries of data technology, which leads to constantly evolving data use cases. And that’s a good thing! However, the RBAC approach is struggling to keep up with those advancements.\n",
       " So what is RBAC? It’s an approach to data access that permits system access based on the end-user’s role. For legacy systems, it’s meant as a simple but effective approach to securing data. Are you a manager? Then you’ll get access to data meant for managers. This is great for small deployments with clearly defined roles. Here at Booz Allen, we invested in Databricks because we have an environment of over 30k users and billions of rows of data.\n",
       " To mitigate this problem and align with our forward-thinking company standard, we introduced Immuta into our stack. Immuta uses ABAC to allow for dynamic data access control. Users are automatically assigned certain attributes, and access is based on those attributes instead of just their role. This allows for more flexibility and allows data access control to easily scale without the need to constantly map a user to their role. Using attributes, we can write policies in one place and have them applied across all of our data platforms. This makes for a truly holistic data governance approach and provides immediate ROI and time savings for the company.</td></tr><tr><td>Increasing Trust in Your Data: Enabling a Data Governance Program on Databricks Using Unity Catalog and ML-Driven MDM</td><td>As part of Comcast Effectv’s transformation into a completely digital advertising agency, it was key to develop an approach to manage and remediate data quality issues related to customer data so that the sales organization is using reliable data to enable data-driven decision making. Like many organizations, Effectv's customer lifecycle processes are spread across many systems utilizing various integrations between them. This results in key challenges like duplicate and redundant customer data that requires rationalization and remediation. Data is at the core of Effectv’s modernization journey with the intended result of winning more business, accelerating order fulfillment, reducing make-goods and identifying revenue.\n",
       "  \n",
       "In partnership with Slalom Consulting, Comcast Effectv built a traditional lakehouse on Databricks to ingest data from all of these systems but with a twist; they anchored every engineering decision in how it will enable their data governance program. \n",
       "  \n",
       "In this talk, we will touch upon the data transformation journey at Effectv and dive deeper into the implementation of data governance leveraging Databricks solutions such as Delta Lake, Unity Catalog and DB SQL. Key focus areas include how we baked master data management into our pipelines by automating the matching and survivorship process, and bringing it all together for the data consumer via DBSQL to use our certified assets in bronze, silver and gold layers.\n",
       "  \n",
       "By making thoughtful decisions about structuring data in Unity Catalog and baking MDM into ETL pipelines, you can greatly increase the quality, reliability, and adoption of single-source-of-truth data so your business users can stop spending cycles on wrangling data and spend more time developing actionable insights for your business.</td><td>Title: Increasing Trust in Your Data: Enabling a Data Governance Program on Databricks Using Unity Catalog and ML-Driven MDM\n",
       "                Abstract:  As part of Comcast Effectv’s transformation into a completely digital advertising agency, it was key to develop an approach to manage and remediate data quality issues related to customer data so that the sales organization is using reliable data to enable data-driven decision making. Like many organizations, Effectv's customer lifecycle processes are spread across many systems utilizing various integrations between them. This results in key challenges like duplicate and redundant customer data that requires rationalization and remediation. Data is at the core of Effectv’s modernization journey with the intended result of winning more business, accelerating order fulfillment, reducing make-goods and identifying revenue.\n",
       "  \n",
       "In partnership with Slalom Consulting, Comcast Effectv built a traditional lakehouse on Databricks to ingest data from all of these systems but with a twist; they anchored every engineering decision in how it will enable their data governance program. \n",
       "  \n",
       "In this talk, we will touch upon the data transformation journey at Effectv and dive deeper into the implementation of data governance leveraging Databricks solutions such as Delta Lake, Unity Catalog and DB SQL. Key focus areas include how we baked master data management into our pipelines by automating the matching and survivorship process, and bringing it all together for the data consumer via DBSQL to use our certified assets in bronze, silver and gold layers.\n",
       "  \n",
       "By making thoughtful decisions about structuring data in Unity Catalog and baking MDM into ETL pipelines, you can greatly increase the quality, reliability, and adoption of single-source-of-truth data so your business users can stop spending cycles on wrangling data and spend more time developing actionable insights for your business.</td></tr><tr><td>Activate Your Lakehouse with Unity Catalog</td><td>Building a Lakehouse is straightforward today thanks to many open-source technologies and Databricks. However, it can be taxing to extract value from Lakehouses as they grow in size without robust data operations. Join us to learn how YipitData uses the Unity Catalog to streamline data operations and discover best practices to scale your own Lakehouse.\n",
       " \n",
       " At YipitData, our 15+ petabyte Lakehouse is a self-service data platform built with Databricks and AWS, supporting analytics for a 250+ data team. We will share how leveraging Unity Catalog accelerates our mission to help financial institutions and corporations leverage alternative data by:\n",
       " \n",
       " - Enabling clients to universally access our data through a spectrum of channels, including Sigma, Delta Sharing, and multiple clouds \n",
       " - Fostering collaboration across internal teams using a data mesh paradigm that yields rich insights\n",
       " - Strengthening the integrity and security of data assets through ACLs, data lineage, audit logs, and further isolation of AWS resources\n",
       " - Reducing the cost of large tables without downtime through automated data expiration and ETL optimizations on managed delta tables\n",
       " \n",
       " Through our migration to Unity Catalog, we have gained tactics and philosophies to seamlessly flow our data assets internally and externally. Data platforms need to be value-generating, secure, and cost-effective in today's world. We are excited to share how Unity Catalog delivers on this and helps you get the most out of your Lakehouse.</td><td>Title: Activate Your Lakehouse with Unity Catalog\n",
       "                Abstract:  Building a Lakehouse is straightforward today thanks to many open-source technologies and Databricks. However, it can be taxing to extract value from Lakehouses as they grow in size without robust data operations. Join us to learn how YipitData uses the Unity Catalog to streamline data operations and discover best practices to scale your own Lakehouse.\n",
       " \n",
       " At YipitData, our 15+ petabyte Lakehouse is a self-service data platform built with Databricks and AWS, supporting analytics for a 250+ data team. We will share how leveraging Unity Catalog accelerates our mission to help financial institutions and corporations leverage alternative data by:\n",
       " \n",
       " - Enabling clients to universally access our data through a spectrum of channels, including Sigma, Delta Sharing, and multiple clouds \n",
       " - Fostering collaboration across internal teams using a data mesh paradigm that yields rich insights\n",
       " - Strengthening the integrity and security of data assets through ACLs, data lineage, audit logs, and further isolation of AWS resources\n",
       " - Reducing the cost of large tables without downtime through automated data expiration and ETL optimizations on managed delta tables\n",
       " \n",
       " Through our migration to Unity Catalog, we have gained tactics and philosophies to seamlessly flow our data assets internally and externally. Data platforms need to be value-generating, secure, and cost-effective in today's world. We are excited to share how Unity Catalog delivers on this and helps you get the most out of your Lakehouse.</td></tr><tr><td>Self Service Data Analytics and governance at enterprise scale with unity catalog</td><td>This session focuses on one of the first Unity Catalog implementations for a large-scale enterprise. In this scenario, a cloud scale analytics platform with 7500 active users based on the lakehouse approach is used. In addition, there is potential for 1500 further users who are subject to special governance rules. They are consuming 600+TB of data stored in Delta Lake - continuously growing at more than 1TB per day. This might grow due to local country data.\n",
       " Therefore, the existing data platform must be extended to enable users to combine global and local data from their countries. A new data management was required, which reflects the strict information security rules at a need to know base. Core requirements are: read only from global data, write into local and share the results.\n",
       " Due to a very pronounced information security awareness and a lack of the technological possibilities it was not possible to interdisciplinary analyze and exchange data so easy or at all so far. Therefore, a lot of business potential and gains could not be identified and realized.\n",
       " With the new developments in the technology used and the basis of the Lakehouse approach, thanks to Unity Catalog, we were able to develop a solution that could meet high requirements for security and process. And enables globally secured interdisciplinary data exchange and analysis at scale.\n",
       " This solution enables the democratization of the data. This results not only in the ability to gain better insights for business management, but also to generate entirely new business cases or products that require a higher degree of data integration and encourage the culture to change.\n",
       " We highlight technical challenges and solutions, present best practices and point out benefits of implementing Unity catalog for enterprises.</td><td>Title: Self Service Data Analytics and governance at enterprise scale with unity catalog\n",
       "                Abstract:  This session focuses on one of the first Unity Catalog implementations for a large-scale enterprise. In this scenario, a cloud scale analytics platform with 7500 active users based on the lakehouse approach is used. In addition, there is potential for 1500 further users who are subject to special governance rules. They are consuming 600+TB of data stored in Delta Lake - continuously growing at more than 1TB per day. This might grow due to local country data.\n",
       " Therefore, the existing data platform must be extended to enable users to combine global and local data from their countries. A new data management was required, which reflects the strict information security rules at a need to know base. Core requirements are: read only from global data, write into local and share the results.\n",
       " Due to a very pronounced information security awareness and a lack of the technological possibilities it was not possible to interdisciplinary analyze and exchange data so easy or at all so far. Therefore, a lot of business potential and gains could not be identified and realized.\n",
       " With the new developments in the technology used and the basis of the Lakehouse approach, thanks to Unity Catalog, we were able to develop a solution that could meet high requirements for security and process. And enables globally secured interdisciplinary data exchange and analysis at scale.\n",
       " This solution enables the democratization of the data. This results not only in the ability to gain better insights for business management, but also to generate entirely new business cases or products that require a higher degree of data integration and encourage the culture to change.\n",
       " We highlight technical challenges and solutions, present best practices and point out benefits of implementing Unity catalog for enterprises.</td></tr><tr><td>PII Detection at Scale on the Lakehouse</td><td>SEEK is Australia’s largest online employment marketplace and a market leader spanning ten countries across Asia Pacific and Latin America. SEEK provides employment opportunities for roughly 16 million monthly active users and process 25 million candidate applications to listings.\n",
       " \n",
       " Processing millions of resumès involves handling and managing highly sensitive candidate information, usually inputted in a highly unstructured format. With recent high-profile data leaks in Australia, personally identifiable information (PII) protection has become a major focus area for large digital organizations. The first step is detection, and SEEK has developed a custom framework built using HuggingFace transformers fine tuned with nuances around employment. For example, “Software Engineer at Databricks” is not PII, but “CEO at Databricks” is PII. \n",
       " \n",
       " After identifying and anonymizing PII in stream and batch data, SEEK uses Unity Catalog’s data lineage to track PII through their reporting, ETL, and other downstream ML use-cases and govern access control achieving an organization-wide data management capability driven by deep learning and enforcement using Databricks.</td><td>Title: PII Detection at Scale on the Lakehouse\n",
       "                Abstract:  SEEK is Australia’s largest online employment marketplace and a market leader spanning ten countries across Asia Pacific and Latin America. SEEK provides employment opportunities for roughly 16 million monthly active users and process 25 million candidate applications to listings.\n",
       " \n",
       " Processing millions of resumès involves handling and managing highly sensitive candidate information, usually inputted in a highly unstructured format. With recent high-profile data leaks in Australia, personally identifiable information (PII) protection has become a major focus area for large digital organizations. The first step is detection, and SEEK has developed a custom framework built using HuggingFace transformers fine tuned with nuances around employment. For example, “Software Engineer at Databricks” is not PII, but “CEO at Databricks” is PII. \n",
       " \n",
       " After identifying and anonymizing PII in stream and batch data, SEEK uses Unity Catalog’s data lineage to track PII through their reporting, ETL, and other downstream ML use-cases and govern access control achieving an organization-wide data management capability driven by deep learning and enforcement using Databricks.</td></tr><tr><td>Essential Data Security Strategies for the Modern Enterprise Data Architecture</td><td>Balancing critical data requirements is a 24-7 task for enterprise-level organizations that must straddle the need to open specific gates to enable self-service data access while closing other access points to maintain internal and external compliance. Data breaches can cost U.S. businesses an average of $9.4 million per occurrence; ignoring this leaves organizations vulnerable to severe losses and crippling costs. \n",
       " \n",
       "The 2022 Gartner Hype Cycle for Data Security reports that more and more enterprises are modernizing their data architecture with cloud and technology partners to help them collect, store and manage business data; a trend that does not appear to be letting up.\n",
       " \n",
       "According to Gartner®, “by 2025, 30% of enterprises will have adopted bDSP (Broad Data Security Platform), up from less than 10% in 2021, due to the pent-up demand for higher levels of data security and the rapid increase in product capabilities.\"</td><td>Title: Essential Data Security Strategies for the Modern Enterprise Data Architecture\n",
       "                Abstract:  Balancing critical data requirements is a 24-7 task for enterprise-level organizations that must straddle the need to open specific gates to enable self-service data access while closing other access points to maintain internal and external compliance. Data breaches can cost U.S. businesses an average of $9.4 million per occurrence; ignoring this leaves organizations vulnerable to severe losses and crippling costs. \n",
       " \n",
       "The 2022 Gartner Hype Cycle for Data Security reports that more and more enterprises are modernizing their data architecture with cloud and technology partners to help them collect, store and manage business data; a trend that does not appear to be letting up.\n",
       " \n",
       "According to Gartner®, “by 2025, 30% of enterprises will have adopted bDSP (Broad Data Security Platform), up from less than 10% in 2021, due to the pent-up demand for higher levels of data security and the rapid increase in product capabilities.\"</td></tr><tr><td> \n",
       "Moving to both a modern data architecture and data-driven culture sets enterprises on the right trajectory for growth</td><td> but it’s important to keep in mind individual public cloud platforms are not guaranteed to protect and secure data. To solve this</td><td>Title:  \n",
       "Moving to both a modern data architecture and data-driven culture sets enterprises on the right trajectory for growth\n",
       "                Abstract:   but it’s important to keep in mind individual public cloud platforms are not guaranteed to protect and secure data. To solve this</td></tr><tr><td>Using Open-Source Tools to Build Privacy-Conscious Data Systems</td><td>With the rapid proliferation of consumer data privacy laws across the world, it is becoming a strict requirement for data organizations to be mindful of data privacy risks. Privacy violation fines are reaching record highs and will only get higher as governments continue to crack down on the runaway abuse of user data. To continue producing value without becoming a liability, data systems must include privacy protections at a foundational level.\n",
       " \n",
       " The most practical way to do this is to enable privacy as code, shifting privacy left and including it as a foundational part of the organization's software development life cycle. The promise of privacy as code is that data organizations can be liberated from inefficient, manual workflows for producing the compliance deliverables their legal teams need, and instead ship at speed with pre-defined privacy guardrails built into the structure of their preferred workflows.\n",
       " \n",
       " Despite being an emerging and complex problem, there are already powerful open source tools available designed to help organizations of all sizes achieve this outcome. Fides is an open source privacy as code tool, written in Python and Typescript, that is engineered to tackle a variety of privacy problems throughout the application lifecycle. The most relevant feature for data organizations is the ability to annotate systems and their datasets with data privacy metadata, thus enabling automatic rejection of dangerous or illegal uses. Fides empowers data organizations to be proactive, not reactive, in terms of protecting user privacy and reducing organizational risk. Moving forward data privacy will need to be top of mind for data teams.</td><td>Title: Using Open-Source Tools to Build Privacy-Conscious Data Systems\n",
       "                Abstract:  With the rapid proliferation of consumer data privacy laws across the world, it is becoming a strict requirement for data organizations to be mindful of data privacy risks. Privacy violation fines are reaching record highs and will only get higher as governments continue to crack down on the runaway abuse of user data. To continue producing value without becoming a liability, data systems must include privacy protections at a foundational level.\n",
       " \n",
       " The most practical way to do this is to enable privacy as code, shifting privacy left and including it as a foundational part of the organization's software development life cycle. The promise of privacy as code is that data organizations can be liberated from inefficient, manual workflows for producing the compliance deliverables their legal teams need, and instead ship at speed with pre-defined privacy guardrails built into the structure of their preferred workflows.\n",
       " \n",
       " Despite being an emerging and complex problem, there are already powerful open source tools available designed to help organizations of all sizes achieve this outcome. Fides is an open source privacy as code tool, written in Python and Typescript, that is engineered to tackle a variety of privacy problems throughout the application lifecycle. The most relevant feature for data organizations is the ability to annotate systems and their datasets with data privacy metadata, thus enabling automatic rejection of dangerous or illegal uses. Fides empowers data organizations to be proactive, not reactive, in terms of protecting user privacy and reducing organizational risk. Moving forward data privacy will need to be top of mind for data teams.</td></tr><tr><td>Leveraging Unity Catalog for Data Governance for Grab’s Use Case</td><td>Grab has been looking for a tool that provides data access to over 2000 users to support our daily operations. This tool should provide granular access control and easily scale to over thousands of users and tables, with data governance and security in mind.\n",
       " \n",
       " Through various evaluations, Grab concluded that Unity Catalog is a good fit that addresses all the requirements of data governance and security. For example, it provides row-level-security (RLS) and dynamic-data-masking (DDM) capabilities (through dynamic views), centralized governance and security at both table and object level. \n",
       " This ensures sensitive PII data is well protected, and we are able to open up tables to a wider audience ensuring only entitled data is shown. More importantly, we are able to meet the required auditing standards.\n",
       " \n",
       " With Unity catalog, we managed to:\n",
       " 1. Build a custom solution that allows users to seamlessly access data with UC.\n",
       " 2. Easily interface and integrate into existing Grab’s in-house entitlement system to translate access requirements, unifying the end-to-end user experience for requesting data access permission. \n",
       " 3. Extend UC’s capability by leveraging existing UC APIs and Databricks workflows to easily automate administrative tasks. For example, we have fully automated the creation of RLS and DDM views on-the-fly.\n",
       " 4. Provide out of the box, comprehensive auditing capabilities easily.\n",
       " \n",
       " As a result, we managed to achieve this in a short period of time to support over 2000 users to achieve better governance and significant cost savings with an ROI of about 2.7x</td><td>Title: Leveraging Unity Catalog for Data Governance for Grab’s Use Case\n",
       "                Abstract:  Grab has been looking for a tool that provides data access to over 2000 users to support our daily operations. This tool should provide granular access control and easily scale to over thousands of users and tables, with data governance and security in mind.\n",
       " \n",
       " Through various evaluations, Grab concluded that Unity Catalog is a good fit that addresses all the requirements of data governance and security. For example, it provides row-level-security (RLS) and dynamic-data-masking (DDM) capabilities (through dynamic views), centralized governance and security at both table and object level. \n",
       " This ensures sensitive PII data is well protected, and we are able to open up tables to a wider audience ensuring only entitled data is shown. More importantly, we are able to meet the required auditing standards.\n",
       " \n",
       " With Unity catalog, we managed to:\n",
       " 1. Build a custom solution that allows users to seamlessly access data with UC.\n",
       " 2. Easily interface and integrate into existing Grab’s in-house entitlement system to translate access requirements, unifying the end-to-end user experience for requesting data access permission. \n",
       " 3. Extend UC’s capability by leveraging existing UC APIs and Databricks workflows to easily automate administrative tasks. For example, we have fully automated the creation of RLS and DDM views on-the-fly.\n",
       " 4. Provide out of the box, comprehensive auditing capabilities easily.\n",
       " \n",
       " As a result, we managed to achieve this in a short period of time to support over 2000 users to achieve better governance and significant cost savings with an ROI of about 2.7x</td></tr><tr><td>Unity Catalog: Flexibility to Fit Your Organization</td><td>Databricks Unity Catalog provides the flexibility to meet the needs of any-sized organization, from startups to companies the size of Nike. \n",
       " \n",
       " Unity Catalog and lakehouse architecture have been chosen by Nike to store and process all of Nike's data. Unity Catalog has resolved systemic challenges in the Data Lake, resulting in 1) a simpler and transparent Data Lake, and, 2) allowing for scaled data and analytics use cases with greater efficiency. \n",
       " \n",
       " Designing Unity Catalog for the scale of Nike met the following goals:\n",
       " \n",
       " * Ownership of data: how all data produced in the lake has clear team ownership, communication channels, and accountability\n",
       " * Data environments: accelerated development of pipelines against real Production Data with zero risk\n",
       " * Consistent team semantics: all teams on Lakehouse follow the same semantics, allowing for scaled onboarding and consistent development expectations\n",
       " * Consistent governed data access: a single simplified, governed, data access process for all data in the Lake\n",
       " * Monitoring of unused data and data pipelines: simplified observability through Unity Catalog Data Lineage to quickly identify any unused data or data pipelines, resulting in less storage and compute costs\n",
       " \n",
       " These capabilities are powering Nike as a data driven organization, allowing the company to quickly make informed decisions.</td><td>Title: Unity Catalog: Flexibility to Fit Your Organization\n",
       "                Abstract:  Databricks Unity Catalog provides the flexibility to meet the needs of any-sized organization, from startups to companies the size of Nike. \n",
       " \n",
       " Unity Catalog and lakehouse architecture have been chosen by Nike to store and process all of Nike's data. Unity Catalog has resolved systemic challenges in the Data Lake, resulting in 1) a simpler and transparent Data Lake, and, 2) allowing for scaled data and analytics use cases with greater efficiency. \n",
       " \n",
       " Designing Unity Catalog for the scale of Nike met the following goals:\n",
       " \n",
       " * Ownership of data: how all data produced in the lake has clear team ownership, communication channels, and accountability\n",
       " * Data environments: accelerated development of pipelines against real Production Data with zero risk\n",
       " * Consistent team semantics: all teams on Lakehouse follow the same semantics, allowing for scaled onboarding and consistent development expectations\n",
       " * Consistent governed data access: a single simplified, governed, data access process for all data in the Lake\n",
       " * Monitoring of unused data and data pipelines: simplified observability through Unity Catalog Data Lineage to quickly identify any unused data or data pipelines, resulting in less storage and compute costs\n",
       " \n",
       " These capabilities are powering Nike as a data driven organization, allowing the company to quickly make informed decisions.</td></tr><tr><td>Automating Sensitive Data (PII/PHI) Detection and Quarantining to a Databricks Clean Room</td><td>Healthcare datasets contain both personally identifiable information (PII) and personal health information (PHI) that needs to be de-identified in order to protect patient confidentiality and ensure HIPAA compliance. This privacy data is easily detected when it’s provided in columns  labeled with names such as “SSN,” First Name,” “Full Name,” and “DOB;” however, it is much harder to detect when it is hidden within columns labeled “Doctor Notes,” “Diagnoses,” or “Comments.” HealthVerity, a leader in the HIPAA-compliant exchange of real-world data (RWD) to uncover patient, payer and genomic insights and power innovation for the healthcare industry, ensures healthcare datasets are de-identified from PII and PHI using elaborate privacy procedures. During this presentation, we will demonstrate how to use a low-code/no-code platform to simplify and automate data pipelines that leverage prebuilt ML models to scan data for PHI/PII leakage and quarantine those rows in Unity Catalog when leakage is identified and move them to a Databricks clean room for analysis.</td><td>Title: Automating Sensitive Data (PII/PHI) Detection and Quarantining to a Databricks Clean Room\n",
       "                Abstract:  Healthcare datasets contain both personally identifiable information (PII) and personal health information (PHI) that needs to be de-identified in order to protect patient confidentiality and ensure HIPAA compliance. This privacy data is easily detected when it’s provided in columns  labeled with names such as “SSN,” First Name,” “Full Name,” and “DOB;” however, it is much harder to detect when it is hidden within columns labeled “Doctor Notes,” “Diagnoses,” or “Comments.” HealthVerity, a leader in the HIPAA-compliant exchange of real-world data (RWD) to uncover patient, payer and genomic insights and power innovation for the healthcare industry, ensures healthcare datasets are de-identified from PII and PHI using elaborate privacy procedures. During this presentation, we will demonstrate how to use a low-code/no-code platform to simplify and automate data pipelines that leverage prebuilt ML models to scan data for PHI/PII leakage and quarantine those rows in Unity Catalog when leakage is identified and move them to a Databricks clean room for analysis.</td></tr><tr><td>Cross-Platform Data Lineage with OpenLineage</td><td>There are more data tools available than ever before, and it's easier to build a pipeline than it's ever been. This has resulted in an explosion of innovation, but it also means that data within today's organizations has become increasingly distributed. It can't be contained within a single brain, a single team, or a single platform. Data lineage can help by tracing the relationships between datasets and providing a map of your entire data universe. OpenLineage provides a standard for lineage collection that spans multiple platforms, including Apache Airflow, Apache Spark™, Flink®, and dbt. This empowers teams to diagnose and address widespread data quality and efficiency issues in real time. In this session, Julien Le Dem from Astronomer will show how to trace data lineage across Apache Spark and Apache Airflow. He will walk through the OpenLineage architecture and provide a live demo of a running pipeline with real-time data lineage.</td><td>Title: Cross-Platform Data Lineage with OpenLineage\n",
       "                Abstract:  There are more data tools available than ever before, and it's easier to build a pipeline than it's ever been. This has resulted in an explosion of innovation, but it also means that data within today's organizations has become increasingly distributed. It can't be contained within a single brain, a single team, or a single platform. Data lineage can help by tracing the relationships between datasets and providing a map of your entire data universe. OpenLineage provides a standard for lineage collection that spans multiple platforms, including Apache Airflow, Apache Spark™, Flink®, and dbt. This empowers teams to diagnose and address widespread data quality and efficiency issues in real time. In this session, Julien Le Dem from Astronomer will show how to trace data lineage across Apache Spark and Apache Airflow. He will walk through the OpenLineage architecture and provide a live demo of a running pipeline with real-time data lineage.</td></tr><tr><td>Ahold Delhaize's Journey to Implementing Unity Catalog at Scale</td><td>In this session we will explore the steps we took to implement Unity Catalog at Ahold Delhaize's data platform. This powerful tool has completely transformed the way we access and manage data objects, making it easier than ever for our entire organization to find and use the data we need. The catalog brings us a governance layer that allows us to more securely and easily share restricted or confidential data by providing row and column level security per project. But that's not all, we're also setting up the Unity Catalog as a self-service tool, allowing users to easily customize and set up their own catalogs, schema's, views or user groups using simple YAML configuration files. We will cover topics such as:\n",
       " * Architecture where we came from and the improvement Unity Catalog has made\n",
       " * Our self-service way of working by showing code snippets\n",
       " * Our fully automatic deployments by explaining our flow\n",
       " By the end of this talk, you will have a better understanding of the process of implementing Unity Catalog at scale.</td><td>Title: Ahold Delhaize's Journey to Implementing Unity Catalog at Scale\n",
       "                Abstract:  In this session we will explore the steps we took to implement Unity Catalog at Ahold Delhaize's data platform. This powerful tool has completely transformed the way we access and manage data objects, making it easier than ever for our entire organization to find and use the data we need. The catalog brings us a governance layer that allows us to more securely and easily share restricted or confidential data by providing row and column level security per project. But that's not all, we're also setting up the Unity Catalog as a self-service tool, allowing users to easily customize and set up their own catalogs, schema's, views or user groups using simple YAML configuration files. We will cover topics such as:\n",
       " * Architecture where we came from and the improvement Unity Catalog has made\n",
       " * Our self-service way of working by showing code snippets\n",
       " * Our fully automatic deployments by explaining our flow\n",
       " By the end of this talk, you will have a better understanding of the process of implementing Unity Catalog at scale.</td></tr><tr><td>Multi-Cloud Data Governance on the Databricks Lakehouse</td><td>Across industries, a multi-cloud setup has quickly become the reality for large organizations. Multi-cloud introduces new governance challenges as permissions models often do not translate from one cloud to the other and if they do, are insufficiently granular to accommodate privacy requirements and principles of least privilege. This problem can be especially acute for Data and AI workloads that rely on sharing and aggregating large and diverse data sources across business unit boundaries and where governance models need to incorporate assets such as table rows/columns and ML features and models. \n",
       " \n",
       " In this session we will provide guidelines on how best to overcome these challenges for companies that have adopted the Databricks Lakehouse as their collaborative space for data teams across the organization, by exploiting some of the unique product features of the Databricks platform.\n",
       " We will focus on a common scenario: a data platform team providing data assets to two different ML teams, one using the same cloud and the other one using a different cloud. We will explain the step-by-step setup of a unified governance model by leveraging the following components and conventions:\n",
       " \n",
       " - Unity Catalog for implementing fine-grained access control across all data assets: files in cloud storage, rows and columns in tables and ML features and models\n",
       " - The Databricks Terraform provider to automatically enforce guardrails and permissions across clouds \n",
       " - Account level SSO Integration and identity federation to centralize administer access across workspaces\n",
       " - Delta sharing to seamlessly propagate changes in provider data sets to consumers in near real-time\n",
       " - Centralized audit logging for a unified view on what asset was accessed by whom</td><td>Title: Multi-Cloud Data Governance on the Databricks Lakehouse\n",
       "                Abstract:  Across industries, a multi-cloud setup has quickly become the reality for large organizations. Multi-cloud introduces new governance challenges as permissions models often do not translate from one cloud to the other and if they do, are insufficiently granular to accommodate privacy requirements and principles of least privilege. This problem can be especially acute for Data and AI workloads that rely on sharing and aggregating large and diverse data sources across business unit boundaries and where governance models need to incorporate assets such as table rows/columns and ML features and models. \n",
       " \n",
       " In this session we will provide guidelines on how best to overcome these challenges for companies that have adopted the Databricks Lakehouse as their collaborative space for data teams across the organization, by exploiting some of the unique product features of the Databricks platform.\n",
       " We will focus on a common scenario: a data platform team providing data assets to two different ML teams, one using the same cloud and the other one using a different cloud. We will explain the step-by-step setup of a unified governance model by leveraging the following components and conventions:\n",
       " \n",
       " - Unity Catalog for implementing fine-grained access control across all data assets: files in cloud storage, rows and columns in tables and ML features and models\n",
       " - The Databricks Terraform provider to automatically enforce guardrails and permissions across clouds \n",
       " - Account level SSO Integration and identity federation to centralize administer access across workspaces\n",
       " - Delta sharing to seamlessly propagate changes in provider data sets to consumers in near real-time\n",
       " - Centralized audit logging for a unified view on what asset was accessed by whom</td></tr><tr><td>How Comcast Effectv Drives Data Observability with Databricks and Monte Carlo</td><td>Comcast Effectv, the 2,000-employee advertising wing of Comcast, America’s largest telecommunications company, provides custom video ad solutions powered by aggregated viewership data. As a global technology and media company connecting millions of customers to personalized experiences and processing billions of transactions, Comcast Effectv was challenged with handling massive loads of data, monitoring hundreds of data pipelines, and managing timely coordination across data teams. In this talk, Robinson Creighton, Sr. DataOps Lead of Advanced Analytics at Effectv, and Lior Gavish, CTO & co-founder at Monte Carlo, will discuss Comcast Effectv’s journey to building a more scalable, reliable lakehouse and driving data observability at scale with Monte Carlo. This has enabled Effectv to have a single pane of glass view of their entire data environment to ensure consumer data trust across their entire AWS, Databricks, and Looker environment.</td><td>Title: How Comcast Effectv Drives Data Observability with Databricks and Monte Carlo\n",
       "                Abstract:  Comcast Effectv, the 2,000-employee advertising wing of Comcast, America’s largest telecommunications company, provides custom video ad solutions powered by aggregated viewership data. As a global technology and media company connecting millions of customers to personalized experiences and processing billions of transactions, Comcast Effectv was challenged with handling massive loads of data, monitoring hundreds of data pipelines, and managing timely coordination across data teams. In this talk, Robinson Creighton, Sr. DataOps Lead of Advanced Analytics at Effectv, and Lior Gavish, CTO & co-founder at Monte Carlo, will discuss Comcast Effectv’s journey to building a more scalable, reliable lakehouse and driving data observability at scale with Monte Carlo. This has enabled Effectv to have a single pane of glass view of their entire data environment to ensure consumer data trust across their entire AWS, Databricks, and Looker environment.</td></tr><tr><td>Unity Catalog and Marketplace: Door to a Data Driven Organization</td><td>Being data-driven can help organizations make better decisions, improve efficiency, increase competitiveness, and reduce compliance risks.\n",
       " \n",
       " Organizations expect the workforce to embrace data but at the same time want the process to be governance-driven, auditable, traceable and cost-center based for back charging.\n",
       " \n",
       " Unity Catalog is a tool that helps organizations manage and organize their data assets in a unified manner. By using Unity Catalog, you can create a central repository for all of your data assets, including structured and unstructured data, and provide access to this repository to authorized users within your organization. This can help you build a data-driven organization by enabling easy access to data for analysis and decision-making, and by promoting data governance and data management best practices.\n",
       " When Unity Catalog is made the front door for an organization’s data estate then it enables the organization to unlock the following:\n",
       " Governance-driven data discovery\n",
       " Governance-driven data access\n",
       " Governance-driven ephemeral workspace access\n",
       " Cost center based back charging\n",
       " Auditable and traceable\n",
       " \n",
       " Internal marketplace will help organizations publish and provide access to data within the organization’s workspaces via an out-of-the-box lightweight solution powered by Unity Catalog. \n",
       " \n",
       " Depending on the level of complexity and back charging, isolation requirements, organizations can choose to use the out-of-the-box internal marketplace or build the end-to-end process of publishing, provisioning, destroying, back charging process via Unity Catalog plus Workspace APIs.</td><td>Title: Unity Catalog and Marketplace: Door to a Data Driven Organization\n",
       "                Abstract:  Being data-driven can help organizations make better decisions, improve efficiency, increase competitiveness, and reduce compliance risks.\n",
       " \n",
       " Organizations expect the workforce to embrace data but at the same time want the process to be governance-driven, auditable, traceable and cost-center based for back charging.\n",
       " \n",
       " Unity Catalog is a tool that helps organizations manage and organize their data assets in a unified manner. By using Unity Catalog, you can create a central repository for all of your data assets, including structured and unstructured data, and provide access to this repository to authorized users within your organization. This can help you build a data-driven organization by enabling easy access to data for analysis and decision-making, and by promoting data governance and data management best practices.\n",
       " When Unity Catalog is made the front door for an organization’s data estate then it enables the organization to unlock the following:\n",
       " Governance-driven data discovery\n",
       " Governance-driven data access\n",
       " Governance-driven ephemeral workspace access\n",
       " Cost center based back charging\n",
       " Auditable and traceable\n",
       " \n",
       " Internal marketplace will help organizations publish and provide access to data within the organization’s workspaces via an out-of-the-box lightweight solution powered by Unity Catalog. \n",
       " \n",
       " Depending on the level of complexity and back charging, isolation requirements, organizations can choose to use the out-of-the-box internal marketplace or build the end-to-end process of publishing, provisioning, destroying, back charging process via Unity Catalog plus Workspace APIs.</td></tr><tr><td>Data Mesh Realization on Databricks: Making Data Engineering and Consumption Self-Service Driven for Data Platforms</td><td>\"Our client, a consulting giant, as part of the famous \"\"Big 4\"\" is creating tax</td><td>Title: Data Mesh Realization on Databricks: Making Data Engineering and Consumption Self-Service Driven for Data Platforms\n",
       "                Abstract:  \"Our client, a consulting giant, as part of the famous \"\"Big 4\"\" is creating tax</td></tr><tr><td>Distributing Data Governance: How Unity Catalog Allows for a Collaborative Approach</td><td>As one of the world’s largest content delivery network (CDN) and security solutions provider, Akamai owns thousands of data assets of various shapes and sizes, some even go up to multiple PBs.\n",
       " Several departments within the company leverage Databricks for their data and AI workloads, which means we have over a hundred Databricks workspaces within a single Databricks account, where some of the assets are shared across products, and some are product-specific.\n",
       "  \n",
       " In this presentation, we will describe how to use the capabilities of Unity Catalog to distribute the administration burden between departments, while still maintaining a unified governance model.\n",
       " \n",
       " We will also share the benefits we’ve found in using Unity Catalog, beyond just access management, such as: \n",
       " * Visibility into which data assets we have in the organization\n",
       " * Ability to identify and potentially eliminate duplicate data workloads between departments \n",
       " * Removing boilerplate code for accessing external sources\n",
       " * Increasing innovation of product teams by exposing the data assets in a better, more efficient way</td><td>Title: Distributing Data Governance: How Unity Catalog Allows for a Collaborative Approach\n",
       "                Abstract:  As one of the world’s largest content delivery network (CDN) and security solutions provider, Akamai owns thousands of data assets of various shapes and sizes, some even go up to multiple PBs.\n",
       " Several departments within the company leverage Databricks for their data and AI workloads, which means we have over a hundred Databricks workspaces within a single Databricks account, where some of the assets are shared across products, and some are product-specific.\n",
       "  \n",
       " In this presentation, we will describe how to use the capabilities of Unity Catalog to distribute the administration burden between departments, while still maintaining a unified governance model.\n",
       " \n",
       " We will also share the benefits we’ve found in using Unity Catalog, beyond just access management, such as: \n",
       " * Visibility into which data assets we have in the organization\n",
       " * Ability to identify and potentially eliminate duplicate data workloads between departments \n",
       " * Removing boilerplate code for accessing external sources\n",
       " * Increasing innovation of product teams by exposing the data assets in a better, more efficient way</td></tr><tr><td>Unity Catalog at Scale in Retail Data Engineering and Data Science</td><td>Retail data science, insights, and media company 84.51° helps Kroger and other organizations create more personalized and valuable experiences for shoppers. Powered by cutting edge science, we leverage first-party retail data from nearly one out of two US households totaling over 2B transactions a year. Databricks is integral to the data science and machine learning work at 84.51° and manages 35+ PB of first-party customer data across 150+ Databricks workspaces. Before Unity Catalog, this data was structured in a decentralized data mesh which had issues including problematic data sharing, minimal data governance, and no auditing of data extracts. In this session, we will discuss how 84.51° incrementally rolled out Unity Catalog company-wide in a way that involved minimal disruption of existing processes and how UC-enabled solutions solved several real-world problems including:\n",
       " • Being able to migrate an on-prem solution to a Databricks SQL Warehouse with HIPAA auditing and data governance which resulted in reduced on-prem costs and enabled new solutions/products to bring significant new revenue sources. \n",
       " • Leveraging Delta Sharing to eliminate duplication of data and processing and enabling new insights to provide a deeper, more personal approach to customer engagement.\n",
       "Databricks has also been key to the 84.51° Collaborative Cloud, which provides our clients' data scientists a platform that provides clean and ready-to-use transaction and UPC-level data from 60 million households empowering companies to get value out of data science, regardless of where they sit on the readiness spectrum. Unity Catalog will be key to extend the collaborative cloud functionality and cleanroom technology with improved security and governance.</td><td>Title: Unity Catalog at Scale in Retail Data Engineering and Data Science\n",
       "                Abstract:  Retail data science, insights, and media company 84.51° helps Kroger and other organizations create more personalized and valuable experiences for shoppers. Powered by cutting edge science, we leverage first-party retail data from nearly one out of two US households totaling over 2B transactions a year. Databricks is integral to the data science and machine learning work at 84.51° and manages 35+ PB of first-party customer data across 150+ Databricks workspaces. Before Unity Catalog, this data was structured in a decentralized data mesh which had issues including problematic data sharing, minimal data governance, and no auditing of data extracts. In this session, we will discuss how 84.51° incrementally rolled out Unity Catalog company-wide in a way that involved minimal disruption of existing processes and how UC-enabled solutions solved several real-world problems including:\n",
       " • Being able to migrate an on-prem solution to a Databricks SQL Warehouse with HIPAA auditing and data governance which resulted in reduced on-prem costs and enabled new solutions/products to bring significant new revenue sources. \n",
       " • Leveraging Delta Sharing to eliminate duplication of data and processing and enabling new insights to provide a deeper, more personal approach to customer engagement.\n",
       "Databricks has also been key to the 84.51° Collaborative Cloud, which provides our clients' data scientists a platform that provides clean and ready-to-use transaction and UPC-level data from 60 million households empowering companies to get value out of data science, regardless of where they sit on the readiness spectrum. Unity Catalog will be key to extend the collaborative cloud functionality and cleanroom technology with improved security and governance.</td></tr><tr><td>Lakehouse as a FAIR Platform</td><td>FAIR (findable, accessible, interoperable, reusable) data and data platforms are becoming more and more important in public sector. Lakehouse platform is strongly aligned with these principles. Lakehouse provides tools required to both adhere to FAIR but also to FAIRify data that isn't FAIR compliant. In this session we will cover parts of the lakehouse that enable end users to FAIRify data products, how to build good robust data products and which parts of Lakehouse align to which principles in FAIR. We'll demonstrate how DLT is crucial for data transformations on nonFAIR data, how Unity Catalog unlocks discoverability (F) and governed data access (A), and how marketplace, cleanrooms and Delta Sharing unlock interoperability and data exchange (I and R). All of these concepts are massive enablers for highly regulated industries such as Public Sector. It undeniably important to align Lakehouse to standards that are widely adopted by standards and policy makers and regulators. These principles transcend all industries and all use cases.</td><td>Title: Lakehouse as a FAIR Platform\n",
       "                Abstract:  FAIR (findable, accessible, interoperable, reusable) data and data platforms are becoming more and more important in public sector. Lakehouse platform is strongly aligned with these principles. Lakehouse provides tools required to both adhere to FAIR but also to FAIRify data that isn't FAIR compliant. In this session we will cover parts of the lakehouse that enable end users to FAIRify data products, how to build good robust data products and which parts of Lakehouse align to which principles in FAIR. We'll demonstrate how DLT is crucial for data transformations on nonFAIR data, how Unity Catalog unlocks discoverability (F) and governed data access (A), and how marketplace, cleanrooms and Delta Sharing unlock interoperability and data exchange (I and R). All of these concepts are massive enablers for highly regulated industries such as Public Sector. It undeniably important to align Lakehouse to standards that are widely adopted by standards and policy makers and regulators. These principles transcend all industries and all use cases.</td></tr><tr><td>Engineers Shouldn't Write Data Governance Policies</td><td>Controlling permissions for accessing data assets can be messy, time consuming, and usually a combination of both. The teams responsible for creating the business rules that govern who should have access to what data are usually different from the teams responsible for administering the grants to achieve that access. On the other side of the equation, the end user who needs access to a data asset may be left waiting for grants to be made as the decision is passed between teams. That is, if they even know the correct path to getting access in the first place.\n",
       "  \n",
       " Separating the concerns of managing data governance at a business level and implementing data governance at an engineering level is the best way to clarify data access permissions. In practice, this involves building systems to enable data governance enforcement based on business rules, with little to no understanding of the individual system where the data lives. \n",
       " \n",
       " In practice, with a concrete business rule, such as “only users from the finance team should have access to critical financial data,” we want a system that deals only with those constituent concepts. For example, “the data is marked as critical financial” and “the user is a part of the finance team”). By abstracting away any source system components, such as “the tables in the finance schema” and “someone who’s a member of the finance databricks group,” the access policies applied will then model the business rules as closely as possible.\n",
       " \n",
       " This talk will focus on how to establish and align the processes, policies, and stakeholders involved in making this type of system work seamlessly. Sharing the experience and learnings of our team at Instacart, we will aim to help attendees streamline and simplify their data security and access strategies.</td><td>Title: Engineers Shouldn't Write Data Governance Policies\n",
       "                Abstract:  Controlling permissions for accessing data assets can be messy, time consuming, and usually a combination of both. The teams responsible for creating the business rules that govern who should have access to what data are usually different from the teams responsible for administering the grants to achieve that access. On the other side of the equation, the end user who needs access to a data asset may be left waiting for grants to be made as the decision is passed between teams. That is, if they even know the correct path to getting access in the first place.\n",
       "  \n",
       " Separating the concerns of managing data governance at a business level and implementing data governance at an engineering level is the best way to clarify data access permissions. In practice, this involves building systems to enable data governance enforcement based on business rules, with little to no understanding of the individual system where the data lives. \n",
       " \n",
       " In practice, with a concrete business rule, such as “only users from the finance team should have access to critical financial data,” we want a system that deals only with those constituent concepts. For example, “the data is marked as critical financial” and “the user is a part of the finance team”). By abstracting away any source system components, such as “the tables in the finance schema” and “someone who’s a member of the finance databricks group,” the access policies applied will then model the business rules as closely as possible.\n",
       " \n",
       " This talk will focus on how to establish and align the processes, policies, and stakeholders involved in making this type of system work seamlessly. Sharing the experience and learnings of our team at Instacart, we will aim to help attendees streamline and simplify their data security and access strategies.</td></tr><tr><td>Managing Data Encryption in Apache Spark™</td><td>Sensitive data sets can be encrypted directly by new Apache Spark™  versions (3.2 and higher). Setting a number of configuration parameters and dataframe options will trigger the Apache Parquet modular encryption mechanism that protects select columns with column-specific keys. The upcoming Spark 3.4 version will also support uniform encryption, where all dataframe columns are encrypted with the same key.\n",
       " \n",
       " Spark data encryption is already leveraged by a number of companies to protect personal or business confidential data in their production environments. The main integration effort is focused on key access control and on building a Spark/Parquet plug-in code that can interact with company’s key management service (KMS).\n",
       " \n",
       " In this talk, we will briefly cover the basics of Spark/Parquet encryption usage, and dive into the details of encryption key management that will help in integrating this Spark data protection mechanism in your deployment. You will learn how to run a HelloWorld encryption sample, and how to extend it into a real world production code integrated with your organization’s KMS and access control policies. We will talk about the standard envelope encryption approach to big data protection, the performance-vs-security trade-offs between single and double envelope wrapping, internal and external key metadata storage. We will see a demo, and discuss the new features such as uniform encryption and two-tier management of encryption keys.</td><td>Title: Managing Data Encryption in Apache Spark™\n",
       "                Abstract:  Sensitive data sets can be encrypted directly by new Apache Spark™  versions (3.2 and higher). Setting a number of configuration parameters and dataframe options will trigger the Apache Parquet modular encryption mechanism that protects select columns with column-specific keys. The upcoming Spark 3.4 version will also support uniform encryption, where all dataframe columns are encrypted with the same key.\n",
       " \n",
       " Spark data encryption is already leveraged by a number of companies to protect personal or business confidential data in their production environments. The main integration effort is focused on key access control and on building a Spark/Parquet plug-in code that can interact with company’s key management service (KMS).\n",
       " \n",
       " In this talk, we will briefly cover the basics of Spark/Parquet encryption usage, and dive into the details of encryption key management that will help in integrating this Spark data protection mechanism in your deployment. You will learn how to run a HelloWorld encryption sample, and how to extend it into a real world production code integrated with your organization’s KMS and access control policies. We will talk about the standard envelope encryption approach to big data protection, the performance-vs-security trade-offs between single and double envelope wrapping, internal and external key metadata storage. We will see a demo, and discuss the new features such as uniform encryption and two-tier management of encryption keys.</td></tr><tr><td>Enabling Data Governance at Enterprise Scale Using Unity Catalog</td><td>For some time, Amgen has been building multiple enterprise platforms on the latest technology with a key focus on tech rationalization, data democratization, overall user experience, increase reusability and cost-effectiveness. One of these platforms is the enterprise data fabric platform. This platform specifically focuses on pulling in data across functions into a single platform building capabilities around the connectedness of data, and access governance.\n",
       " For a while, we have been trying to setup robust data governance capabilities which are simple, yet easy to manage through Databricks. As part of this process we evaluated products like privacera, Immuta, and Okera. While these tools could solve a few immediate needs like managing tables and database level, for the use cases like maintaining governance on highly restricted data domains like EDW(Finance) and Workday(HR), we felt that for the long term we will require a more Databricks native solution because of below reasons:\n",
       " - There were ways to bypass these tools on databricks cluster\n",
       " - Unsupported DBR runtime\n",
       " - Complexity of fine-grained security\n",
       " - Policy management – AWS IAM + Intool policies\n",
       "  To address these challenges, and for large-scale enterprise adoption of our governance capability, we started working on UC integration with our governance processes. Following tech benefits we realized:\n",
       " - Independent of Databricks runtime\n",
       " - Easy fine-grained access control\n",
       " - Eliminated management of IAM roles\n",
       " - Dynamic access control using UC and dynamic views\n",
       " As a result of these technical benefits, we were able to implement fine-grained and complex governance policies around the restricted data set of Amgen including Finance and Workday. At this point we have around 100K objects mapped in the Unity Catalog and growing rapidly.</td><td>Title: Enabling Data Governance at Enterprise Scale Using Unity Catalog\n",
       "                Abstract:  For some time, Amgen has been building multiple enterprise platforms on the latest technology with a key focus on tech rationalization, data democratization, overall user experience, increase reusability and cost-effectiveness. One of these platforms is the enterprise data fabric platform. This platform specifically focuses on pulling in data across functions into a single platform building capabilities around the connectedness of data, and access governance.\n",
       " For a while, we have been trying to setup robust data governance capabilities which are simple, yet easy to manage through Databricks. As part of this process we evaluated products like privacera, Immuta, and Okera. While these tools could solve a few immediate needs like managing tables and database level, for the use cases like maintaining governance on highly restricted data domains like EDW(Finance) and Workday(HR), we felt that for the long term we will require a more Databricks native solution because of below reasons:\n",
       " - There were ways to bypass these tools on databricks cluster\n",
       " - Unsupported DBR runtime\n",
       " - Complexity of fine-grained security\n",
       " - Policy management – AWS IAM + Intool policies\n",
       "  To address these challenges, and for large-scale enterprise adoption of our governance capability, we started working on UC integration with our governance processes. Following tech benefits we realized:\n",
       " - Independent of Databricks runtime\n",
       " - Easy fine-grained access control\n",
       " - Eliminated management of IAM roles\n",
       " - Dynamic access control using UC and dynamic views\n",
       " As a result of these technical benefits, we were able to implement fine-grained and complex governance policies around the restricted data set of Amgen including Finance and Workday. At this point we have around 100K objects mapped in the Unity Catalog and growing rapidly.</td></tr><tr><td>Lineage System Table in Unity Catalog</td><td>Unity Catalog provides fully automated data lineage for all workloads in SQL, R, Python, Scala and across all asset types at Databricks. The aggregated view has been available to end users through data explorer and API. In this session, we are excited to share that lineage is available via delta table in their UC metastore. It stores full history of recent lineage records and it is near real time. Additionally, customers can query it through standard SQL interface. With that, customers can get significant operational insights about their workload for impact analysis, troubleshooting, quality assurance, data discovery, and data governance. Together with the system table platform effort, which provides query history, job run operational data, audit logs and more, lineage table will be a critical piece to link all the data asset and entity asset together. It will provide better Lakehouse observability and unification to customers.</td><td>Title: Lineage System Table in Unity Catalog\n",
       "                Abstract:  Unity Catalog provides fully automated data lineage for all workloads in SQL, R, Python, Scala and across all asset types at Databricks. The aggregated view has been available to end users through data explorer and API. In this session, we are excited to share that lineage is available via delta table in their UC metastore. It stores full history of recent lineage records and it is near real time. Additionally, customers can query it through standard SQL interface. With that, customers can get significant operational insights about their workload for impact analysis, troubleshooting, quality assurance, data discovery, and data governance. Together with the system table platform effort, which provides query history, job run operational data, audit logs and more, lineage table will be a critical piece to link all the data asset and entity asset together. It will provide better Lakehouse observability and unification to customers.</td></tr><tr><td>Combining Privacy Solutions to Solve Data Access at Scale</td><td>The trend that has made data easier to collect and analyze, has only aggravated privacy risks. Luckily, a range of privacy technologies have emerged to enable private data management (differential privacy, synthetic data, confidential computing).  In isolation, those technologies have had a limited impact because they did not always bring the 10x improvement expected by data leaders.\n",
       " \n",
       " Combining these privacy technologies has been the real game changer. We will demonstrate that the right mix of technologies brings the optimal balance of privacy and flexibility at the scale of the data warehouse.  We will illustrate this by real-life applications of Sarus in three domains:\n",
       "Healthcare: how to make hospital data available for research at scale in full compliance\n",
       "Finance: how to pool data between several banks to fight criminal transactions\n",
       "Marketing: how to build insights on combined data from partners and distributors\n",
       " \n",
       " The examples will be illustrated using data stored in Databricks and queried using Sarus differential privacy engine.</td><td>Title: Combining Privacy Solutions to Solve Data Access at Scale\n",
       "                Abstract:  The trend that has made data easier to collect and analyze, has only aggravated privacy risks. Luckily, a range of privacy technologies have emerged to enable private data management (differential privacy, synthetic data, confidential computing).  In isolation, those technologies have had a limited impact because they did not always bring the 10x improvement expected by data leaders.\n",
       " \n",
       " Combining these privacy technologies has been the real game changer. We will demonstrate that the right mix of technologies brings the optimal balance of privacy and flexibility at the scale of the data warehouse.  We will illustrate this by real-life applications of Sarus in three domains:\n",
       "Healthcare: how to make hospital data available for research at scale in full compliance\n",
       "Finance: how to pool data between several banks to fight criminal transactions\n",
       "Marketing: how to build insights on combined data from partners and distributors\n",
       " \n",
       " The examples will be illustrated using data stored in Databricks and queried using Sarus differential privacy engine.</td></tr><tr><td>Map Your Lakehouse Content with DiscoverX</td><td>An enterprise lakehouse contains many different datasets which are related to different sources and might belong to different business units. \n",
       " These datasets can span across hundreds of tables, and each table has a different schema, and those schemas evolve over time. The Cyber security domain is a good example where datasets come from many different source systems and land in the lakehouse.\n",
       " With such a complex dataset ecosystem, answers to simple questions like “Have we ever detected this IP address?” or “Which columns contain IP addresses?” can become impractical and expensive.\n",
       " DiscoverX can automate the discovery of all columns that might contain specific patterns, (e.g. IP addresses, MAC addresses, fully qualified domain names, etc.) and automatically generate search and indexing queries that span across multiple tables and columns.</td><td>Title: Map Your Lakehouse Content with DiscoverX\n",
       "                Abstract:  An enterprise lakehouse contains many different datasets which are related to different sources and might belong to different business units. \n",
       " These datasets can span across hundreds of tables, and each table has a different schema, and those schemas evolve over time. The Cyber security domain is a good example where datasets come from many different source systems and land in the lakehouse.\n",
       " With such a complex dataset ecosystem, answers to simple questions like “Have we ever detected this IP address?” or “Which columns contain IP addresses?” can become impractical and expensive.\n",
       " DiscoverX can automate the discovery of all columns that might contain specific patterns, (e.g. IP addresses, MAC addresses, fully qualified domain names, etc.) and automatically generate search and indexing queries that span across multiple tables and columns.</td></tr><tr><td>Post-Merger: Implementing Unity Catalog Across Multiple Accounts</td><td>Warner Media and Discovery have recently merged to form Warner Bros Discovery. Owning two Databricks accounts and wanting to maintain their separation, our data governance team has successfully implemented Unity Catalog as our data governance solution across both accounts, allowing our teams to collaboratively and securely use the data assets of two organizations. This session is aimed at sharing that success story, including initial challenges, our approach, our architecture, the actual implementation, and user success post-implementation.</td><td>Title: Post-Merger: Implementing Unity Catalog Across Multiple Accounts\n",
       "                Abstract:  Warner Media and Discovery have recently merged to form Warner Bros Discovery. Owning two Databricks accounts and wanting to maintain their separation, our data governance team has successfully implemented Unity Catalog as our data governance solution across both accounts, allowing our teams to collaboratively and securely use the data assets of two organizations. This session is aimed at sharing that success story, including initial challenges, our approach, our architecture, the actual implementation, and user success post-implementation.</td></tr><tr><td>Advanced Governance with Collibra on Databricks</td><td>Define security policies in Collibra that are seamlessly enforced on Databricks</td><td>Title: Advanced Governance with Collibra on Databricks\n",
       "                Abstract:  Define security policies in Collibra that are seamlessly enforced on Databricks</td></tr><tr><td>How Nestle is Leveraging Unity Catalog for Governance At Scale</td><td>Governance with Unity Catalog across multiple regions, markets, teams and environments at Nestle. Establish data isolation at all levels</td><td>Title: How Nestle is Leveraging Unity Catalog for Governance At Scale\n",
       "                Abstract:  Governance with Unity Catalog across multiple regions, markets, teams and environments at Nestle. Establish data isolation at all levels</td></tr><tr><td>Wrangling Your Security Data: Cybersecurity as a Data Management Problem</td><td>Today, the average Fortune 500 company manages 100+ cybersecurity vendors. The sheer number of enterprise cybersecurity toolsets, each capturing and generating its own data, creates significant data management and data engineering challenges for all enterprises. Because each of these security tools has separate logs, outputs, and databases, it prevents any security team from meaningfully understanding their overall security posture and using their own data effectively. We believe that cybersecurity is fundamentally a data management problem. In this presentation, we will show how treating cybersecurity as a data management problem, and subsequently opening disparate cybersecurity data to queries can help organizations secure their systems and data more effectively. In this presentation, practitioners will walk away understanding the value of modeled and organized security on a data warehouse. We will leverage a real-world use case that shows how Databricks can help solve vulnerability management and end-to-end workflows for security practitioners.</td><td>Title: Wrangling Your Security Data: Cybersecurity as a Data Management Problem\n",
       "                Abstract:  Today, the average Fortune 500 company manages 100+ cybersecurity vendors. The sheer number of enterprise cybersecurity toolsets, each capturing and generating its own data, creates significant data management and data engineering challenges for all enterprises. Because each of these security tools has separate logs, outputs, and databases, it prevents any security team from meaningfully understanding their overall security posture and using their own data effectively. We believe that cybersecurity is fundamentally a data management problem. In this presentation, we will show how treating cybersecurity as a data management problem, and subsequently opening disparate cybersecurity data to queries can help organizations secure their systems and data more effectively. In this presentation, practitioners will walk away understanding the value of modeled and organized security on a data warehouse. We will leverage a real-world use case that shows how Databricks can help solve vulnerability management and end-to-end workflows for security practitioners.</td></tr><tr><td>Lakehouse Architecture to Advance Security Analytics at the Department of State</td><td>In 2023, the Department of State surged forward on implementing a Lakehouse architecture to get faster, smarter, and more effective on cybersecurity log monitoring and incident response. In addition to getting us ahead of federal mandates, this approach promises to enable advanced analytics and machine learning across our highly federated global IT environment while minimizing costs associated with data retention and aggregation. \n",
       " \n",
       " This session will include a high-level overview of the technical and policy challenge and a technical deeper dive on the tactical implementation choices made. We’ll share lessons learned related to governance and securing organizational support, connecting between multiple cloud environments, and standardizing data to make it useful for analytics. \n",
       " \n",
       " And finally, we’ll discuss how the Lakehouse leverages Databricks in multi-cloud environments to promote decentralized ownership of data while enabling strong, centralized data governance practices.</td><td>Title: Lakehouse Architecture to Advance Security Analytics at the Department of State\n",
       "                Abstract:  In 2023, the Department of State surged forward on implementing a Lakehouse architecture to get faster, smarter, and more effective on cybersecurity log monitoring and incident response. In addition to getting us ahead of federal mandates, this approach promises to enable advanced analytics and machine learning across our highly federated global IT environment while minimizing costs associated with data retention and aggregation. \n",
       " \n",
       " This session will include a high-level overview of the technical and policy challenge and a technical deeper dive on the tactical implementation choices made. We’ll share lessons learned related to governance and securing organizational support, connecting between multiple cloud environments, and standardizing data to make it useful for analytics. \n",
       " \n",
       " And finally, we’ll discuss how the Lakehouse leverages Databricks in multi-cloud environments to promote decentralized ownership of data while enabling strong, centralized data governance practices.</td></tr><tr><td>Engineering Data Lakehouse Systems for the Next Ten Years of Growth</td><td>For most of my professional life, I've dealt with data. As a data practitioner, I developed algorithms to solve real-world problems leveraging machine learning techniques. As an engineer, I led the direction that brought the value of my hands-on machine learning experience into our products and services by building upon cutting-edge and emerging technologies. This experience has taught me that scaling data systems is harder than you might think. \n",
       "  \n",
       " Supporting the operations of scalable data environments poses a challenge greater than the known application-level support due to the complexity of managing data together with the application. Why is data adding so much complexity? Well, data is big, so all systems are now becoming distributed. Data changes and evolves, and it's hard to create repeatable, automated pipelines. Plus, technology is advancing at an alarming rate and changes are messy.\n",
       "  \n",
       " Interested in learning about all the most recent updates in the data space? How does that impact our data systems? In this talk, you will learn about the challenges of product quality, delivery velocity, production monitoring, and outage recovery, see how those can be met using best practices in the tech stack, and develop empathy for those who manage scalable data environments.</td><td>Title: Engineering Data Lakehouse Systems for the Next Ten Years of Growth\n",
       "                Abstract:  For most of my professional life, I've dealt with data. As a data practitioner, I developed algorithms to solve real-world problems leveraging machine learning techniques. As an engineer, I led the direction that brought the value of my hands-on machine learning experience into our products and services by building upon cutting-edge and emerging technologies. This experience has taught me that scaling data systems is harder than you might think. \n",
       "  \n",
       " Supporting the operations of scalable data environments poses a challenge greater than the known application-level support due to the complexity of managing data together with the application. Why is data adding so much complexity? Well, data is big, so all systems are now becoming distributed. Data changes and evolves, and it's hard to create repeatable, automated pipelines. Plus, technology is advancing at an alarming rate and changes are messy.\n",
       "  \n",
       " Interested in learning about all the most recent updates in the data space? How does that impact our data systems? In this talk, you will learn about the challenges of product quality, delivery velocity, production monitoring, and outage recovery, see how those can be met using best practices in the tech stack, and develop empathy for those who manage scalable data environments.</td></tr><tr><td>Optimize Your Delta Lake</td><td>Delta tables in Databricks just work. They're so easy to setup to deliver value in your organization. What about all those features and optimization that you can use to super charge your delta lakehouse? \n",
       " \n",
       " In this lighting talk we will discuss structuring, partitioning, optimizing, delta log retention of your Delta Tables all in 15 minutes or less!</td><td>Title: Optimize Your Delta Lake\n",
       "                Abstract:  Delta tables in Databricks just work. They're so easy to setup to deliver value in your organization. What about all those features and optimization that you can use to super charge your delta lakehouse? \n",
       " \n",
       " In this lighting talk we will discuss structuring, partitioning, optimizing, delta log retention of your Delta Tables all in 15 minutes or less!</td></tr><tr><td>Lakehouses for Data Engineers: What You Need to Consider to Build Efficient ETL Pipelines</td><td>ETL pipelines are ubiquitous in data-warehousing and Lakehouse ecosystem to make business decisions by building raw and derived tables from a plethora of fragmented data. As companies are investing in building data applications for richer user experiences, ETL pipelines need to solve some of the modern-day challenges of helping deliver low-latency analytics. \n",
       " \n",
       " In this lightning talk, we’ll focus on how to build efficient Apache Spark™  ETL pipelines for Lakehouses where you can effectively ingest and utilize streaming data. Will cover details on:\n",
       " - leveraging incrementally processing framework to improve compute efficiency\n",
       " - index data to deliver low-latency analytics\n",
       " - advanced concurrency control mechanisms to improve throughput</td><td>Title: Lakehouses for Data Engineers: What You Need to Consider to Build Efficient ETL Pipelines\n",
       "                Abstract:  ETL pipelines are ubiquitous in data-warehousing and Lakehouse ecosystem to make business decisions by building raw and derived tables from a plethora of fragmented data. As companies are investing in building data applications for richer user experiences, ETL pipelines need to solve some of the modern-day challenges of helping deliver low-latency analytics. \n",
       " \n",
       " In this lightning talk, we’ll focus on how to build efficient Apache Spark™  ETL pipelines for Lakehouses where you can effectively ingest and utilize streaming data. Will cover details on:\n",
       " - leveraging incrementally processing framework to improve compute efficiency\n",
       " - index data to deliver low-latency analytics\n",
       " - advanced concurrency control mechanisms to improve throughput</td></tr><tr><td>Bringing Data in-House! Behind the New York Jets' Winning Game Plan to Create a Data Lakehouse on Databricks</td><td>On the field, the New York Jets are one of the most recognizable brands in professional sports. Off the field, they’re a small business within a finite market with lots of competition. In order to fuel revenue growth, improve operational efficiencies, and drive fan engagement, the Jets brought its data infrastructure in-house and moved to Databricks. \n",
       " \n",
       " This 40-minute presentation will include a discussion of the New York Jets data journey including its overall business and technical goals, the organization's multiple attempts at building a data stack, what drove the shift to Databricks, its current architectural approach, and the benefits of leveraging Databricks as its cloud data platform. \n",
       " \n",
       " Benefits to data practitioners:\n",
       "  - Learn about the New York Jets data journey and the similarities to other organizations’ data journeys including challenges, build vs. buy, and the decision \n",
       " making processes required to implement a modern data stack\n",
       "  - Discussion around the Data Lakehouse architectural pattern and the steps required to implement this on Databricks\n",
       "  - Identification of the benefits of a customer 360 analysis and process and technologies required to create this record \n",
       "  - Understand the benefits of leveraging Databricks as a cloud data platform \n",
       "  - Highlight future opportunities to leverage Databricks for ML / AI use cases to improve upon current baseline analytics</td><td>Title: Bringing Data in-House! Behind the New York Jets' Winning Game Plan to Create a Data Lakehouse on Databricks\n",
       "                Abstract:  On the field, the New York Jets are one of the most recognizable brands in professional sports. Off the field, they’re a small business within a finite market with lots of competition. In order to fuel revenue growth, improve operational efficiencies, and drive fan engagement, the Jets brought its data infrastructure in-house and moved to Databricks. \n",
       " \n",
       " This 40-minute presentation will include a discussion of the New York Jets data journey including its overall business and technical goals, the organization's multiple attempts at building a data stack, what drove the shift to Databricks, its current architectural approach, and the benefits of leveraging Databricks as its cloud data platform. \n",
       " \n",
       " Benefits to data practitioners:\n",
       "  - Learn about the New York Jets data journey and the similarities to other organizations’ data journeys including challenges, build vs. buy, and the decision \n",
       " making processes required to implement a modern data stack\n",
       "  - Discussion around the Data Lakehouse architectural pattern and the steps required to implement this on Databricks\n",
       "  - Identification of the benefits of a customer 360 analysis and process and technologies required to create this record \n",
       "  - Understand the benefits of leveraging Databricks as a cloud data platform \n",
       "  - Highlight future opportunities to leverage Databricks for ML / AI use cases to improve upon current baseline analytics</td></tr><tr><td>Databricks-As-Code: How to Effectively Automate a Secure Lakehouse Using Terraform for Resource Provisioning</td><td>At Rivian, we have automated >95% of our Databricks resource provisioning workflows using an in-house Terraform module, affording us a lean admin team to manage 750+ users. In this talk, we will cover the following elements of our approach and how others can benefit from improved team efficiency.\n",
       " \n",
       " User and service principal management\n",
       " Our permission model on Unity Catalog for data governance\n",
       " Workspace and secrets resource management\n",
       " Managing internal package dependencies using init scripts\n",
       " Facilitating dashboards, SQL queries and their associated permissions\n",
       " Scaling source of truth Petabyte scale Delta Lake table ingestion jobs and workflows</td><td>Title: Databricks-As-Code: How to Effectively Automate a Secure Lakehouse Using Terraform for Resource Provisioning\n",
       "                Abstract:  At Rivian, we have automated >95% of our Databricks resource provisioning workflows using an in-house Terraform module, affording us a lean admin team to manage 750+ users. In this talk, we will cover the following elements of our approach and how others can benefit from improved team efficiency.\n",
       " \n",
       " User and service principal management\n",
       " Our permission model on Unity Catalog for data governance\n",
       " Workspace and secrets resource management\n",
       " Managing internal package dependencies using init scripts\n",
       " Facilitating dashboards, SQL queries and their associated permissions\n",
       " Scaling source of truth Petabyte scale Delta Lake table ingestion jobs and workflows</td></tr><tr><td>Real-Time Reporting and Analytics for Construction Data Powered by Delta Lake and DBSQL</td><td>Procore is a construction project management software that helps construction professionals efficiently manage their projects and collaborate with their teams. Our mission is to connect everyone in construction on a global platform. \n",
       " \n",
       " Procore is the system of record for all construction projects. Our customers need to access the data in near real-time for construction insights. Enhanced reporting is a self-service operational reporting module that allows quick data access with consistency to thousands of tables and reports.\n",
       " \n",
       " Procore Data platform rebuilt the module (originally built on the relational database) using Databricks and Delta lake. We used Apache Spark™  streaming to maintain the consistent state on the ingestion side from Kafka and plan to leverage the fully capable functionalities of DBSQL using the serverless SQL warehouse to read the medallion models (built via DBT) in Delta Lake. In addition, the Unity Catalog and the Delta share features helped us share the data across regions seamlessly. This design enabled us to improve the p95 and p99 read time by x% (which were initially timing out).\n",
       " \n",
       "  To learn about the learnings and experience of building a Data Lakehouse architecture, attend this talk.</td><td>Title: Real-Time Reporting and Analytics for Construction Data Powered by Delta Lake and DBSQL\n",
       "                Abstract:  Procore is a construction project management software that helps construction professionals efficiently manage their projects and collaborate with their teams. Our mission is to connect everyone in construction on a global platform. \n",
       " \n",
       " Procore is the system of record for all construction projects. Our customers need to access the data in near real-time for construction insights. Enhanced reporting is a self-service operational reporting module that allows quick data access with consistency to thousands of tables and reports.\n",
       " \n",
       " Procore Data platform rebuilt the module (originally built on the relational database) using Databricks and Delta lake. We used Apache Spark™  streaming to maintain the consistent state on the ingestion side from Kafka and plan to leverage the fully capable functionalities of DBSQL using the serverless SQL warehouse to read the medallion models (built via DBT) in Delta Lake. In addition, the Unity Catalog and the Delta share features helped us share the data across regions seamlessly. This design enabled us to improve the p95 and p99 read time by x% (which were initially timing out).\n",
       " \n",
       "  To learn about the learnings and experience of building a Data Lakehouse architecture, attend this talk.</td></tr><tr><td>Made in Italy: How Barilla Uses Databricks Lakehouse to Optimize its Operations</td><td>Barilla Data2Value digital transformation program democratized data usage. The Databricks centered platform provides BI for a supply chain extended in 100 countries in the world, across 20 factories, >50 logistic hubs, 1000 suppliers, >10.000 B2B clients and >100.000 transports every year. By using metadata-driven and event-driven approaches, Barilla is able to meet the different needs of its 1000 data consumers from top management to operation management with about 50 use cases, consuming TB of data daily and execute more than 150 data pipelines. During the presentation, Barilla digital transformation strategy will be discussed, the platform architecture, the expected evolution as well as the lessons learned along the way with EY. Will be explained how Barilla has reduced the time to insight using Unity Catalog and MLFlow. Mariama Kamanda, a data scientist from the Barilla Acceleration Team, will provide testimony on the impact of Databricks Lakehouse daily journey. Will be explained how Barilla saved M$ every year by improving the factory losses through advanced analytics and machine learning leveraging Databricks.</td><td>Title: Made in Italy: How Barilla Uses Databricks Lakehouse to Optimize its Operations\n",
       "                Abstract:  Barilla Data2Value digital transformation program democratized data usage. The Databricks centered platform provides BI for a supply chain extended in 100 countries in the world, across 20 factories, >50 logistic hubs, 1000 suppliers, >10.000 B2B clients and >100.000 transports every year. By using metadata-driven and event-driven approaches, Barilla is able to meet the different needs of its 1000 data consumers from top management to operation management with about 50 use cases, consuming TB of data daily and execute more than 150 data pipelines. During the presentation, Barilla digital transformation strategy will be discussed, the platform architecture, the expected evolution as well as the lessons learned along the way with EY. Will be explained how Barilla has reduced the time to insight using Unity Catalog and MLFlow. Mariama Kamanda, a data scientist from the Barilla Acceleration Team, will provide testimony on the impact of Databricks Lakehouse daily journey. Will be explained how Barilla saved M$ every year by improving the factory losses through advanced analytics and machine learning leveraging Databricks.</td></tr><tr><td>Using Lakehouse to Fight Cancer: Ontada’s Journey to Establish a RWD Platform on Databricks Lakehouse</td><td>Ontada, a McKesson business, is an oncology real-world data and evidence, clinical education and provider of technology business dedicated to transforming the fight against cancer. Core to Ontada’s mission is using real-world data (RWD) and evidence generation to improve patient health outcomes and to accelerate life science research. \n",
       " \n",
       " To support its mission, Ontada embarked on a journey to migrate its enterprise data warehouse (EDW) from an on-premise Oracle database to Databricks Lakehouse. This move allows Ontada to now consume data from any source, including structured and unstructured data from its own EHR and genomics lab results, and realize faster time to insight. In addition, using the Lakehouse has helped Ontada eliminate data silos, enabling the organization to realize the full potential of RWD – from running traditional descriptive analytics to extracting biomarkers from unstructured data. \n",
       " \n",
       " The presentation will cover the following topics: \n",
       " - Oracle to Databricks: migration best practices and lessons learned \n",
       " - People, process, and tools: expediting innovation while protecting patient information using Unity Catalog \n",
       " - Getting the most out of the Databricks Lakehouse: from BI to genomics, running all analytics under one platform \n",
       " - Hyperscale biomarker abstraction: reducing the manual effort needed to extract biomarkers from large unstructured data (medical notes, scanned/faxed documents) using spaCY and John Snow Lab NLP libraries \n",
       " \n",
       " Join this session to hear how Ontada is transforming RWD to deliver safe and effective cancer treatment.</td><td>Title: Using Lakehouse to Fight Cancer: Ontada’s Journey to Establish a RWD Platform on Databricks Lakehouse\n",
       "                Abstract:  Ontada, a McKesson business, is an oncology real-world data and evidence, clinical education and provider of technology business dedicated to transforming the fight against cancer. Core to Ontada’s mission is using real-world data (RWD) and evidence generation to improve patient health outcomes and to accelerate life science research. \n",
       " \n",
       " To support its mission, Ontada embarked on a journey to migrate its enterprise data warehouse (EDW) from an on-premise Oracle database to Databricks Lakehouse. This move allows Ontada to now consume data from any source, including structured and unstructured data from its own EHR and genomics lab results, and realize faster time to insight. In addition, using the Lakehouse has helped Ontada eliminate data silos, enabling the organization to realize the full potential of RWD – from running traditional descriptive analytics to extracting biomarkers from unstructured data. \n",
       " \n",
       " The presentation will cover the following topics: \n",
       " - Oracle to Databricks: migration best practices and lessons learned \n",
       " - People, process, and tools: expediting innovation while protecting patient information using Unity Catalog \n",
       " - Getting the most out of the Databricks Lakehouse: from BI to genomics, running all analytics under one platform \n",
       " - Hyperscale biomarker abstraction: reducing the manual effort needed to extract biomarkers from large unstructured data (medical notes, scanned/faxed documents) using spaCY and John Snow Lab NLP libraries \n",
       " \n",
       " Join this session to hear how Ontada is transforming RWD to deliver safe and effective cancer treatment.</td></tr><tr><td>CPG Lakehouse Analytics: Rapidly Implementing Walmart’s Luminate API at the Hershey Company</td><td>Accurate, reliable, and timely data is critical for CPG companies to stay ahead in highly competitive retailer relationships, and for a company like the Hershey Company, the commercial relationship with Walmart is one of the most important. The team at Hershey found themselves with a looming deadline for their legacy analytics services and targeted a migration to the brand new Walmart Luminate API.\n",
       "  \n",
       " Working in partnership with Advancing Analytics, the Hershey Company leveraged a metadata-driven Lakehouse architecture to rapidly onboard the new Luminate API, helping the category management teams to overhaul how they measure, predict, and plan their business operations.\n",
       "  \n",
       " In this session, we will discuss the impact Luminate has had on Hershey's business covering key areas such as sales, supply chain, and retail field execution, and the technical building blocks that can be used to rapidly provision business users with the data they need, when they need it. We will discuss how key technologies enable this rapid approach, with Databricks Autoloader ingesting and shaping our data, Delta Streaming processing the data through the Lakehouse and Databricks SQL providing a responsive serving layer.\n",
       "  \n",
       " The session will be jointly run by Hersheys and Advancing Analytics, with Jordan Donmoyer, manager of Hershey's direct data team and Dan Davies, director of Walmart sales analytics providing the commercial commentary, and Simon Whiteley and Zach Stagers from Advancing Analytics covering the technical journey.</td><td>Title: CPG Lakehouse Analytics: Rapidly Implementing Walmart’s Luminate API at the Hershey Company\n",
       "                Abstract:  Accurate, reliable, and timely data is critical for CPG companies to stay ahead in highly competitive retailer relationships, and for a company like the Hershey Company, the commercial relationship with Walmart is one of the most important. The team at Hershey found themselves with a looming deadline for their legacy analytics services and targeted a migration to the brand new Walmart Luminate API.\n",
       "  \n",
       " Working in partnership with Advancing Analytics, the Hershey Company leveraged a metadata-driven Lakehouse architecture to rapidly onboard the new Luminate API, helping the category management teams to overhaul how they measure, predict, and plan their business operations.\n",
       "  \n",
       " In this session, we will discuss the impact Luminate has had on Hershey's business covering key areas such as sales, supply chain, and retail field execution, and the technical building blocks that can be used to rapidly provision business users with the data they need, when they need it. We will discuss how key technologies enable this rapid approach, with Databricks Autoloader ingesting and shaping our data, Delta Streaming processing the data through the Lakehouse and Databricks SQL providing a responsive serving layer.\n",
       "  \n",
       " The session will be jointly run by Hersheys and Advancing Analytics, with Jordan Donmoyer, manager of Hershey's direct data team and Dan Davies, director of Walmart sales analytics providing the commercial commentary, and Simon Whiteley and Zach Stagers from Advancing Analytics covering the technical journey.</td></tr><tr><td>Building a Minimalistic Open Lakehouse Using Open Source Projects Apache Spark™, Project Nessie and Iceberg</td><td>A Lakehouse architecture is a combination of various components such as Storage, File format, Table format, and Catalog. What truly makes a lakehouse 'open' is data being stored in open-source table & file formats like Iceberg, Delta & Parquet respectively and the technology being open-sourced for easy & quick adoption by the community. Like any new technology, implementation of a lakehouse may seem daunting at first. However, when we break down the architecture to its open components, this becomes easy to adopt & scale.\n",
       " \n",
       " Though this session, the idea is to help data engineers getting their leg into the world of data lakehouses, easily learn & implement it. We will go through a Notebook-style presentation to show beginners how to build a minimalistic functional lakehouse using Apache Spark, Project Nessie & Iceberg.\n",
       " \n",
       " We will talk about:\n",
       " - configuring the 3 different components\n",
       " - creating tables from raw data files\n",
       " - ingesting new data from various sources into the tables, querying it and making updates\n",
       " - time travel, compaction, etc capabilities</td><td>Title: Building a Minimalistic Open Lakehouse Using Open Source Projects Apache Spark™, Project Nessie and Iceberg\n",
       "                Abstract:  A Lakehouse architecture is a combination of various components such as Storage, File format, Table format, and Catalog. What truly makes a lakehouse 'open' is data being stored in open-source table & file formats like Iceberg, Delta & Parquet respectively and the technology being open-sourced for easy & quick adoption by the community. Like any new technology, implementation of a lakehouse may seem daunting at first. However, when we break down the architecture to its open components, this becomes easy to adopt & scale.\n",
       " \n",
       " Though this session, the idea is to help data engineers getting their leg into the world of data lakehouses, easily learn & implement it. We will go through a Notebook-style presentation to show beginners how to build a minimalistic functional lakehouse using Apache Spark, Project Nessie & Iceberg.\n",
       " \n",
       " We will talk about:\n",
       " - configuring the 3 different components\n",
       " - creating tables from raw data files\n",
       " - ingesting new data from various sources into the tables, querying it and making updates\n",
       " - time travel, compaction, etc capabilities</td></tr><tr><td>Labcorp Data Platform Journey: From Selection to Go-Live in six months</td><td>Please join with us to learn about the Labcorp data platform transformation from on-premises Hadoop to AWS Databricks Lakehouse. We will share best practices and lessons learned from cloud-native data platform selection, implementation, and migration from Hadoop (with in 6 months) with Unity Catalog. We will share steps taken to retire several legacy on-premises technologies and leverage Databricks native features like Spark streaming, workflows, job pools, cluster policies and Spark JDBC within Databricks platform. Lessons learned in Implementing Unity Catalog and building a security and governance model that scales across applications. Demos and walkthrough’s of batch frameworks, streaming frameworks, data compare tools used across several applications to improve data quality and speed of delivery. Discover how we have improved operational efficiency, resiliency and reduced TCO, and how we scaled building workspaces and associated cloud infrastructure using Terraform provider.</td><td>Title: Labcorp Data Platform Journey: From Selection to Go-Live in six months\n",
       "                Abstract:  Please join with us to learn about the Labcorp data platform transformation from on-premises Hadoop to AWS Databricks Lakehouse. We will share best practices and lessons learned from cloud-native data platform selection, implementation, and migration from Hadoop (with in 6 months) with Unity Catalog. We will share steps taken to retire several legacy on-premises technologies and leverage Databricks native features like Spark streaming, workflows, job pools, cluster policies and Spark JDBC within Databricks platform. Lessons learned in Implementing Unity Catalog and building a security and governance model that scales across applications. Demos and walkthrough’s of batch frameworks, streaming frameworks, data compare tools used across several applications to improve data quality and speed of delivery. Discover how we have improved operational efficiency, resiliency and reduced TCO, and how we scaled building workspaces and associated cloud infrastructure using Terraform provider.</td></tr><tr><td>Deep Dive Into Grammarly's Data Platform</td><td>Grammarly helps 30 million people and 50,000 teams to communicate more effectively. Using the Databricks Lakehouse Platform, we are able to rapidly ingest, transform, aggregate, and query complex data sets from an ecosystem of sources, all governed by Unity Catalog. This presentation overviews Grammarly’s data platform and the decisions that shaped the implementation. We will dive deep into some architectural challenges the Grammarly Data Platform team overcame as we developed a self-service framework for incremental event processing.\n",
       " \n",
       " Our investment in the Lakehouse and Unity Catalog has dramatically improved the speed of our data value chain: making 5 billion events (ingested, aggregated, de-identified, and governed) available to stakeholders (data scientists, business analysts, sales, marketing) and downstream services (feature store, reporting/dashboards, customer support, operations) available within 15 minutes. As a result, we have improved our query cost performance (110% faster at 10% the cost) compared to our legacy system on AWS EMR.\n",
       " \n",
       " I will share architecture diagrams, their implications at scale, code samples, and problems solved and to be solved in a technology-focused discussion about Grammarly’s iterative Lakehouse Data Platform.</td><td>Title: Deep Dive Into Grammarly's Data Platform\n",
       "                Abstract:  Grammarly helps 30 million people and 50,000 teams to communicate more effectively. Using the Databricks Lakehouse Platform, we are able to rapidly ingest, transform, aggregate, and query complex data sets from an ecosystem of sources, all governed by Unity Catalog. This presentation overviews Grammarly’s data platform and the decisions that shaped the implementation. We will dive deep into some architectural challenges the Grammarly Data Platform team overcame as we developed a self-service framework for incremental event processing.\n",
       " \n",
       " Our investment in the Lakehouse and Unity Catalog has dramatically improved the speed of our data value chain: making 5 billion events (ingested, aggregated, de-identified, and governed) available to stakeholders (data scientists, business analysts, sales, marketing) and downstream services (feature store, reporting/dashboards, customer support, operations) available within 15 minutes. As a result, we have improved our query cost performance (110% faster at 10% the cost) compared to our legacy system on AWS EMR.\n",
       " \n",
       " I will share architecture diagrams, their implications at scale, code samples, and problems solved and to be solved in a technology-focused discussion about Grammarly’s iterative Lakehouse Data Platform.</td></tr><tr><td>Simplifying Migrations to Lakehouse</td><td>Following on from Perenti Global's successful presentation at the ANZ (Syd) Data and AI summit in Nov 2022, they were asked by Bede Hackney if they would be interested in supporting Databricks events on a bigger stage, ramping up their reference-ability etc... they are confirmed as able to support us at this event and are excited to do so. \n",
       " Link to client presentation noting tweaks to be made upon confirmation of attendance --> https://docs.google.com/presentation/d/1OHMsyFhV-gq6lcVvXqkp-0LY8wS6C6TS/edit#slide=id.p1\n",
       " \n",
       " \n",
       " High level Synopsis:\n",
       " - Challenges with Legacy Platforms \n",
       " - Perenti Databricks Migration Journey\n",
       " - Reimagining Migrations the Databricks Way\n",
       " - The Databricks Migration Methodology & Approach</td><td>Title: Simplifying Migrations to Lakehouse\n",
       "                Abstract:  Following on from Perenti Global's successful presentation at the ANZ (Syd) Data and AI summit in Nov 2022, they were asked by Bede Hackney if they would be interested in supporting Databricks events on a bigger stage, ramping up their reference-ability etc... they are confirmed as able to support us at this event and are excited to do so. \n",
       " Link to client presentation noting tweaks to be made upon confirmation of attendance --> https://docs.google.com/presentation/d/1OHMsyFhV-gq6lcVvXqkp-0LY8wS6C6TS/edit#slide=id.p1\n",
       " \n",
       " \n",
       " High level Synopsis:\n",
       " - Challenges with Legacy Platforms \n",
       " - Perenti Databricks Migration Journey\n",
       " - Reimagining Migrations the Databricks Way\n",
       " - The Databricks Migration Methodology & Approach</td></tr><tr><td>Having Your Cake and Eating it Too: How Vizio Built a Next-Generation ACR Data Platform While Lowering TCO</td><td>As the top manufacturer of smart TVs, Vizio uses TV data to drive its business and provide customers with best digital experiences. Our company's mission is to continually improve the viewing experience for our customers, which is why we developed our award-winning automatic content recognition (ACR) platform. When we first built our data platform almost ten years ago, there was no single platform to run a data as a service business, so we got creative and built our own by stitching together different AWS services and a data warehouse. As our business needs and data volumes have grown exponentially over the years, we made the strategic decision to replatform on Databricks Lakehouse, as it was the only platform that could satisfy all our needs out-of-the-box such as BI analytics, real-time streaming, and AI/ML. Now the Lakehouse is our sole source of truth for all analytics and machine learning projects. The technical value of the Databricks Lakehouse platform, such as traditional data warehousing low-latency query processing with complex joins thanks to Photon to using Apache Spark™  structured streaming; analytics and model serving, will be covered in this session as we talk about our path to the Lakehouse.</td><td>Title: Having Your Cake and Eating it Too: How Vizio Built a Next-Generation ACR Data Platform While Lowering TCO\n",
       "                Abstract:  As the top manufacturer of smart TVs, Vizio uses TV data to drive its business and provide customers with best digital experiences. Our company's mission is to continually improve the viewing experience for our customers, which is why we developed our award-winning automatic content recognition (ACR) platform. When we first built our data platform almost ten years ago, there was no single platform to run a data as a service business, so we got creative and built our own by stitching together different AWS services and a data warehouse. As our business needs and data volumes have grown exponentially over the years, we made the strategic decision to replatform on Databricks Lakehouse, as it was the only platform that could satisfy all our needs out-of-the-box such as BI analytics, real-time streaming, and AI/ML. Now the Lakehouse is our sole source of truth for all analytics and machine learning projects. The technical value of the Databricks Lakehouse platform, such as traditional data warehousing low-latency query processing with complex joins thanks to Photon to using Apache Spark™  structured streaming; analytics and model serving, will be covered in this session as we talk about our path to the Lakehouse.</td></tr><tr><td>From Insights to Recommendations: How SkyWatch Predicts Demand for Satellite Imagery Using Databricks</td><td>SkyWatch is on a mission to democratize earth observation data and make it simple for anyone to use.\n",
       " \n",
       " In this session, you will learn about how SkyWatch aggregates demand signals for EO market and turns them into monetizable recommendations for satellite operators.\n",
       " Skywatch’s head of data engineering, Vincent, and data engineer, Aayush, will share how the team built a serverless architecture that synthesizes customer requests for satellite images and identifies geographic locations with high demand, helping satellite operators maximize revenue and satisfying a broad range of EO data hungry consumers.\n",
       " \n",
       " This session will cover the usage of\n",
       " \n",
       " - Databricks in-built H3 functions \n",
       " - Open Source Mosaic library to process geospatial data\n",
       " - Delta Lake to efficiently store data leveraging optimization techniques like Z-Ordering\n",
       " - Databricks Serverless SQL endpoint to build REST API with AWS Lambda and step functions.\n",
       " - Databricks with AWS serverless architecture (Lambda and step functions)</td><td>Title: From Insights to Recommendations: How SkyWatch Predicts Demand for Satellite Imagery Using Databricks\n",
       "                Abstract:  SkyWatch is on a mission to democratize earth observation data and make it simple for anyone to use.\n",
       " \n",
       " In this session, you will learn about how SkyWatch aggregates demand signals for EO market and turns them into monetizable recommendations for satellite operators.\n",
       " Skywatch’s head of data engineering, Vincent, and data engineer, Aayush, will share how the team built a serverless architecture that synthesizes customer requests for satellite images and identifies geographic locations with high demand, helping satellite operators maximize revenue and satisfying a broad range of EO data hungry consumers.\n",
       " \n",
       " This session will cover the usage of\n",
       " \n",
       " - Databricks in-built H3 functions \n",
       " - Open Source Mosaic library to process geospatial data\n",
       " - Delta Lake to efficiently store data leveraging optimization techniques like Z-Ordering\n",
       " - Databricks Serverless SQL endpoint to build REST API with AWS Lambda and step functions.\n",
       " - Databricks with AWS serverless architecture (Lambda and step functions)</td></tr><tr><td>Determining When to Use GPU for Your ETL Pipelines at Scale</td><td>Assuming you have hundreds of jobs and/or clusters in your Databricks workspace, what is the best way to determine if those pipelines can take advantage of GPU for speed and/or cost saving? We are presenting NVIDIA GPU Qualification Tool applied at scale to project potential cost saving for your entire workspace.</td><td>Title: Determining When to Use GPU for Your ETL Pipelines at Scale\n",
       "                Abstract:  Assuming you have hundreds of jobs and/or clusters in your Databricks workspace, what is the best way to determine if those pipelines can take advantage of GPU for speed and/or cost saving? We are presenting NVIDIA GPU Qualification Tool applied at scale to project potential cost saving for your entire workspace.</td></tr><tr><td>Massive Data Processing in Adobe Using Delta Lake:  A Year In</td><td>At Adobe Experience Platform, we ingest TBs of data everyday and manage PBs of data for our customers as part of the unified profile offering. At the heart of this is a bunch of complex ingestion of a mix of normalized and denormalized data with various linkage scenarios power by a central identity linking graph. This helps power various marketing scenarios that are activated in multiple platforms and channels like email, advertisements etc. We will go over how we built a cost effective and scalable data pipeline using Apache Spark™  and Delta Lake and share our experiences after one year in production.\n",
       " \n",
       " - What are we storing?\n",
       " - Multi-source, multi-channel problem\n",
       " - Access pattern to optimize for\n",
       " - Custom high performance query engine\n",
       " - Data representation and nested schema evolution\n",
       " - Performance trade-offs with various formats\n",
       " - Go over anti-patterns used \n",
       " - String FTW\n",
       " - Data manipulation using UDFs \n",
       " - Writer worries and how to wipe them away\n",
       "  - Locking for contention mechanism\n",
       " - Gotchas\n",
       " - Concurrency\n",
       " - Column size\n",
       " - Update frequency\n",
       " - Transaction management for a healthy state\n",
       " - Staging tables FTW \n",
       " - Why we can't live without them\n",
       " - Datalake replication lag tracking\n",
       " - Instrumentation of the data pipeline gives more confidence to the reade\n",
       " - Maintenance jobs\n",
       " - Go over essentials of compaction and vacuuming\n",
       " - Performance time!\n",
       " - What scale are we operating at?\n",
       " - Settings like autoCompact and optimizeWrite\n",
       " - Timings with and without delta\n",
       " - Cost \n",
       "  - Lesson learned and burnt</td><td>Title: Massive Data Processing in Adobe Using Delta Lake:  A Year In\n",
       "                Abstract:  At Adobe Experience Platform, we ingest TBs of data everyday and manage PBs of data for our customers as part of the unified profile offering. At the heart of this is a bunch of complex ingestion of a mix of normalized and denormalized data with various linkage scenarios power by a central identity linking graph. This helps power various marketing scenarios that are activated in multiple platforms and channels like email, advertisements etc. We will go over how we built a cost effective and scalable data pipeline using Apache Spark™  and Delta Lake and share our experiences after one year in production.\n",
       " \n",
       " - What are we storing?\n",
       " - Multi-source, multi-channel problem\n",
       " - Access pattern to optimize for\n",
       " - Custom high performance query engine\n",
       " - Data representation and nested schema evolution\n",
       " - Performance trade-offs with various formats\n",
       " - Go over anti-patterns used \n",
       " - String FTW\n",
       " - Data manipulation using UDFs \n",
       " - Writer worries and how to wipe them away\n",
       "  - Locking for contention mechanism\n",
       " - Gotchas\n",
       " - Concurrency\n",
       " - Column size\n",
       " - Update frequency\n",
       " - Transaction management for a healthy state\n",
       " - Staging tables FTW \n",
       " - Why we can't live without them\n",
       " - Datalake replication lag tracking\n",
       " - Instrumentation of the data pipeline gives more confidence to the reade\n",
       " - Maintenance jobs\n",
       " - Go over essentials of compaction and vacuuming\n",
       " - Performance time!\n",
       " - What scale are we operating at?\n",
       " - Settings like autoCompact and optimizeWrite\n",
       " - Timings with and without delta\n",
       " - Cost \n",
       "  - Lesson learned and burnt</td></tr><tr><td>DHL eCommerce US – Building a scalable and robust Cloud Data Platform as enabler for Enterprise Analytics</td><td>DHL eCommerce Solutions Americas is a high volume B2C domestic and international parcel delivery business and that is on mission to become a data-driven organization to by democratizing the use of data and to maximizing the business value.\n",
       "  \n",
       " Learn how the DHL eCommerce Solutions Americas data team built a centralized Enterprise grade scalable cloud data platform processing 100s of millions of events daily. This single source of truth powering all analytics use cases helps DHL ecommerce US team to deliver business insights 5x faster.\n",
       " We will dive into how we solved challenging key requirements including secure multi-tenant central data platform, scalable data ingestion and processing, cost effective big data repository, fast and regular data refreshes, diverse analytics workload needs.\n",
       " \n",
       " Finally we will share some example usecases operated on this platform and the business value those usecases are generating along various lines of the business including operations, commercial, customer service, sales, product management, etc.</td><td>Title: DHL eCommerce US – Building a scalable and robust Cloud Data Platform as enabler for Enterprise Analytics\n",
       "                Abstract:  DHL eCommerce Solutions Americas is a high volume B2C domestic and international parcel delivery business and that is on mission to become a data-driven organization to by democratizing the use of data and to maximizing the business value.\n",
       "  \n",
       " Learn how the DHL eCommerce Solutions Americas data team built a centralized Enterprise grade scalable cloud data platform processing 100s of millions of events daily. This single source of truth powering all analytics use cases helps DHL ecommerce US team to deliver business insights 5x faster.\n",
       " We will dive into how we solved challenging key requirements including secure multi-tenant central data platform, scalable data ingestion and processing, cost effective big data repository, fast and regular data refreshes, diverse analytics workload needs.\n",
       " \n",
       " Finally we will share some example usecases operated on this platform and the business value those usecases are generating along various lines of the business including operations, commercial, customer service, sales, product management, etc.</td></tr><tr><td>Learnings From the Field: Migration From Oracle DW and IBM DataStage to Databricks on AWS</td><td>Legacy data warehouses are costly to maintain, unscalable and cannot deliver on data science, ML and real-time analytics use cases. Migrating from your enterprise data warehouse to Databricks lets you scale as your business needs grow and accelerate innovation by running all your data, analytics and AI workloads on a single unified data platform.\n",
       " \n",
       " In the first part of this talk we will guide you through the well-designed process and tools that will help you from the assessment phase to the actual implementation of an EDW migration project. Also, we will address ways to convert PL/SQL proprietary code to an open standard python code and take advantage of PySpark for ETL workloads and Databricks SQL’s data analytics workload power.\n",
       " \n",
       " The second part of this talk will be based on an EDW migration project of SNCF (French national railways); one of the major enterprise customers of Databricks in France. Databricks partnered with SNCF to migrate its real estate entity from Oracle DW and IBM DataStage to Databricks on AWS. We will walk you through the customer context, urgency to migration, challenges, target architecture, nitty-gritty details of implementation, best practices, recommendations, and learnings in order to execute a successful migration project in a very accelerated time frame.</td><td>Title: Learnings From the Field: Migration From Oracle DW and IBM DataStage to Databricks on AWS\n",
       "                Abstract:  Legacy data warehouses are costly to maintain, unscalable and cannot deliver on data science, ML and real-time analytics use cases. Migrating from your enterprise data warehouse to Databricks lets you scale as your business needs grow and accelerate innovation by running all your data, analytics and AI workloads on a single unified data platform.\n",
       " \n",
       " In the first part of this talk we will guide you through the well-designed process and tools that will help you from the assessment phase to the actual implementation of an EDW migration project. Also, we will address ways to convert PL/SQL proprietary code to an open standard python code and take advantage of PySpark for ETL workloads and Databricks SQL’s data analytics workload power.\n",
       " \n",
       " The second part of this talk will be based on an EDW migration project of SNCF (French national railways); one of the major enterprise customers of Databricks in France. Databricks partnered with SNCF to migrate its real estate entity from Oracle DW and IBM DataStage to Databricks on AWS. We will walk you through the customer context, urgency to migration, challenges, target architecture, nitty-gritty details of implementation, best practices, recommendations, and learnings in order to execute a successful migration project in a very accelerated time frame.</td></tr><tr><td>Why a Major Japanese Financial Institution Chose Databricks to Accelerate its Data and AI-Driven Journey</td><td>In this session, we will introduce a case study of migrating the Japanese largest data analysis platform to Databricks.\n",
       " \n",
       "NTT DATA is one of the largest system integrators in Japan. In the Japanese market, many companies are working on BI, and we are now in the phase of using AI. Our team provides solutions that provide data analytics infrastructure to drive the democratization of data and AI for leading Japanese companies.\n",
       " \n",
       " The customer in this case study is the largest insurance company in Japan. This project has the following characteristics:\n",
       "  - As a financial institution, security requirements are very strict.\n",
       "  - Since it is used company-wide, including group companies, it is necessary to support various use cases.\n",
       " \n",
       " We started operating a data analysis platform on AWS in 2017. Over the next five years, we leveraged AWS-managed services such as Amazon EMR, Amazon Athena, and Amazon Sage Maker to modernize our architecture.\n",
       " \n",
       " In the near future, in order to promote the use cases of AI as well as BI, we have begun to consider upgrading to a platform that realizes both BI and AI.\n",
       " \n",
       " This session will cover:\n",
       "  - Challenges in developing AI on a DWH-based data analysis platform and why a data lake house is the best choice\n",
       "  - Examining the architecture of a platform that supports both AI and BI use cases. In this case study, we will introduce the results of a comparative study of a proposal based on Databricks, a proposal based on Snowflake, and a proposal combining Snowflake and Databricks.\n",
       " \n",
       " This session is recommended for those who want to accelerate their business by utilizing AI as well as BI.\n",
       " \n",
       " \n",
       "</td><td>Title: Why a Major Japanese Financial Institution Chose Databricks to Accelerate its Data and AI-Driven Journey\n",
       "                Abstract:  In this session, we will introduce a case study of migrating the Japanese largest data analysis platform to Databricks.\n",
       " \n",
       "NTT DATA is one of the largest system integrators in Japan. In the Japanese market, many companies are working on BI, and we are now in the phase of using AI. Our team provides solutions that provide data analytics infrastructure to drive the democratization of data and AI for leading Japanese companies.\n",
       " \n",
       " The customer in this case study is the largest insurance company in Japan. This project has the following characteristics:\n",
       "  - As a financial institution, security requirements are very strict.\n",
       "  - Since it is used company-wide, including group companies, it is necessary to support various use cases.\n",
       " \n",
       " We started operating a data analysis platform on AWS in 2017. Over the next five years, we leveraged AWS-managed services such as Amazon EMR, Amazon Athena, and Amazon Sage Maker to modernize our architecture.\n",
       " \n",
       " In the near future, in order to promote the use cases of AI as well as BI, we have begun to consider upgrading to a platform that realizes both BI and AI.\n",
       " \n",
       " This session will cover:\n",
       "  - Challenges in developing AI on a DWH-based data analysis platform and why a data lake house is the best choice\n",
       "  - Examining the architecture of a platform that supports both AI and BI use cases. In this case study, we will introduce the results of a comparative study of a proposal based on Databricks, a proposal based on Snowflake, and a proposal combining Snowflake and Databricks.\n",
       " \n",
       " This session is recommended for those who want to accelerate their business by utilizing AI as well as BI.</td></tr><tr><td>Eliminating Shuffles in Delete Update, and Merge</td><td>If you’ve ever had to delete a set of records for regulatory compliance, update a set of records to fix an issue in the ingestion pipeline, or apply changes in a transaction log to a fact table, you know that row-level operations are becoming critical for modern data lake workflows. Even though the industry has seen a tremendous amount of innovation in this area, row-level operations can be still fairly expensive if the underlying data has to be shuffled. This talk will explain how Apache Spark can completely avoid shuffles during row-level operations by leveraging storage-partitioned joins, a key to efficiently modify data at PB scale.</td><td>Title: Eliminating Shuffles in Delete Update, and Merge\n",
       "                Abstract:  If you’ve ever had to delete a set of records for regulatory compliance, update a set of records to fix an issue in the ingestion pipeline, or apply changes in a transaction log to a fact table, you know that row-level operations are becoming critical for modern data lake workflows. Even though the industry has seen a tremendous amount of innovation in this area, row-level operations can be still fairly expensive if the underlying data has to be shuffled. This talk will explain how Apache Spark can completely avoid shuffles during row-level operations by leveraging storage-partitioned joins, a key to efficiently modify data at PB scale.</td></tr><tr><td>When Self-Service Meets Earnings Calls: A Journey Towards FinOps</td><td>At Disney Streaming, we extensively use Databricks and the delta lake as an operational and analytical tool. But as we shift focus from growth to profit, our data platform needed to pivot from self-service and ease use and towards automatic tracking, auditing, and continuous efficiency chasing. \n",
       " \n",
       " In this talk, I'll explain our FinOps journey; how we democratized cost data, increased transparency, tightened policies, and got dozens of analytical and engineering teams to fit cost savings into their roadmap.\n",
       "</td><td>Title: When Self-Service Meets Earnings Calls: A Journey Towards FinOps\n",
       "                Abstract:  At Disney Streaming, we extensively use Databricks and the delta lake as an operational and analytical tool. But as we shift focus from growth to profit, our data platform needed to pivot from self-service and ease use and towards automatic tracking, auditing, and continuous efficiency chasing. \n",
       " \n",
       " In this talk, I'll explain our FinOps journey; how we democratized cost data, increased transparency, tightened policies, and got dozens of analytical and engineering teams to fit cost savings into their roadmap.</td></tr><tr><td>Delta Lake Migration At Scale</td><td>Adobe Experience Platform (AEP) serves a large number Adobe digital experience customers with tens of thousands of datasets in non-Delta format. To leverage Delta Lake, we migrate existing tables to Delta format after resolving the following challenges: 1). Short downtime to minimize customer impact; 2). Adding new system metadata; 3). Migration automation for scalability; and 4). Error detection and catastrophic rollback. This talk will cover the solutions to the above challenges and lessons learned from migration. Finally, scalable migration service becomes a core capability of the data platform.</td><td>Title: Delta Lake Migration At Scale\n",
       "                Abstract:  Adobe Experience Platform (AEP) serves a large number Adobe digital experience customers with tens of thousands of datasets in non-Delta format. To leverage Delta Lake, we migrate existing tables to Delta format after resolving the following challenges: 1). Short downtime to minimize customer impact; 2). Adding new system metadata; 3). Migration automation for scalability; and 4). Error detection and catastrophic rollback. This talk will cover the solutions to the above challenges and lessons learned from migration. Finally, scalable migration service becomes a core capability of the data platform.</td></tr><tr><td>Databricks Cost Management</td><td>With Databricks you only pay for the compute resources you use. With growing usage it is advisable to have a cost management strategy to avoid surprises at the end of the month. It is also useful to monitor and break down costs for budgeting purposes or to calculate the return on investment (ROI) for specific projects.\n",
       " \n",
       " In this talk I will:\n",
       " - Describe the Databricks pricing model\n",
       " - Show how to analyze and break down costs\n",
       " - Discuss best practises for cost optimization</td><td>Title: Databricks Cost Management\n",
       "                Abstract:  With Databricks you only pay for the compute resources you use. With growing usage it is advisable to have a cost management strategy to avoid surprises at the end of the month. It is also useful to monitor and break down costs for budgeting purposes or to calculate the return on investment (ROI) for specific projects.\n",
       " \n",
       " In this talk I will:\n",
       " - Describe the Databricks pricing model\n",
       " - Show how to analyze and break down costs\n",
       " - Discuss best practises for cost optimization</td></tr><tr><td>Data Asset Bundles: A Standard, Unified Approach to Deploying Data Products on Databricks</td><td>In this talk we will introduce data asset bundles, provide a demonstration of how they work for a variety of data products, and how to fit them into an overall CICD strategy for the well-architected Lakehouse.\n",
       " \n",
       " Data teams produce a variety of assets; datasets, reports and dashboards, ML models, and business applications. These assets depend upon code (notebooks, repos, queries, pipelines), infrastructure (clusters, SQL warehouses, serverless endpoints), and supporting services/resources like Unity Catalog, Databricks Workflows, and DBSQL dashboards. Today, each organization must figure out a deployment strategy for the variety of data products they build on Databricks as there is no consistent way to describe the infrastructure and services associated with project code.\n",
       " \n",
       " Data asset bundles is a new capability on Databricks that standardizes and unifies the deployment strategy for all data products developed on the platform. It allows developers to describe the infrastructure and resources of their project through a YAML or JSON configuration file, regardless of whether they are producing a report, dashboard, online ML model, or Delta Live Tables pipeline. Behind the scenes, these configuration files use Terraform to manage resources in a Databricks workspace, but knowledge of Terraform is not required to use Data Asset Bundles.</td><td>Title: Data Asset Bundles: A Standard, Unified Approach to Deploying Data Products on Databricks\n",
       "                Abstract:  In this talk we will introduce data asset bundles, provide a demonstration of how they work for a variety of data products, and how to fit them into an overall CICD strategy for the well-architected Lakehouse.\n",
       " \n",
       " Data teams produce a variety of assets; datasets, reports and dashboards, ML models, and business applications. These assets depend upon code (notebooks, repos, queries, pipelines), infrastructure (clusters, SQL warehouses, serverless endpoints), and supporting services/resources like Unity Catalog, Databricks Workflows, and DBSQL dashboards. Today, each organization must figure out a deployment strategy for the variety of data products they build on Databricks as there is no consistent way to describe the infrastructure and services associated with project code.\n",
       " \n",
       " Data asset bundles is a new capability on Databricks that standardizes and unifies the deployment strategy for all data products developed on the platform. It allows developers to describe the infrastructure and resources of their project through a YAML or JSON configuration file, regardless of whether they are producing a report, dashboard, online ML model, or Delta Live Tables pipeline. Behind the scenes, these configuration files use Terraform to manage resources in a Databricks workspace, but knowledge of Terraform is not required to use Data Asset Bundles.</td></tr><tr><td>Lakehouses: The Best Start to Your Graph Data and Analytics Journey</td><td>Data architects and IT executives are continually looking for the best ways to integrate graph data and analytics into their organizations to improve business outcomes. This presentation outlines how the Data Lakehouse provides the perfect starting point for a successful journey. We will explore how the Data Lakehouse offer the unique combination of scalability, flexibility, and speed to quickly and effectively ingest, pre-process, curate, and analyze graph data to create powerful analytics. Additionally, this talk will discuss the benefits of using the Data Lakehouse over traditional graph databases and how it can help improve time to insight, time to production and overall satisfaction. \n",
       " \n",
       "At the end of this presentation, attendees will: \n",
       " - Understand the benefits of using a Data Lakehouse for graph data and analytics \n",
       " - Learn how to get started with a successful Lakehouse implementation (demo)\n",
       " - Discover the advantages of using a Data Lakehouse over graph databases\n",
       " - Learn specifically where graph databases integrate and perform better together\n",
       " \n",
       " Key Takeaways: \n",
       " - Data Lakehouses provide the perfect starting point for a successful graph data and analytics journey \n",
       " - Data Lakehouses offer scalability, flexibility, and speed to quickly and effectively analyze graph data \n",
       " - The Data Lakehouse is a cost-effective alternative to traditional graph database shortening your time to insight and de-risk your project.\n",
       " \n",
       " Presentation Components:\n",
       " The customer journey\n",
       " Access patterns\n",
       " Reference architecture (logical, physical)\n",
       " Customer stories\n",
       " Demo (notebook + live graph visualization)</td><td>Title: Lakehouses: The Best Start to Your Graph Data and Analytics Journey\n",
       "                Abstract:  Data architects and IT executives are continually looking for the best ways to integrate graph data and analytics into their organizations to improve business outcomes. This presentation outlines how the Data Lakehouse provides the perfect starting point for a successful journey. We will explore how the Data Lakehouse offer the unique combination of scalability, flexibility, and speed to quickly and effectively ingest, pre-process, curate, and analyze graph data to create powerful analytics. Additionally, this talk will discuss the benefits of using the Data Lakehouse over traditional graph databases and how it can help improve time to insight, time to production and overall satisfaction. \n",
       " \n",
       "At the end of this presentation, attendees will: \n",
       " - Understand the benefits of using a Data Lakehouse for graph data and analytics \n",
       " - Learn how to get started with a successful Lakehouse implementation (demo)\n",
       " - Discover the advantages of using a Data Lakehouse over graph databases\n",
       " - Learn specifically where graph databases integrate and perform better together\n",
       " \n",
       " Key Takeaways: \n",
       " - Data Lakehouses provide the perfect starting point for a successful graph data and analytics journey \n",
       " - Data Lakehouses offer scalability, flexibility, and speed to quickly and effectively analyze graph data \n",
       " - The Data Lakehouse is a cost-effective alternative to traditional graph database shortening your time to insight and de-risk your project.\n",
       " \n",
       " Presentation Components:\n",
       " The customer journey\n",
       " Access patterns\n",
       " Reference architecture (logical, physical)\n",
       " Customer stories\n",
       " Demo (notebook + live graph visualization)</td></tr><tr><td>Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse</td><td>As you embark on a lakehouse project or evolve your existing data lake, you may want to improve your security posture and take advantage of new security features—there may even be a security team at your company that demands it! Databricks has worked with thousands of customers to securely deploy the Databricks Platform to meet their architecture and security requirements. While many organizations deploy security differently, we have found a common set of guidelines and features among organizations that require a high level of security. In this session, we will detail the security features and architectural choices frequently used by these organizations and walk through a series of threat models for the risks that most concern security teams. While this session is great for people who already know Databricks—don’t worry—that knowledge isn’t required. \n",
       "You will walk away with a full handbook detailing all of the concepts, configurations, check lists, security analysis tool (SAT), and security reference architecture (SRA) automation scripts  from the session so that you can make immediate progress when you get back to the office. \\\n",
       "Security can be hard, but we’ve collected the hard work already done by some of the best in the industry, and built tools, to make it easier. Come learn how. See how good looks like via a demo.\n",
       "</td><td>Title: Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse\n",
       "                Abstract:  As you embark on a lakehouse project or evolve your existing data lake, you may want to improve your security posture and take advantage of new security features—there may even be a security team at your company that demands it! Databricks has worked with thousands of customers to securely deploy the Databricks Platform to meet their architecture and security requirements. While many organizations deploy security differently, we have found a common set of guidelines and features among organizations that require a high level of security. In this session, we will detail the security features and architectural choices frequently used by these organizations and walk through a series of threat models for the risks that most concern security teams. While this session is great for people who already know Databricks—don’t worry—that knowledge isn’t required. \n",
       "You will walk away with a full handbook detailing all of the concepts, configurations, check lists, security analysis tool (SAT), and security reference architecture (SRA) automation scripts  from the session so that you can make immediate progress when you get back to the office. \\\n",
       "Security can be hard, but we’ve collected the hard work already done by some of the best in the industry, and built tools, to make it easier. Come learn how. See how good looks like via a demo.</td></tr><tr><td>Data Globalization at Conde Nast Using Delta Sharing</td><td>Databricks has been an essential part of the Conde Nast architecture for the last few years.\n",
       " \n",
       " Prior to building our centralized data platform, “evergreen”, we had similar challenges as many other organizations; siloed data, duplicated efforts for engineers, and a lack of collaboration between data teams. These problems led to mistrust in data sets and made it difficult to scale to meet the strategic globalization plan we had for Conde Nast.\n",
       " \n",
       " Over the last few years we have been extremely successful in building a centralized data platform on Databricks in AWS, fully embracing the lakehouse vision from end-to-end. Now, our analysts and marketers can derive the same insights from one dataset and data scientists can use the same datasets for use cases such as personalization, subscriber propensity models, churn models and on-site recommendations for our iconic brands.\n",
       " \n",
       " In this talk, we’ll discuss how we plan to incorporate Unity Catalog and Delta Sharing as the next phase of our globalization mission. The evergreen platform has become the global standard for data processing and analytics at Conde. In order to manage the worldwide data and comply with GDPR requirements, we need to make sure data is processed in the appropriate region and PII data is handled appropriately. At the same time, we need to have a global view of the data to allow us to make business decisions at the global level. We’ll talk about how delta sharing allows us a simple, secure way to share de-identified datasets across regions in order to make these strategic business decisions, while complying with security requirements. Additionally, we’ll discuss how Unity Catalog allows us to secure, govern and audit these datasets in an easy and scalable manner.</td><td>Title: Data Globalization at Conde Nast Using Delta Sharing\n",
       "                Abstract:  Databricks has been an essential part of the Conde Nast architecture for the last few years.\n",
       " \n",
       " Prior to building our centralized data platform, “evergreen”, we had similar challenges as many other organizations; siloed data, duplicated efforts for engineers, and a lack of collaboration between data teams. These problems led to mistrust in data sets and made it difficult to scale to meet the strategic globalization plan we had for Conde Nast.\n",
       " \n",
       " Over the last few years we have been extremely successful in building a centralized data platform on Databricks in AWS, fully embracing the lakehouse vision from end-to-end. Now, our analysts and marketers can derive the same insights from one dataset and data scientists can use the same datasets for use cases such as personalization, subscriber propensity models, churn models and on-site recommendations for our iconic brands.\n",
       " \n",
       " In this talk, we’ll discuss how we plan to incorporate Unity Catalog and Delta Sharing as the next phase of our globalization mission. The evergreen platform has become the global standard for data processing and analytics at Conde. In order to manage the worldwide data and comply with GDPR requirements, we need to make sure data is processed in the appropriate region and PII data is handled appropriately. At the same time, we need to have a global view of the data to allow us to make business decisions at the global level. We’ll talk about how delta sharing allows us a simple, secure way to share de-identified datasets across regions in order to make these strategic business decisions, while complying with security requirements. Additionally, we’ll discuss how Unity Catalog allows us to secure, govern and audit these datasets in an easy and scalable manner.</td></tr><tr><td>Unlocking the Value of Data Sharing in Financial Services With Lakehouse</td><td>The emergence of secure data sharing is already having a tremendous economic impact, in large part due to the increasing ease and safety of sharing financial data. McKinsey predicts that the impact of open financial data will be 1-4.5% of GDP globally by 2030. This indicates there is a narrowing window on a massive opportunity for financial institutions and it is critical that they prioritize data sharing. This session will first address the ways in which Delta Sharing and Unity Catalog on a Databricks Lakehouse architecture provides a simple and open framework for building a Secure Data Sharing platform in the financial services industry. Next we will use a Databricks environment to walk through different use cases for open banking data and secure data sharing, demonstrating how they will be implemented using Delta Sharing, Unity Catalog, and other parts of the Lakehouse platform. The use cases will include examples of new product features such as Databricks to Databricks sharing, change data feed and streaming on Delta Sharing, table/column lineage, and the Delta Sharing Excel plugin to demonstrate state of the art sharing capabilities.\n",
       " Spencer Cook, a Sr. Solutions Architect at Databricks focussed on the Financial Services industry will discuss secure data sharing on Databricks Lakehouse and will demonstrate architecture and code for common sharing use cases in the finance industry.</td><td>Title: Unlocking the Value of Data Sharing in Financial Services With Lakehouse\n",
       "                Abstract:  The emergence of secure data sharing is already having a tremendous economic impact, in large part due to the increasing ease and safety of sharing financial data. McKinsey predicts that the impact of open financial data will be 1-4.5% of GDP globally by 2030. This indicates there is a narrowing window on a massive opportunity for financial institutions and it is critical that they prioritize data sharing. This session will first address the ways in which Delta Sharing and Unity Catalog on a Databricks Lakehouse architecture provides a simple and open framework for building a Secure Data Sharing platform in the financial services industry. Next we will use a Databricks environment to walk through different use cases for open banking data and secure data sharing, demonstrating how they will be implemented using Delta Sharing, Unity Catalog, and other parts of the Lakehouse platform. The use cases will include examples of new product features such as Databricks to Databricks sharing, change data feed and streaming on Delta Sharing, table/column lineage, and the Delta Sharing Excel plugin to demonstrate state of the art sharing capabilities.\n",
       " Spencer Cook, a Sr. Solutions Architect at Databricks focussed on the Financial Services industry will discuss secure data sharing on Databricks Lakehouse and will demonstrate architecture and code for common sharing use cases in the finance industry.</td></tr><tr><td>Extending Lakehouse Architecture with Collaborative Identity</td><td>Lakehouse architecture has become a valuable solution for unifying data processing for AI, but faces limitations in maximizing data’s full potential. Additional data infrastructure is helpful for strengthening data consolidation and data connectivity with third-party sources, which are necessary for building full data sets for accurate audience modeling.\n",
       "\n",
       "In this session, LiveRamp will demonstrate to data and analytics decision-makers how to build on the Lakehouse architecture with extensions for collaborative identity graph construction, including how to simplify and improve data enrichment, data activation and data collaboration. LiveRamp will also introduce a complete data marketplace, which enables easy, pseudonymized data enhancements that widen the attribute set for better behavioral model construction.\n",
       "\n",
       "With these techniques and technologies, enterprises across financial services, retail, media, travel and more can safely unlock partner insights and ultimately produce more accurate inputs for personalization engines, and more engaging offers and recommendations for customers.\n",
       "</td><td>Title: Extending Lakehouse Architecture with Collaborative Identity\n",
       "                Abstract:  Lakehouse architecture has become a valuable solution for unifying data processing for AI, but faces limitations in maximizing data’s full potential. Additional data infrastructure is helpful for strengthening data consolidation and data connectivity with third-party sources, which are necessary for building full data sets for accurate audience modeling.\n",
       "\n",
       "In this session, LiveRamp will demonstrate to data and analytics decision-makers how to build on the Lakehouse architecture with extensions for collaborative identity graph construction, including how to simplify and improve data enrichment, data activation and data collaboration. LiveRamp will also introduce a complete data marketplace, which enables easy, pseudonymized data enhancements that widen the attribute set for better behavioral model construction.\n",
       "\n",
       "With these techniques and technologies, enterprises across financial services, retail, media, travel and more can safely unlock partner insights and ultimately produce more accurate inputs for personalization engines, and more engaging offers and recommendations for customers.</td></tr><tr><td>Writing Data-Sharing Apps Using Node.js and Delta Sharing</td><td>Javascript remains the top programming language today, with most code repositories written using JavaScript on GitHub. However, JavaScript is evolving beyond just a language for web application development into a language built for tomorrow. Everyday tasks like data wrangling, data analysis, and predictive analytics are possible today directly from a web browser. For example, many popular data analytics libraries, like Tensorflow.js, now support JavaScript SDKs. Another popular library, Danfo.js, makes it possible to wrangle data using familiar Pandas-like operations, shortening the learning curve and arming the typical data engineer or data scientist with another data tool in their toolbox. In this presentation, we’ll explore using the Node.js connector for Delta Sharing to build a data analytics app that summarizes a Twitter dataset.</td><td>Title: Writing Data-Sharing Apps Using Node.js and Delta Sharing\n",
       "                Abstract:  Javascript remains the top programming language today, with most code repositories written using JavaScript on GitHub. However, JavaScript is evolving beyond just a language for web application development into a language built for tomorrow. Everyday tasks like data wrangling, data analysis, and predictive analytics are possible today directly from a web browser. For example, many popular data analytics libraries, like Tensorflow.js, now support JavaScript SDKs. Another popular library, Danfo.js, makes it possible to wrangle data using familiar Pandas-like operations, shortening the learning curve and arming the typical data engineer or data scientist with another data tool in their toolbox. In this presentation, we’ll explore using the Node.js connector for Delta Sharing to build a data analytics app that summarizes a Twitter dataset.</td></tr><tr><td>Creating the First Sports and Entertainment Data Marketplace Powered by Pumpjack Dataworks, Revelate, Immuta and Databricks</td><td>Creating a secure and easily actionable marketplace is no simple task. Add to this governance requirements of privacy frameworks and responsibilities of protecting consumer data, and things get harder. With Pumpjack Dataworks partnering with Databricks, Immuta, and Revelate, we bring secure, privacy-focused data products directly to data consumers. With Delta Share, the service solves redundancy of data purchases, time to action, and lowers cost of data acquisition while providing new revenue streams for rights holders. Its a win-win solution for bringing Data Acquisition and Commercial teams together into the new age of data.</td><td>Title: Creating the First Sports and Entertainment Data Marketplace Powered by Pumpjack Dataworks, Revelate, Immuta and Databricks\n",
       "                Abstract:  Creating a secure and easily actionable marketplace is no simple task. Add to this governance requirements of privacy frameworks and responsibilities of protecting consumer data, and things get harder. With Pumpjack Dataworks partnering with Databricks, Immuta, and Revelate, we bring secure, privacy-focused data products directly to data consumers. With Delta Share, the service solves redundancy of data purchases, time to action, and lowers cost of data acquisition while providing new revenue streams for rights holders. Its a win-win solution for bringing Data Acquisition and Commercial teams together into the new age of data.</td></tr><tr><td>Data Interoperability Across Clouds and Regions</td><td>L’Oréal has the largest and richest data repository in the beauty industry, with a 110-year heritage solely dedicated to analyzing, measuring and magnifying the beauty needs and desires of consumers around the world. As L’Oréal’s ambition is to become the leader in beauty tech, a profound IT transformation started three years ago to meet their most strategic priorities: \n",
       " - Hyper personalization\n",
       " - Consumer experience online and offline \n",
       " - Preventive care\n",
       " - Integrating artificial intelligence into the business\n",
       " \n",
       " This IT transformation now needs to reach all of their services and teams worldwide with the same level of security, quality and exacting standards. To do so, data must be unlocked and democratized across the world in order to meet their strategic priorities. This is why L’Oréal and Databricks worked together on the data interoperability value proposition in order to :\n",
       " - Facilitate multi-cloud interoperability exchanges thanks to open standards technologies\n",
       " - Secure and trace through a centralized governance tool to respond to the growing regulatory environment and the application of other data laws (such as in Vietnam)\n",
       " - Optimize and rationalize data exchanges to reduce transported volumes and meet L'Oréal's sustainable development objectives\n",
       " - Ensure the quality of the data exchanged through quality and validation checks</td><td>Title: Data Interoperability Across Clouds and Regions\n",
       "                Abstract:  L’Oréal has the largest and richest data repository in the beauty industry, with a 110-year heritage solely dedicated to analyzing, measuring and magnifying the beauty needs and desires of consumers around the world. As L’Oréal’s ambition is to become the leader in beauty tech, a profound IT transformation started three years ago to meet their most strategic priorities: \n",
       " - Hyper personalization\n",
       " - Consumer experience online and offline \n",
       " - Preventive care\n",
       " - Integrating artificial intelligence into the business\n",
       " \n",
       " This IT transformation now needs to reach all of their services and teams worldwide with the same level of security, quality and exacting standards. To do so, data must be unlocked and democratized across the world in order to meet their strategic priorities. This is why L’Oréal and Databricks worked together on the data interoperability value proposition in order to :\n",
       " - Facilitate multi-cloud interoperability exchanges thanks to open standards technologies\n",
       " - Secure and trace through a centralized governance tool to respond to the growing regulatory environment and the application of other data laws (such as in Vietnam)\n",
       " - Optimize and rationalize data exchanges to reduce transported volumes and meet L'Oréal's sustainable development objectives\n",
       " - Ensure the quality of the data exchanged through quality and validation checks</td></tr><tr><td>Data Extraction and Sharing Via the Delta Sharing Protocol: Overfetching, Underfetching and Other Lessons and Tips for Development Learned While Building the Delta Sharing Excel Add-in</td><td>The Delta Sharing open protocol for secure sharing and distribution of Lakehouse data is designed to reduce friction in getting data to users. Delivering custom data solutions from this protocol further leverages the technical investment committed to your Delta Lake infrastructure.\n",
       " \n",
       " There are key design and computational concepts unique to Delta Sharing to know when undertaking development. And there are pitfalls and hazards to avoid when delivering modern cloud data to traditional data platforms and users.\n",
       " \n",
       " In this session we introduce Delta Sharing Protocol development and examine our journey and the lessons learned while creating the Delta Sharing Excel Add-in. We will demonstrate scenarios of overfetching, underfetching, and interpretation of types. We will suggest methods to overcome these development challenges.\n",
       " \n",
       " The session will combine live demonstrations that exercise the Delta Sharing REST protocol with detailed analysis of the responses. The demonstrations will elaborate on optional capabilities of the protocol’s query mechanism, and how they are used and interpreted in real-life scenarios.\n",
       " \n",
       " As a reference baseline for data professionals, the Delta Sharing exercises will be framed relative to SQL counterparts. Specific attention will be paid to how they differ, and how Delta Sharing’s Change Data Feed (CDF) can power next-generation data architectures.\n",
       " \n",
       " The session will conclude with a survey of available integration solutions for getting the most out of your Delta Sharing environment, including frameworks, connectors, and managed services.\n",
       " \n",
       " Attendees are encouraged to be familiar with REST, JSON, and modern programming concepts. A working knowledge of Delta Lake, the Parquet file format, and the Delta Sharing Protocol are advised.</td><td>Title: Data Extraction and Sharing Via the Delta Sharing Protocol: Overfetching, Underfetching and Other Lessons and Tips for Development Learned While Building the Delta Sharing Excel Add-in\n",
       "                Abstract:  The Delta Sharing open protocol for secure sharing and distribution of Lakehouse data is designed to reduce friction in getting data to users. Delivering custom data solutions from this protocol further leverages the technical investment committed to your Delta Lake infrastructure.\n",
       " \n",
       " There are key design and computational concepts unique to Delta Sharing to know when undertaking development. And there are pitfalls and hazards to avoid when delivering modern cloud data to traditional data platforms and users.\n",
       " \n",
       " In this session we introduce Delta Sharing Protocol development and examine our journey and the lessons learned while creating the Delta Sharing Excel Add-in. We will demonstrate scenarios of overfetching, underfetching, and interpretation of types. We will suggest methods to overcome these development challenges.\n",
       " \n",
       " The session will combine live demonstrations that exercise the Delta Sharing REST protocol with detailed analysis of the responses. The demonstrations will elaborate on optional capabilities of the protocol’s query mechanism, and how they are used and interpreted in real-life scenarios.\n",
       " \n",
       " As a reference baseline for data professionals, the Delta Sharing exercises will be framed relative to SQL counterparts. Specific attention will be paid to how they differ, and how Delta Sharing’s Change Data Feed (CDF) can power next-generation data architectures.\n",
       " \n",
       " The session will conclude with a survey of available integration solutions for getting the most out of your Delta Sharing environment, including frameworks, connectors, and managed services.\n",
       " \n",
       " Attendees are encouraged to be familiar with REST, JSON, and modern programming concepts. A working knowledge of Delta Lake, the Parquet file format, and the Delta Sharing Protocol are advised.</td></tr><tr><td>Data sharing & beyond with Delta Sharing</td><td>Stepping into this brave new digital world we are certain that data will be a central product for many organizations. The way to convey their knowledge and their assets will be through data and analytics. Delta Sharing was the world's first open protocol for secure and scalable real-time data sharing. Through our customer conversations, there is a lot of anticipation of how Delta Sharing can be extended to non-tabular assets, such as machine learning experiments and models.\n",
       " \n",
       " In this talk, we will cover how we extended the Delta Sharing protocol to other sharing workflows, enabling sharing of ML models, arbitrary files and more. The development resulted in Arcuate, a Databricks Labs project with a data sharing flavour. The talk will start with the high-level approach and how it can be extended to cover other similar use cases. It will then move to our implementation and how it integrates seamlessly with Databricks-managed Delta Sharing server and notebooks. We finally conclude with lessons learned, and our visions for a future of data sharing & beyond</td><td>Title: Data sharing & beyond with Delta Sharing\n",
       "                Abstract:  Stepping into this brave new digital world we are certain that data will be a central product for many organizations. The way to convey their knowledge and their assets will be through data and analytics. Delta Sharing was the world's first open protocol for secure and scalable real-time data sharing. Through our customer conversations, there is a lot of anticipation of how Delta Sharing can be extended to non-tabular assets, such as machine learning experiments and models.\n",
       " \n",
       " In this talk, we will cover how we extended the Delta Sharing protocol to other sharing workflows, enabling sharing of ML models, arbitrary files and more. The development resulted in Arcuate, a Databricks Labs project with a data sharing flavour. The talk will start with the high-level approach and how it can be extended to cover other similar use cases. It will then move to our implementation and how it integrates seamlessly with Databricks-managed Delta Sharing server and notebooks. We finally conclude with lessons learned, and our visions for a future of data sharing & beyond</td></tr><tr><td>Ad Measurement: From Impressions to Attribution</td><td>\"Effectv, the advertising sales division of Comcast, delivers linear and digital advertising to help advertisers reach potential customers. In this talk, Joe Walsh, Director of Attribution & Measurement and Derek Sugden, Ad Measurement Lead, will discuss their team's journey of measuring the impact of advertising campaigns on customer behavior by combining Comcast's household-level exposure data with various types of third-party conversion data sourced externally. They will highlight the challenges of sharing and receiving data securely while preserving customer privacy, and share how they have implemented an internal \"\"clean room\"\" concept to restrict roles and usage of a cluster</td><td>Title: Ad Measurement: From Impressions to Attribution\n",
       "                Abstract:  \"Effectv, the advertising sales division of Comcast, delivers linear and digital advertising to help advertisers reach potential customers. In this talk, Joe Walsh, Director of Attribution & Measurement and Derek Sugden, Ad Measurement Lead, will discuss their team's journey of measuring the impact of advertising campaigns on customer behavior by combining Comcast's household-level exposure data with various types of third-party conversion data sourced externally. They will highlight the challenges of sharing and receiving data securely while preserving customer privacy, and share how they have implemented an internal \"\"clean room\"\" concept to restrict roles and usage of a cluster</td></tr><tr><td>Clean Room Primer: Using Clean Rooms on Databricks to Utilize More and Better Data in your BI, ML, and Beyond</td><td>In this session, Habu’s Chief Product Officer, Matt Karasick and Senior Architect, Anil Puliyeril, will discuss the foundational changes in the ecosystem, the implications of data insights on marketing, analytics, and measurement, and how companies are coming together to collaborate through data clean rooms in new and exciting ways to power mutually beneficial value for their businesses while preserving privacy and governance.\n",
       " \n",
       " We will delve into the concept and key features of clean room technology and how they can be used to access more and better data for business intelligence (BI), machine learning (ML), and other data-driven initiatives. By examining real-world use cases of clean rooms in action, attendees will gain a clear understanding of the benefits they can bring to industries like CPG, retail, and media & entertainment. \n",
       " \n",
       " In addition, we will unpack the advantages of using Databricks as a clean room platform, specifically showcasing how interoperable clean rooms can be leveraged to enhance BI, ML and other compute scenarios. By the end of this talk, you will be equipped with the knowledge and inspiration to explore how clean rooms can unlock new collaboration opportunities that drive better outcomes for your business and improved experiences for consumers.</td><td>Title: Clean Room Primer: Using Clean Rooms on Databricks to Utilize More and Better Data in your BI, ML, and Beyond\n",
       "                Abstract:  In this session, Habu’s Chief Product Officer, Matt Karasick and Senior Architect, Anil Puliyeril, will discuss the foundational changes in the ecosystem, the implications of data insights on marketing, analytics, and measurement, and how companies are coming together to collaborate through data clean rooms in new and exciting ways to power mutually beneficial value for their businesses while preserving privacy and governance.\n",
       " \n",
       " We will delve into the concept and key features of clean room technology and how they can be used to access more and better data for business intelligence (BI), machine learning (ML), and other data-driven initiatives. By examining real-world use cases of clean rooms in action, attendees will gain a clear understanding of the benefits they can bring to industries like CPG, retail, and media & entertainment. \n",
       " \n",
       " In addition, we will unpack the advantages of using Databricks as a clean room platform, specifically showcasing how interoperable clean rooms can be leveraged to enhance BI, ML and other compute scenarios. By the end of this talk, you will be equipped with the knowledge and inspiration to explore how clean rooms can unlock new collaboration opportunities that drive better outcomes for your business and improved experiences for consumers.</td></tr><tr><td>Embrace First-Party Data Collaboration to Lower Acquisition Costs with Lookalike Audiences in Media Cleanrooms</td><td>Third-party tracking is dead! Yet customer acquisition costs continue to rise to meet the demands of customer attention. Lookalike audience modeling leveraging first-party customer data has offered organizations a cost-effective means to identifying and engaging new customers at scale. However, as we move into a privacy-centric era, collaboration with first-party data across organizations becomes increasingly challenging. So how can we work together to meet both needs? The answer is data clean rooms for the lakehouse, fueled by first-party customer data to build lookalike audience profiles and drive highly personalized ads and experiences in media and commerce.\n",
       " \n",
       " Using Snowplow's generation of granular first-party customer behavioral data with governance metadata and consent tracking, managed through data clean rooms in the lakehouse, organizations can collaborate securely across their first-party datasets. For example, brands and advertisers can join their first-party onsite data with third-party datasets shared by their agency and advertising partners to boost CTR and ROAS with lookalike audiences.\n",
       " \n",
       "This session you focus on:\n",
       " - Best practices for creating lookalike audiences with first-party customer data to lower acquisition costs\n",
       " - See a live demonstration of how audiences derived through the Databricks Lakehouse can be securely exchanged with customers and partners to foster data-driven innovations creating a customer behavioral profile using Snowplow \n",
       " - Data engineering and preparation of Snowplow Behavioral Data with DBT and Databricks\n",
       " - Development and training of an ML model using PySpark on Databricks to create a model to drive contextual advertising\n",
       " - Implementation of the model for scoring\n",
       " - Engage in Q&A</td><td>Title: Embrace First-Party Data Collaboration to Lower Acquisition Costs with Lookalike Audiences in Media Cleanrooms\n",
       "                Abstract:  Third-party tracking is dead! Yet customer acquisition costs continue to rise to meet the demands of customer attention. Lookalike audience modeling leveraging first-party customer data has offered organizations a cost-effective means to identifying and engaging new customers at scale. However, as we move into a privacy-centric era, collaboration with first-party data across organizations becomes increasingly challenging. So how can we work together to meet both needs? The answer is data clean rooms for the lakehouse, fueled by first-party customer data to build lookalike audience profiles and drive highly personalized ads and experiences in media and commerce.\n",
       " \n",
       " Using Snowplow's generation of granular first-party customer behavioral data with governance metadata and consent tracking, managed through data clean rooms in the lakehouse, organizations can collaborate securely across their first-party datasets. For example, brands and advertisers can join their first-party onsite data with third-party datasets shared by their agency and advertising partners to boost CTR and ROAS with lookalike audiences.\n",
       " \n",
       "This session you focus on:\n",
       " - Best practices for creating lookalike audiences with first-party customer data to lower acquisition costs\n",
       " - See a live demonstration of how audiences derived through the Databricks Lakehouse can be securely exchanged with customers and partners to foster data-driven innovations creating a customer behavioral profile using Snowplow \n",
       " - Data engineering and preparation of Snowplow Behavioral Data with DBT and Databricks\n",
       " - Development and training of an ML model using PySpark on Databricks to create a model to drive contextual advertising\n",
       " - Implementation of the model for scoring\n",
       " - Engage in Q&A</td></tr><tr><td>null</td><td>S&P merged with IHS Markit unlocking massive market opportunities, but it also created organizational and technology challenges that made it difficult to centralize all their data and make it readily available for analytics and AI. This impacted cross-team collaboration and limited their ability to provide innovative solutions for their clients looking to make smarter, sustainable investment choices. There were infrastructure complexities and they significantly increased data volumes that limited their ability to democratize data and insights. Their legacy data warehouse also struggled with unifying alternative data sources with other data sets. This impacted various use cases across the business including credit risk, ESG, and Marketplace Workbench. The Sustainable1 team has spearheaded getting all their datasets readily available for sharing across all of their internal divisions via Delta Share, and also to share S&P data externally with their customers. </td><td>Title: None\n",
       "                Abstract:  S&P merged with IHS Markit unlocking massive market opportunities, but it also created organizational and technology challenges that made it difficult to centralize all their data and make it readily available for analytics and AI. This impacted cross-team collaboration and limited their ability to provide innovative solutions for their clients looking to make smarter, sustainable investment choices. There were infrastructure complexities and they significantly increased data volumes that limited their ability to democratize data and insights. Their legacy data warehouse also struggled with unifying alternative data sources with other data sets. This impacted various use cases across the business including credit risk, ESG, and Marketplace Workbench. The Sustainable1 team has spearheaded getting all their datasets readily available for sharing across all of their internal divisions via Delta Share, and also to share S&P data externally with their customers.</td></tr><tr><td>How to Create and Manage a High-Performance Analytics Team</td><td>Data Science and analytics teams are unique. Large and small corporations want to build and manage analytics teams to convert their data and analytic assets into revenue and competitive advantage, but many are failing before they make their first hire. In this presentation, the audience will learn how to structure, hire, manage and grow an analytics team. Organizational structure, project and program portfolios, neurodiversity, developing talent, and more will be discussed. Questions and discussion will be encouraged and engaged in. The audience will leave with a deeper understanding of how to succeed in turning data and analytics into tangible results.</td><td>Title: How to Create and Manage a High-Performance Analytics Team\n",
       "                Abstract:  Data Science and analytics teams are unique. Large and small corporations want to build and manage analytics teams to convert their data and analytic assets into revenue and competitive advantage, but many are failing before they make their first hire. In this presentation, the audience will learn how to structure, hire, manage and grow an analytics team. Organizational structure, project and program portfolios, neurodiversity, developing talent, and more will be discussed. Questions and discussion will be encouraged and engaged in. The audience will leave with a deeper understanding of how to succeed in turning data and analytics into tangible results.</td></tr><tr><td>What it Takes to Democratize AI and ML in a Large Company: The Importance of User Enablement and Technical Training</td><td>The biggest critical factor to success in a Cloud transformation is people. As such, having a change management process in place to manage the impact of the transformation and user enablement is foundational to any large program. In this session we will dive into how TD bank democratizes data, mobilizes our 2000+ analytics user community and the tactics we used to successfully enable new use cases on Cloud. The session will focus on\n",
       " \n",
       " To democratize data:\n",
       " • Centralize a data platform that is accessible to all employees and allow for easy data sharing \n",
       " • Implement privacy and security to protect data and use data ethically\n",
       " • Compliance and governance for using data in responsible and compliant way\n",
       " • Simplification of processes and procedures to reduce redundancy and faster adoption\n",
       "\n",
       " To mobilize end users:\n",
       " • Increase data literacy: provide training and resources for employees to increase their abilities and skills\n",
       " • Foster a culture of collaborationand openness: cross-functional teams to collaborate and share ideas\n",
       " • Encourage exploration of innovative ideas that impact the organization's values and customers\n",
       " \n",
       "Technical enablement and adoption tactics we've used at TD Bank:\n",
       " • Hands-on training for over 1300+ analytics users with emphasis on learn by doing, to relate to real-life situations\n",
       " • Online tutorials and documentations to be used as self-paced study \n",
       " • Workshops and office hours on specific topics to empower business users\n",
       " • Coaching to work with teams on a specific use case/complex issue and provide recommendations for a faster, cost effective solutions\n",
       " • Offer certification and encourage continuous education for employees to keep up to date with latest \n",
       " • Feedback loop: get user feedback on training and user experience to improve future trainings</td><td>Title: What it Takes to Democratize AI and ML in a Large Company: The Importance of User Enablement and Technical Training\n",
       "                Abstract:  The biggest critical factor to success in a Cloud transformation is people. As such, having a change management process in place to manage the impact of the transformation and user enablement is foundational to any large program. In this session we will dive into how TD bank democratizes data, mobilizes our 2000+ analytics user community and the tactics we used to successfully enable new use cases on Cloud. The session will focus on\n",
       " \n",
       " To democratize data:\n",
       " • Centralize a data platform that is accessible to all employees and allow for easy data sharing \n",
       " • Implement privacy and security to protect data and use data ethically\n",
       " • Compliance and governance for using data in responsible and compliant way\n",
       " • Simplification of processes and procedures to reduce redundancy and faster adoption\n",
       "\n",
       " To mobilize end users:\n",
       " • Increase data literacy: provide training and resources for employees to increase their abilities and skills\n",
       " • Foster a culture of collaborationand openness: cross-functional teams to collaborate and share ideas\n",
       " • Encourage exploration of innovative ideas that impact the organization's values and customers\n",
       " \n",
       "Technical enablement and adoption tactics we've used at TD Bank:\n",
       " • Hands-on training for over 1300+ analytics users with emphasis on learn by doing, to relate to real-life situations\n",
       " • Online tutorials and documentations to be used as self-paced study \n",
       " • Workshops and office hours on specific topics to empower business users\n",
       " • Coaching to work with teams on a specific use case/complex issue and provide recommendations for a faster, cost effective solutions\n",
       " • Offer certification and encourage continuous education for employees to keep up to date with latest \n",
       " • Feedback loop: get user feedback on training and user experience to improve future trainings</td></tr><tr><td>Weaving the Data Mesh in the Department of Defense</td><td>The Chief Digital and AI Office (CDAO) was created to lead the strategy and policy on data, analytics, and AI adoption across the Department of Defense. To enable that vision, the Department must achieve new ways to scale and standardize delivery under a global strategy while enabling decentralized workflows that capture the wealth of data and domain expertise. \n",
       " \n",
       " CDAO’s strategy and goals are aligned with data mesh principles. This alignment starts with providing enterprise-level infrastructure and services to advance the adoption of data, analytics, and AI, creating the self-service data infrastructure as a platform. And it continues through implementing policy for federated computational governance centered around decentralizing data ownership to become domain-oriented but enforcing the quality and trustworthiness of data. CDAO seeks to expand and make enterprise data more accessible through providing data as a product and leveraging a federated data catalog to designate authoritative data and common data models. This results in domain-oriented, decentralized data ownership to empower the business domains across the Department to increase mission and business impact that result in significant cost savings, saving lives, and data serving as a “public good.”\n",
       " \n",
       " Please join us in our presentation as we discuss how the CDAO leverages modern, innovative implementations that accelerate the delivery of data and AI throughout one of the largest distributed organizations in the world; the Department of Defence.  We will walk through how this enables delivery in various Department of Defence use cases.</td><td>Title: Weaving the Data Mesh in the Department of Defense\n",
       "                Abstract:  The Chief Digital and AI Office (CDAO) was created to lead the strategy and policy on data, analytics, and AI adoption across the Department of Defense. To enable that vision, the Department must achieve new ways to scale and standardize delivery under a global strategy while enabling decentralized workflows that capture the wealth of data and domain expertise. \n",
       " \n",
       " CDAO’s strategy and goals are aligned with data mesh principles. This alignment starts with providing enterprise-level infrastructure and services to advance the adoption of data, analytics, and AI, creating the self-service data infrastructure as a platform. And it continues through implementing policy for federated computational governance centered around decentralizing data ownership to become domain-oriented but enforcing the quality and trustworthiness of data. CDAO seeks to expand and make enterprise data more accessible through providing data as a product and leveraging a federated data catalog to designate authoritative data and common data models. This results in domain-oriented, decentralized data ownership to empower the business domains across the Department to increase mission and business impact that result in significant cost savings, saving lives, and data serving as a “public good.”\n",
       " \n",
       " Please join us in our presentation as we discuss how the CDAO leverages modern, innovative implementations that accelerate the delivery of data and AI throughout one of the largest distributed organizations in the world; the Department of Defence.  We will walk through how this enables delivery in various Department of Defence use cases.</td></tr><tr><td>The Modern Composable CX Stack: Deconstructing the Data Lifecycle From Infrastructure to Orchestration</td><td> In today’s privacy-conscious world, it’s imperative for IT to have ownership and full control of sensitive customer data. But more than ever, business stakeholders are in dogged pursuit of customer data that can drive better customer understanding and more effective campaigns. For IT, maintaining the integrity of data and architecture while driving customer 360 and other customer experience initiatives can become costly, time consuming and a source of friction in the organization.\n",
       " \n",
       " Hear from ActionIQ’s SVP of Product Justin DeBrabant and Northwestern Mutual’s Chief Data Officer Don Vu as they discuss how Northwestern Mutual is leveraging Databricks and ActionIQ’s CX Hub to bring together business and technical teams around existing data and architecture investments while enabling powerful CX initiatives.\n",
       " \n",
       " You’ll learn:\n",
       " - How to maintain IT choice, agility and control across the stack with composable technology \n",
       " - Strategies to optimize and showcase existing data investments of IT while elevating the stack \n",
       " - Tactics to drive business impact from data management, governance and modernization initiatives</td><td>Title: The Modern Composable CX Stack: Deconstructing the Data Lifecycle From Infrastructure to Orchestration\n",
       "                Abstract:   In today’s privacy-conscious world, it’s imperative for IT to have ownership and full control of sensitive customer data. But more than ever, business stakeholders are in dogged pursuit of customer data that can drive better customer understanding and more effective campaigns. For IT, maintaining the integrity of data and architecture while driving customer 360 and other customer experience initiatives can become costly, time consuming and a source of friction in the organization.\n",
       " \n",
       " Hear from ActionIQ’s SVP of Product Justin DeBrabant and Northwestern Mutual’s Chief Data Officer Don Vu as they discuss how Northwestern Mutual is leveraging Databricks and ActionIQ’s CX Hub to bring together business and technical teams around existing data and architecture investments while enabling powerful CX initiatives.\n",
       " \n",
       " You’ll learn:\n",
       " - How to maintain IT choice, agility and control across the stack with composable technology \n",
       " - Strategies to optimize and showcase existing data investments of IT while elevating the stack \n",
       " - Tactics to drive business impact from data management, governance and modernization initiatives</td></tr><tr><td>Leveraging Product Thinking to Scale a Data Platform to a 400-person (Or More!) Data & Analytics Team</td><td>There exists a large and complex set of big data and analytics tools. How do you decide which tools to use? When to use them? In what way? And how to ensure availability and productivity? In this session we'll talk about how we organized the data platform team so that we could provide the best data environment for the 400+ engineers and data scientists at Anheuser-Busch InBev. \n",
       " \n",
       " Data platforms are generally a set of tools that enable data analysis activities. They are built on many different technologies and processes. How to guarantee reliability and constant evolutions for our users was the problem we started to face during the first months after the launch of our Data Platform. To solve this issue, we used an agile mindset and product management framework, already consolidated in software management teams. As a result, we divided the data platform team into squads, each one having its own OKRs and backlog aimed at improving user productivity. \n",
       " \n",
       " Having well-defined responsibilities for each squad according to the stages of the data journey within the platform made it possible to establish clear priorities for evolution in terms of benefits generated for users and the company. This also allowed us to have a highly specialized operations team to each part of the architecture, as each squad is responsible for keeping its product up and running. \n",
       " \n",
       " In this session, we will show the evolution of the team's structure over three years, as well as the challenges encountered. In addition, we will show how the daily work of the squads is organized.</td><td>Title: Leveraging Product Thinking to Scale a Data Platform to a 400-person (Or More!) Data & Analytics Team\n",
       "                Abstract:  There exists a large and complex set of big data and analytics tools. How do you decide which tools to use? When to use them? In what way? And how to ensure availability and productivity? In this session we'll talk about how we organized the data platform team so that we could provide the best data environment for the 400+ engineers and data scientists at Anheuser-Busch InBev. \n",
       " \n",
       " Data platforms are generally a set of tools that enable data analysis activities. They are built on many different technologies and processes. How to guarantee reliability and constant evolutions for our users was the problem we started to face during the first months after the launch of our Data Platform. To solve this issue, we used an agile mindset and product management framework, already consolidated in software management teams. As a result, we divided the data platform team into squads, each one having its own OKRs and backlog aimed at improving user productivity. \n",
       " \n",
       " Having well-defined responsibilities for each squad according to the stages of the data journey within the platform made it possible to establish clear priorities for evolution in terms of benefits generated for users and the company. This also allowed us to have a highly specialized operations team to each part of the architecture, as each squad is responsible for keeping its product up and running. \n",
       " \n",
       " In this session, we will show the evolution of the team's structure over three years, as well as the challenges encountered. In addition, we will show how the daily work of the squads is organized.</td></tr><tr><td>Experience the New Era of Data & AI: Taking Bold Steps</td><td>Being an organization operating in >65 countries, PETRONAS has a complex and  growing data landscape across its value chain. It is undoubted that digital transformation is a necessity; however, its success is underpinned by data being a strategic enabler. At PETRONAS, we took bold steps in pioneering new frontiers through the undertaking of several strategic initiatives since 2020 to accelerate the digital transformation in PETRONAS:\n",
       " \n",
       " 1. Liberalization of data & analytics\n",
       " Data & analytics are made available and accessible across PETRONAS through a paradigm shift in ways of working with PETRONAS having 'default access rights' to data.\n",
       "  \n",
       " 2. Insight velocity via single source of truth data platform for analytics\n",
       " Enterprise data hub as a dual cloud-based data platform with integrated capabilities and robust technology stacks, whereby data (internal and external) and analytic products that brings the biggest impact and value is curated for insights.\n",
       " \n",
       " 3. Analytics for everyone\n",
       " Everyone can now step-up in using data and advanced analytics without the need for programming skillsets through advanced analytics in EDH that is low-code and augmented by AI\n",
       " \n",
       " 4. Line of sight across multiple data platforms and applications\n",
       " Data Governance Control Tower to provide the line of sight on areas such as metadata management, data access, security and data quality across all data platforms and applications\n",
       " \n",
       " 5. Modernized data governance adaptive data governance; a modern approach, with centralized governance and decentralised assurance to achieve a centralized, inclusive and resilient data governance, while removing bottlenecks and breaking down siloes through federated execution across the business\n",
       " \n",
       " PETRONAS has achieved major milestones with the above. As such, we would like to share our knowledge & expertise.</td><td>Title: Experience the New Era of Data & AI: Taking Bold Steps\n",
       "                Abstract:  Being an organization operating in >65 countries, PETRONAS has a complex and  growing data landscape across its value chain. It is undoubted that digital transformation is a necessity; however, its success is underpinned by data being a strategic enabler. At PETRONAS, we took bold steps in pioneering new frontiers through the undertaking of several strategic initiatives since 2020 to accelerate the digital transformation in PETRONAS:\n",
       " \n",
       " 1. Liberalization of data & analytics\n",
       " Data & analytics are made available and accessible across PETRONAS through a paradigm shift in ways of working with PETRONAS having 'default access rights' to data.\n",
       "  \n",
       " 2. Insight velocity via single source of truth data platform for analytics\n",
       " Enterprise data hub as a dual cloud-based data platform with integrated capabilities and robust technology stacks, whereby data (internal and external) and analytic products that brings the biggest impact and value is curated for insights.\n",
       " \n",
       " 3. Analytics for everyone\n",
       " Everyone can now step-up in using data and advanced analytics without the need for programming skillsets through advanced analytics in EDH that is low-code and augmented by AI\n",
       " \n",
       " 4. Line of sight across multiple data platforms and applications\n",
       " Data Governance Control Tower to provide the line of sight on areas such as metadata management, data access, security and data quality across all data platforms and applications\n",
       " \n",
       " 5. Modernized data governance adaptive data governance; a modern approach, with centralized governance and decentralised assurance to achieve a centralized, inclusive and resilient data governance, while removing bottlenecks and breaking down siloes through federated execution across the business\n",
       " \n",
       " PETRONAS has achieved major milestones with the above. As such, we would like to share our knowledge & expertise.</td></tr><tr><td>Advancing Customer Centricity: Mission Data's Data & Analytics Transformation</td><td>Overview - \n",
       " Mission Data was a three-year transformation focused on leveraging data to better understand our members and engage with them in a personalized way to help them better manage their finances. We’re accomplishing this by transforming Navy Federal into a member-centric, data-driven and AI-powered organization. Leveraging member data in new and innovative ways allows us to create a superior member experience by delivering value back to our members to show them that we know them, and this has always been at the core of our Member Centric strategy. Due to the evolving nature of the broader technological marketplace, consumers have come to expect increasing degrees of personalization across their experiences. Navy Federal members expect the same from their financial institutions and Mission Data is enabling the financial service and guidance our members have come to expect through personalized insights. A personalized insight takes the guess work out of managing finances for members. We show them we know them AND we do it for them…From providing target savings guidance, tracking financial goals, or letting members know they’ve reached a milestone achievement. Delivering these types of insights tailored to our member’s individual needs ultimately deepens their relationship and loyalty with Navy Federal as their trusted financial partner. This is an effort we’re accelerating as we democratize the Mission Data Platform—making member data accessible and actionable across the enterprise. In 2019, we developed a strategy and roadmap for Navy Federal to build the data and analytics capabilities needed to deliver these types of experiences. Key Enablers in the Journey: People, Mindset, Change, Adoption.</td><td>Title: Advancing Customer Centricity: Mission Data's Data & Analytics Transformation\n",
       "                Abstract:  Overview - \n",
       " Mission Data was a three-year transformation focused on leveraging data to better understand our members and engage with them in a personalized way to help them better manage their finances. We’re accomplishing this by transforming Navy Federal into a member-centric, data-driven and AI-powered organization. Leveraging member data in new and innovative ways allows us to create a superior member experience by delivering value back to our members to show them that we know them, and this has always been at the core of our Member Centric strategy. Due to the evolving nature of the broader technological marketplace, consumers have come to expect increasing degrees of personalization across their experiences. Navy Federal members expect the same from their financial institutions and Mission Data is enabling the financial service and guidance our members have come to expect through personalized insights. A personalized insight takes the guess work out of managing finances for members. We show them we know them AND we do it for them…From providing target savings guidance, tracking financial goals, or letting members know they’ve reached a milestone achievement. Delivering these types of insights tailored to our member’s individual needs ultimately deepens their relationship and loyalty with Navy Federal as their trusted financial partner. This is an effort we’re accelerating as we democratize the Mission Data Platform—making member data accessible and actionable across the enterprise. In 2019, we developed a strategy and roadmap for Navy Federal to build the data and analytics capabilities needed to deliver these types of experiences. Key Enablers in the Journey: People, Mindset, Change, Adoption.</td></tr><tr><td>The C-Level Guide to Data Strategy Success With the Lakehouse</td><td>Join us for a practical session on implementing a data strategy leveraging people, process, and technology to meet the growing demands of your business stakeholders for faster innovation at lower cost. Dael Williamson and Robin Sutara, EMEA Field CTOs at Databricks, will share real-world examples on best practices and things to avoid as you drive your strategy from the board to the business units in your organization</td><td>Title: The C-Level Guide to Data Strategy Success With the Lakehouse\n",
       "                Abstract:  Join us for a practical session on implementing a data strategy leveraging people, process, and technology to meet the growing demands of your business stakeholders for faster innovation at lower cost. Dael Williamson and Robin Sutara, EMEA Field CTOs at Databricks, will share real-world examples on best practices and things to avoid as you drive your strategy from the board to the business units in your organization</td></tr><tr><td>Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse</td><td>Abstract\n",
       " How customers implemented Data Mesh on Databricks and how standardizing on delta format enabled Delta-to-Delta Share to non-Databricks consumers.\n",
       " \n",
       " Presentation Agenda:\n",
       " \n",
       " 1. Current State of the IT Landscape\n",
       " a. Data Silos (problems with organizations not having connected data in the ecosystem)\n",
       " b. A look back on why we moved away from Data warehouses and choose cloud in the first place\n",
       " c. What caused the data chaos in the cloud (instrumentation and too much stitching together) ~ periodic table list of services of the cloud\n",
       " \n",
       " 2. How to strike the balance between Autonomy and Centralization\n",
       " \n",
       " 3. Why Databricks Unity Catalog puts you in the right path to implementing Data Mesh strategy\n",
       " \n",
       " 4. What are the process and features that enable and end-to-end Implementation of a Data Strategy\n",
       " \n",
       " 5. How customers were able to successfully implement the Data mesh on out of the box Unity Catalog and Delta Sharing without overwhelming their IT tool Stack\n",
       " \n",
       " 6. Use-Cases\n",
       " a. Delta-to-Delta Data Sharing \n",
       " b. Delta-to-Others Data Sharing\n",
       " \n",
       " 7. How do you navigate when data today is available across regions, across clouds,on-prem and external systems\n",
       " a. Change Data Feed to share only “data that has changed”\n",
       " \n",
       " 8. Data stewardship\n",
       " a. Why ABAC is important\n",
       " b. How file based access policies and governance play an important role\n",
       " \n",
       " 9. Future State and its pitfalls\n",
       " a. Egress Costs\n",
       " b. Data compliances</td><td>Title: Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse\n",
       "                Abstract:  Abstract\n",
       " How customers implemented Data Mesh on Databricks and how standardizing on delta format enabled Delta-to-Delta Share to non-Databricks consumers.\n",
       " \n",
       " Presentation Agenda:\n",
       " \n",
       " 1. Current State of the IT Landscape\n",
       " a. Data Silos (problems with organizations not having connected data in the ecosystem)\n",
       " b. A look back on why we moved away from Data warehouses and choose cloud in the first place\n",
       " c. What caused the data chaos in the cloud (instrumentation and too much stitching together) ~ periodic table list of services of the cloud\n",
       " \n",
       " 2. How to strike the balance between Autonomy and Centralization\n",
       " \n",
       " 3. Why Databricks Unity Catalog puts you in the right path to implementing Data Mesh strategy\n",
       " \n",
       " 4. What are the process and features that enable and end-to-end Implementation of a Data Strategy\n",
       " \n",
       " 5. How customers were able to successfully implement the Data mesh on out of the box Unity Catalog and Delta Sharing without overwhelming their IT tool Stack\n",
       " \n",
       " 6. Use-Cases\n",
       " a. Delta-to-Delta Data Sharing \n",
       " b. Delta-to-Others Data Sharing\n",
       " \n",
       " 7. How do you navigate when data today is available across regions, across clouds,on-prem and external systems\n",
       " a. Change Data Feed to share only “data that has changed”\n",
       " \n",
       " 8. Data stewardship\n",
       " a. Why ABAC is important\n",
       " b. How file based access policies and governance play an important role\n",
       " \n",
       " 9. Future State and its pitfalls\n",
       " a. Egress Costs\n",
       " b. Data compliances</td></tr><tr><td>Event Driven Real-Time Supply Chain Ecosystem Powered by Lakehouse</td><td>As the backbone of Australia’s supply chain, the Australia Rail Track Corporation (ARTC) plays a vital role in the management and monitoring of goods transportation across 8,500km of its rail network throughout Australia. ARTC provides weighbridges along their track which read train weights as they pass at speeds of up to 60 kilometers an hour. This information is highly valuable and is required both by ARTC and their customers to provide accurate haulage weight details, analyze technical equipment, and help ensure wagons have been loaded correctly. \n",
       " \n",
       " 750 trains run across a network of 8500 km in a day and generate real-time data at approximately 50 sensor platforms. \n",
       " \n",
       " With the help of structured streaming and Delta Lake, ARTC was able to analyze and store: \n",
       " \n",
       " 1. Precise train location\n",
       " 2. Weight of the train in real-time\n",
       " 3. Train crossing time to the second level \n",
       " 4. Train speed, temperature, sound frequency, and friction \n",
       " 5. Train schedule lookups \n",
       " \n",
       " Once all the IoT data has been pulled together from an IoT event hub, it is processed in real-time using structured streaming and stored in Delta Lake. To understand the train GPS location API calls are then made per minute per train from the Lakehouse. API calls are made in real-time to another scheduling system to lookup customer info. Once the processed, enriched, data is stored in Delta Lake, an API layer was also created on top of it to expose this data to all consumers. \n",
       " \n",
       " Outcome:\n",
       " \n",
       " Increased transparency on weight data as it is now made available to customers. \n",
       "A digital data ecosystem that now ARTC’s customers use to meet their KPIs/ planning. \n",
       "Ability to determine temporary speed restrictions across the network to improve train scheduling accuracy and also schedule network maintenance based on train schedules/ speed.</td><td>Title: Event Driven Real-Time Supply Chain Ecosystem Powered by Lakehouse\n",
       "                Abstract:  As the backbone of Australia’s supply chain, the Australia Rail Track Corporation (ARTC) plays a vital role in the management and monitoring of goods transportation across 8,500km of its rail network throughout Australia. ARTC provides weighbridges along their track which read train weights as they pass at speeds of up to 60 kilometers an hour. This information is highly valuable and is required both by ARTC and their customers to provide accurate haulage weight details, analyze technical equipment, and help ensure wagons have been loaded correctly. \n",
       " \n",
       " 750 trains run across a network of 8500 km in a day and generate real-time data at approximately 50 sensor platforms. \n",
       " \n",
       " With the help of structured streaming and Delta Lake, ARTC was able to analyze and store: \n",
       " \n",
       " 1. Precise train location\n",
       " 2. Weight of the train in real-time\n",
       " 3. Train crossing time to the second level \n",
       " 4. Train speed, temperature, sound frequency, and friction \n",
       " 5. Train schedule lookups \n",
       " \n",
       " Once all the IoT data has been pulled together from an IoT event hub, it is processed in real-time using structured streaming and stored in Delta Lake. To understand the train GPS location API calls are then made per minute per train from the Lakehouse. API calls are made in real-time to another scheduling system to lookup customer info. Once the processed, enriched, data is stored in Delta Lake, an API layer was also created on top of it to expose this data to all consumers. \n",
       " \n",
       " Outcome:\n",
       " \n",
       " Increased transparency on weight data as it is now made available to customers. \n",
       "A digital data ecosystem that now ARTC’s customers use to meet their KPIs/ planning. \n",
       "Ability to determine temporary speed restrictions across the network to improve train scheduling accuracy and also schedule network maintenance based on train schedules/ speed.</td></tr><tr><td>Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark™</td><td>Apache Spark™’s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important.\n",
       " \n",
       " Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa’s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data.</td><td>Title: Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark™\n",
       "                Abstract:  Apache Spark™’s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important.\n",
       " \n",
       " Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa’s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data.</td></tr><tr><td>Taking Control of Streaming Healthcare Data</td><td>Chesapeake Regional Information System for our Patients (CRISP), a nonprofit healthcare information exchange (HIE), initially partnered with Slalom to build a Databricks data lakehouse architecture in response to the analytics demands of the COVID-19 pandemic, since then they have expanded the platform to additional use cases. Recently they have worked together to engineer streaming data pipelines to process healthcare messages, such as HL7, to help CRISP become vendor independent.\n",
       " \n",
       " Our talk will focus on the improvements CRISP has made to their data lakehouse platform to support streaming use cases and the impact these changes have had for the organization. We will touch on using Databricks Auto Loader to efficiently ingest incoming files, ensuring data quality with Delta Live Tables, and sharing data internally with a SQL warehouse, as well as some of the work CRISP has done to parse and standardize HL7 messages from hundreds of sources. These efforts have allowed CRISP to stream over 4 million messages daily in near real-time with the scalability it needs to continue to onboard new healthcare providers so it can continue to facilitate care and improve health outcomes.</td><td>Title: Taking Control of Streaming Healthcare Data\n",
       "                Abstract:  Chesapeake Regional Information System for our Patients (CRISP), a nonprofit healthcare information exchange (HIE), initially partnered with Slalom to build a Databricks data lakehouse architecture in response to the analytics demands of the COVID-19 pandemic, since then they have expanded the platform to additional use cases. Recently they have worked together to engineer streaming data pipelines to process healthcare messages, such as HL7, to help CRISP become vendor independent.\n",
       " \n",
       " Our talk will focus on the improvements CRISP has made to their data lakehouse platform to support streaming use cases and the impact these changes have had for the organization. We will touch on using Databricks Auto Loader to efficiently ingest incoming files, ensuring data quality with Delta Live Tables, and sharing data internally with a SQL warehouse, as well as some of the work CRISP has done to parse and standardize HL7 messages from hundreds of sources. These efforts have allowed CRISP to stream over 4 million messages daily in near real-time with the scalability it needs to continue to onboard new healthcare providers so it can continue to facilitate care and improve health outcomes.</td></tr><tr><td>Deploying the Lakehouse to Improve the Viewer Experience on Discovery+</td><td>In this session we will discuss how real-time data streaming can be used to gain insights into user behavior and preferences, and how this data is being used to provide personalized content and recommendations on discovery+. We will examine techniques that enables faster decision making and insights on accurate real time data including data masking and data validation. To enable a wide set of data consumers from data engineers to data scientists to data analysts, we will discuss how Unity Catalog is leveraged for secure data access and sharing while still allowing teams flexibility. Operating at this scale requires examining the value being created by the data being processed and optimizing along the way and we will share some of our success in this area.</td><td>Title: Deploying the Lakehouse to Improve the Viewer Experience on Discovery+\n",
       "                Abstract:  In this session we will discuss how real-time data streaming can be used to gain insights into user behavior and preferences, and how this data is being used to provide personalized content and recommendations on discovery+. We will examine techniques that enables faster decision making and insights on accurate real time data including data masking and data validation. To enable a wide set of data consumers from data engineers to data scientists to data analysts, we will discuss how Unity Catalog is leveraged for secure data access and sharing while still allowing teams flexibility. Operating at this scale requires examining the value being created by the data being processed and optimizing along the way and we will share some of our success in this area.</td></tr><tr><td>Apache Spark™ Streaming and Delta Live Tables Accelerates KPMG Clients For Real-Time IoT Insights</td><td>Unplanned downtime in manufacturing costs firms up to a trillion dollars annually. Time that materials spend sitting on a production line is lost revenue. Even just 15 hours of downtime a week adds up to over 800 hours of downtime yearly. The use of internet of things or IoT devices can cut this time down by providing details of machine metrics. However, IoT predictive maintenance is challenged by the lack of effective, scalable infrastructure and machine learning solutions. IoT data can be the size of multiple terabytes per day and can come in a variety of formats. Furthermore, without any insights and analysis, this data becomes just another table. \n",
       " \n",
       " The KPMG Databricks IoT Accelerator is an end-to-end solution enabling manufacturing plant operators to have a bird’s eye view of their machines’ health and empowers proactive machine maintenance across their portfolio of IoT devices. The Databricks Accelerator ingests IoT streaming data at scale and implements the Databricks medallion architecture while leveraging delta live tables to clean and process data. Real time machine learning models are developed from IoT machine measurements and are managed in MLflow. The AI predictions and IoT device readings are compiled in the gold table powering downstream dashboards like Tableau. Dashboards inform machine operators of not only machines’ ailments, but action they can take to mitigate issues before they arise. Operators can see fault history to aid in understanding failure trends, and can filter dashboards by fault type, machine, or specific sensor reading.</td><td>Title: Apache Spark™ Streaming and Delta Live Tables Accelerates KPMG Clients For Real-Time IoT Insights\n",
       "                Abstract:  Unplanned downtime in manufacturing costs firms up to a trillion dollars annually. Time that materials spend sitting on a production line is lost revenue. Even just 15 hours of downtime a week adds up to over 800 hours of downtime yearly. The use of internet of things or IoT devices can cut this time down by providing details of machine metrics. However, IoT predictive maintenance is challenged by the lack of effective, scalable infrastructure and machine learning solutions. IoT data can be the size of multiple terabytes per day and can come in a variety of formats. Furthermore, without any insights and analysis, this data becomes just another table. \n",
       " \n",
       " The KPMG Databricks IoT Accelerator is an end-to-end solution enabling manufacturing plant operators to have a bird’s eye view of their machines’ health and empowers proactive machine maintenance across their portfolio of IoT devices. The Databricks Accelerator ingests IoT streaming data at scale and implements the Databricks medallion architecture while leveraging delta live tables to clean and process data. Real time machine learning models are developed from IoT machine measurements and are managed in MLflow. The AI predictions and IoT device readings are compiled in the gold table powering downstream dashboards like Tableau. Dashboards inform machine operators of not only machines’ ailments, but action they can take to mitigate issues before they arise. Operators can see fault history to aid in understanding failure trends, and can filter dashboards by fault type, machine, or specific sensor reading.</td></tr><tr><td>Optimizing batch and streaming aggregations</td><td>I've got a client recently who asked me to optimize their batch and streaming workloads. It happened to be aggregations using DataFrame.groupBy operation with a custom Scala UDAF over a data stream from Kafka. Just a single simple-looking request that turned itself up into a a-few-month-long hunt to find a more performant query execution planning than ObjectHashAggregateExec that kept falling back to a sort-based aggregation (i.e. the worst possible aggregation runtime performance). It quickly taught us that an aggregation using a custom Scala UDAF cannot be planned other than ObjectHashAggregateExec but at least tasks don't always have to fall back. And that's just batch workloads. When you throw in streaming semantics and think of the different output modes, windowing and streaming watermark optimizing aggregation can take a long time to do right.\n",
       " \n",
       " During the talk you'll learn the different execution strategies of aggregation (groupBy) in Apache Spark and what issues I faced while hunting down the tiniest performance improvements. You will learn why an aggregation with a Scala UDAF cannot be planned using HashAggregateExec and what problems you may come across while using streaming aggregation over the built-in Kafka data source. Lots of optimization tips and tricks with quite a few looks at the internals of aggregation query execution.</td><td>Title: Optimizing batch and streaming aggregations\n",
       "                Abstract:  I've got a client recently who asked me to optimize their batch and streaming workloads. It happened to be aggregations using DataFrame.groupBy operation with a custom Scala UDAF over a data stream from Kafka. Just a single simple-looking request that turned itself up into a a-few-month-long hunt to find a more performant query execution planning than ObjectHashAggregateExec that kept falling back to a sort-based aggregation (i.e. the worst possible aggregation runtime performance). It quickly taught us that an aggregation using a custom Scala UDAF cannot be planned other than ObjectHashAggregateExec but at least tasks don't always have to fall back. And that's just batch workloads. When you throw in streaming semantics and think of the different output modes, windowing and streaming watermark optimizing aggregation can take a long time to do right.\n",
       " \n",
       " During the talk you'll learn the different execution strategies of aggregation (groupBy) in Apache Spark and what issues I faced while hunting down the tiniest performance improvements. You will learn why an aggregation with a Scala UDAF cannot be planned using HashAggregateExec and what problems you may come across while using streaming aggregation over the built-in Kafka data source. Lots of optimization tips and tricks with quite a few looks at the internals of aggregation query execution.</td></tr><tr><td>Structured Streaming: Demystifying Arbitrary Stateful Operations</td><td>Let’s face it -- data is messy. And your company’s business requirements? Even messier! You’re staring at your screen, knowing there is a tool that will let you give your business partners the information they need as quickly as they need it. There’s even a Python version of it now. But…it looks kinda scary. You’ve never used it before, and you don’t know where to start. Yes, we’re talking about the dreaded flatMapGroupsWithState! But fear not - we’ve got you covered. In this session we’ll take a real-word use case and use it to show you how to break down flatMapGroupsWithState into its basic building blocks. We’ll explain each piece in both Scala and the newly-released Python, and at the end we’ll illustrate how it all comes together to enable the implementation of arbitrary stateful operations with Spark Structured Streaming.</td><td>Title: Structured Streaming: Demystifying Arbitrary Stateful Operations\n",
       "                Abstract:  Let’s face it -- data is messy. And your company’s business requirements? Even messier! You’re staring at your screen, knowing there is a tool that will let you give your business partners the information they need as quickly as they need it. There’s even a Python version of it now. But…it looks kinda scary. You’ve never used it before, and you don’t know where to start. Yes, we’re talking about the dreaded flatMapGroupsWithState! But fear not - we’ve got you covered. In this session we’ll take a real-word use case and use it to show you how to break down flatMapGroupsWithState into its basic building blocks. We’ll explain each piece in both Scala and the newly-released Python, and at the end we’ll illustrate how it all comes together to enable the implementation of arbitrary stateful operations with Spark Structured Streaming.</td></tr><tr><td>How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience</td><td>Disney+ uses Amazon Kinesis and Databricks Apache Spark™ streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers.</td><td>Title: How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience\n",
       "                Abstract:  Disney+ uses Amazon Kinesis and Databricks Apache Spark™ streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers.</td></tr><tr><td>The Future is Open: Data Streaming in an Omni-Cloud Reality</td><td>An assortment of low-code connectors boasts the ability to make data available for analytics in real time. However, many such turn-key solutions face challenges: cost-inefficient EDW targets; inability to evolve schema; forbiddingly expensive data exports due to cloud and vendor lock-in. The alternative: an open data lake that unifies batch and streaming workloads. Increasingly, we must adapt to a multi-cloud reality. Delta Lake decouples data storage from proprietary formats, dramatically reducing data extraction costs. Exporting data from delta tables eliminates the compute and storage API costs required by EDW. Bronze landing zones in Delta format build the foundation for a medallion architecture. Apache Spark™ Structured Streaming provides a unified ingestion strategy. Streaming triggers allow us to switch back and forth between batch and stream with one-line code changes. Streaming aggregation enables us to incrementally compute on data that arrives near each other.\n",
       " It is a misconception that streaming requires always-on clusters and thus is prohibitively expensive. Streaming means the ability to automatically handle new data and avoid reprocessing of historical data; real time is optional depending on use cases. Autoloader is a technique to discover newly arrived data and ensure exactly once, incremental processing. DLT further simplifies streaming jobs, accelerating the development cycle.\n",
       " Workflows bring it all together and the open source dbx project applies SWE best practices. Workflows can orchestrate tasks such as DLT pipelines, spark jobs, SQL scripts, as well as dbt transformations. Developers can define compute resources, scheduling, and dependencies in descriptive yaml format. Both task definitions and job code are versioned with popular source control systems.</td><td>Title: The Future is Open: Data Streaming in an Omni-Cloud Reality\n",
       "                Abstract:  An assortment of low-code connectors boasts the ability to make data available for analytics in real time. However, many such turn-key solutions face challenges: cost-inefficient EDW targets; inability to evolve schema; forbiddingly expensive data exports due to cloud and vendor lock-in. The alternative: an open data lake that unifies batch and streaming workloads. Increasingly, we must adapt to a multi-cloud reality. Delta Lake decouples data storage from proprietary formats, dramatically reducing data extraction costs. Exporting data from delta tables eliminates the compute and storage API costs required by EDW. Bronze landing zones in Delta format build the foundation for a medallion architecture. Apache Spark™ Structured Streaming provides a unified ingestion strategy. Streaming triggers allow us to switch back and forth between batch and stream with one-line code changes. Streaming aggregation enables us to incrementally compute on data that arrives near each other.\n",
       " It is a misconception that streaming requires always-on clusters and thus is prohibitively expensive. Streaming means the ability to automatically handle new data and avoid reprocessing of historical data; real time is optional depending on use cases. Autoloader is a technique to discover newly arrived data and ensure exactly once, incremental processing. DLT further simplifies streaming jobs, accelerating the development cycle.\n",
       " Workflows bring it all together and the open source dbx project applies SWE best practices. Workflows can orchestrate tasks such as DLT pipelines, spark jobs, SQL scripts, as well as dbt transformations. Developers can define compute resources, scheduling, and dependencies in descriptive yaml format. Both task definitions and job code are versioned with popular source control systems.</td></tr><tr><td>Getting Insight From Anything: Gathering Data With IoT Devices and Delta Live</td><td>The drive to make data-driven decisions shapes the strategy of many companies. However, this dream has a problem; just because you need to make a decision does not mean you have the data to make it! Often these decisions require data from sources typically unconnected to the internet. This could be an architect determined to understand how the construction of her building is progressing, or a manufacturer wanting to live their best Industry 4.0 life. It's not uncommon for companies to find this journey to become “smart” is a long one; with difficult ethical ground to cover, or prohibitive costs associated with procuring the necessary equipment. However, the development of cheap and accessible IoT devices allows for the gathering of data from anything to be more within reach than ever.\n",
       " \n",
       " This talk will look at efforts to build proof-of-concept IoT systems. We will discuss our (continuing) journey of using and managing hardware and the problems we faced along the way. The main focus however will be the software stack. Azure IoT Hub is used for device management, with Power BI supporting the front end. However, the centre-piece is Delta Live Tables - allowing us a method of near real-time analysis of our data. We will discuss the advantages of using Delta Live for this, and how we wish to extend our IoT systems in the future.</td><td>Title: Getting Insight From Anything: Gathering Data With IoT Devices and Delta Live\n",
       "                Abstract:  The drive to make data-driven decisions shapes the strategy of many companies. However, this dream has a problem; just because you need to make a decision does not mean you have the data to make it! Often these decisions require data from sources typically unconnected to the internet. This could be an architect determined to understand how the construction of her building is progressing, or a manufacturer wanting to live their best Industry 4.0 life. It's not uncommon for companies to find this journey to become “smart” is a long one; with difficult ethical ground to cover, or prohibitive costs associated with procuring the necessary equipment. However, the development of cheap and accessible IoT devices allows for the gathering of data from anything to be more within reach than ever.\n",
       " \n",
       " This talk will look at efforts to build proof-of-concept IoT systems. We will discuss our (continuing) journey of using and managing hardware and the problems we faced along the way. The main focus however will be the software stack. Azure IoT Hub is used for device management, with Power BI supporting the front end. However, the centre-piece is Delta Live Tables - allowing us a method of near real-time analysis of our data. We will discuss the advantages of using Delta Live for this, and how we wish to extend our IoT systems in the future.</td></tr><tr><td>Practical Pipelines: A Houseplant Alerting System With ksqlDB</td><td>Houseplants can be hard; in many cases, over-watering and under-watering can have the same symptoms. Take away the guesswork involved in caring for your houseplants while also gaining valuable experience in building a practical, event-driven pipeline in your own home! This session explores the process of building a houseplant monitoring and alerting system using a Raspberry Pi and Apache Kafka. \n",
       " \n",
       " Moisture and temperature readings are captured from sensors in the soil and streamed into Kafka. From here, we’ll use stream processing to transform the data, create a summary view of the current state, and drive real-time push alerts through Telegram. In this session, I’ll talk about how I ingest the data followed by the tools – including ksqlDB and Kafka Connect – that help transform the raw data into useful information, and finally, I’ll show how to use Kafka Producers and Consumers to make the entire application more interactive.\n",
       " \n",
       " By the end of this session you’ll have everything you need to start building practical streaming pipelines in your own home. Roll up your sleeves – let’s get our hands dirty!</td><td>Title: Practical Pipelines: A Houseplant Alerting System With ksqlDB\n",
       "                Abstract:  Houseplants can be hard; in many cases, over-watering and under-watering can have the same symptoms. Take away the guesswork involved in caring for your houseplants while also gaining valuable experience in building a practical, event-driven pipeline in your own home! This session explores the process of building a houseplant monitoring and alerting system using a Raspberry Pi and Apache Kafka. \n",
       " \n",
       " Moisture and temperature readings are captured from sensors in the soil and streamed into Kafka. From here, we’ll use stream processing to transform the data, create a summary view of the current state, and drive real-time push alerts through Telegram. In this session, I’ll talk about how I ingest the data followed by the tools – including ksqlDB and Kafka Connect – that help transform the raw data into useful information, and finally, I’ll show how to use Kafka Producers and Consumers to make the entire application more interactive.\n",
       " \n",
       " By the end of this session you’ll have everything you need to start building practical streaming pipelines in your own home. Roll up your sleeves – let’s get our hands dirty!</td></tr><tr><td>Disaster Recovery Strategies for Structured Streams</td><td>In recent years, many businesses have adopted real-time streaming applications to enable faster decision making, quicker predictions and improved customer experiences. Few of these applications are driving critical business use cases like financial fraud detection, loan application processing, personalized offers, etc. These business critical applications need robust disaster recovery strategies to recover from the catastrophic events to reduce the lost uptime. However, most organizations find it hard to set up disaster recovery for streaming applications as it involves continuous data flow. Streaming state and temporal behavior of data brings additional complexities to the DR strategy.\n",
       " \n",
       " A reliable disaster recovery strategy includes backup, failover and failback approaches for the streaming application. Unlike the batch applications, these steps include many moving elements and need a very sophisticated approach to ensure that the services are failing over the DR region and meet the set RTO and RPO requirements. In this session, we will cover following topics with a FINSERV use case demo:\n",
       " 1. Backup strategy: backup of delta tables, message bus services and checkpoint including offsets\n",
       " 2. Failover strategy: failover strategy to disable services in the primary region and start the services in the secondary region with minimum data loss \n",
       " 3. Failback strategy: failback strategy to restart the services in the primary region once all the services are restored \n",
       " 4. Common challenges and best practices for backup</td><td>Title: Disaster Recovery Strategies for Structured Streams\n",
       "                Abstract:  In recent years, many businesses have adopted real-time streaming applications to enable faster decision making, quicker predictions and improved customer experiences. Few of these applications are driving critical business use cases like financial fraud detection, loan application processing, personalized offers, etc. These business critical applications need robust disaster recovery strategies to recover from the catastrophic events to reduce the lost uptime. However, most organizations find it hard to set up disaster recovery for streaming applications as it involves continuous data flow. Streaming state and temporal behavior of data brings additional complexities to the DR strategy.\n",
       " \n",
       " A reliable disaster recovery strategy includes backup, failover and failback approaches for the streaming application. Unlike the batch applications, these steps include many moving elements and need a very sophisticated approach to ensure that the services are failing over the DR region and meet the set RTO and RPO requirements. In this session, we will cover following topics with a FINSERV use case demo:\n",
       " 1. Backup strategy: backup of delta tables, message bus services and checkpoint including offsets\n",
       " 2. Failover strategy: failover strategy to disable services in the primary region and start the services in the secondary region with minimum data loss \n",
       " 3. Failback strategy: failback strategy to restart the services in the primary region once all the services are restored \n",
       " 4. Common challenges and best practices for backup</td></tr><tr><td>Near Real-Time Data Linkage and Entity Resolution for High Volumes of Continuous and Unsynchronized Data Streams</td><td>Citrix Analytics is a cloud-based service that provides analytical insights from data across Citrix product portfolio. It collates data from various sources and proactively detects security threats.\n",
       " \n",
       " The data consists of information from various entities like users, devices, applications, network, files and shares, along with their correlation over a period. This data is analyzed by various ML algorithms to detect any suspicious activity. The ML models need correlated data in real-time so vulnerabilities can be detected instantly and remediation actions can be taken.\n",
       " \n",
       " The data pipeline comprises of multiple event sources, each one of them producing high volumes of continuous data. This data generated from different sources need to be correlated using common identifiers to produce final stream of data with low latency and high data quality.\n",
       " \n",
       " The proposed solution is to build scalable, extensible, fault-tolerant system for stateful joining of two or more high volume event streams that are not fully synchronised, event ordering is not guaranteed, and certain events arrive a bit late. The system also ensures to combine the events/link so the data is in near real-time with low latency so that there is no impact on the downstream applications like ML models in determining the suspicious behavior. Apart from combining events the system also ensures to propagate the needed entities to other product streams to help in entity resolution. If any of the needed data is yet to arrive, we can configure few parameters based on use cases, so as to achieve the required eventual or attribute consistency.\n",
       " \n",
       " The architecture is an agnostic streaming framework, but our implementation leverages Spark Streaming and Kafka. The solution is implemented and is processing billions of events per day.</td><td>Title: Near Real-Time Data Linkage and Entity Resolution for High Volumes of Continuous and Unsynchronized Data Streams\n",
       "                Abstract:  Citrix Analytics is a cloud-based service that provides analytical insights from data across Citrix product portfolio. It collates data from various sources and proactively detects security threats.\n",
       " \n",
       " The data consists of information from various entities like users, devices, applications, network, files and shares, along with their correlation over a period. This data is analyzed by various ML algorithms to detect any suspicious activity. The ML models need correlated data in real-time so vulnerabilities can be detected instantly and remediation actions can be taken.\n",
       " \n",
       " The data pipeline comprises of multiple event sources, each one of them producing high volumes of continuous data. This data generated from different sources need to be correlated using common identifiers to produce final stream of data with low latency and high data quality.\n",
       " \n",
       " The proposed solution is to build scalable, extensible, fault-tolerant system for stateful joining of two or more high volume event streams that are not fully synchronised, event ordering is not guaranteed, and certain events arrive a bit late. The system also ensures to combine the events/link so the data is in near real-time with low latency so that there is no impact on the downstream applications like ML models in determining the suspicious behavior. Apart from combining events the system also ensures to propagate the needed entities to other product streams to help in entity resolution. If any of the needed data is yet to arrive, we can configure few parameters based on use cases, so as to achieve the required eventual or attribute consistency.\n",
       " \n",
       " The architecture is an agnostic streaming framework, but our implementation leverages Spark Streaming and Kafka. The solution is implemented and is processing billions of events per day.</td></tr><tr><td>Realtime ML in Marketplace @ Lyft</td><td>Lyft is a ride-sharing company which is a two-sided marketplace; balancing supply and demand using various levers (passenger pricing, driver incentive etc.) to maintain an efficient system. Lyft has built a real-time optimization platform that helps to build the product faster. This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. This infrastructure consumes a massive number of events from different sources to make real-time product decisions.\n",
       " Rakesh discusses how Lyft organically evolved and scaled the streaming platform that provides a consistent view of the marketplace to aid an individual team independently run their optimization. The platform offers online and offline feature access that helps teams to back test their model in the future. It provides various other powerful capabilities such as replaying the production ML feature in PyNotebook, feature validation, near-realtime model training, executing multi-layer of models in a DAG, etc. \n",
       " He is also going to elaborate things that helped him scale the systems to process millions of events per minute and power T0 products with tighter latency SLA.</td><td>Title: Realtime ML in Marketplace @ Lyft\n",
       "                Abstract:  Lyft is a ride-sharing company which is a two-sided marketplace; balancing supply and demand using various levers (passenger pricing, driver incentive etc.) to maintain an efficient system. Lyft has built a real-time optimization platform that helps to build the product faster. This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. This infrastructure consumes a massive number of events from different sources to make real-time product decisions.\n",
       " Rakesh discusses how Lyft organically evolved and scaled the streaming platform that provides a consistent view of the marketplace to aid an individual team independently run their optimization. The platform offers online and offline feature access that helps teams to back test their model in the future. It provides various other powerful capabilities such as replaying the production ML feature in PyNotebook, feature validation, near-realtime model training, executing multi-layer of models in a DAG, etc. \n",
       " He is also going to elaborate things that helped him scale the systems to process millions of events per minute and power T0 products with tighter latency SLA.</td></tr><tr><td>Streaming Data Analytics With Power BI and Databricks</td><td>In this session we will present a series of end-to-end technical demos illustrating the synergy between Databricks and Power BI for streaming use cases. \n",
       "  - Scenario 1: DLT + Power BI Direct Query and Auto Refresh\n",
       "  - Scenario 2: Structured Streaming + Power BI streaming datasets \n",
       "  - Scenario 3: DLT + Power BI composite datasets\n",
       "  - Considerations when to choose which scenario</td><td>Title: Streaming Data Analytics With Power BI and Databricks\n",
       "                Abstract:  In this session we will present a series of end-to-end technical demos illustrating the synergy between Databricks and Power BI for streaming use cases. \n",
       "  - Scenario 1: DLT + Power BI Direct Query and Auto Refresh\n",
       "  - Scenario 2: Structured Streaming + Power BI streaming datasets \n",
       "  - Scenario 3: DLT + Power BI composite datasets\n",
       "  - Considerations when to choose which scenario</td></tr><tr><td>How Coinbase Built and Optimized SOON, a Streaming Ingestion Framework</td><td>Data with low latency is important for real-time incident analysis and metrics. Though we have up-to-date data in OLTP databases, they cannot support those scenarios. Data need to be replicated to a data warehouse to serve queries doing group-bys, and joins across tables from different systems. At Coinbase, we designed SOON (Spark cOntinuOus iNgestion) based on Kafka, Kafka Connect, and Spark as an incremental table replication solution to replicate tables of any size from any database to Delta Lake in a timely manner. It also supports Kafka events ingestion naturally.\n",
       " \n",
       " SOON incrementally ingests Kafka events as appends, updates, and deletes to an existing table on Delta Lake. The events are grouped into two categories: CDC (change data capture) events generated by Kafka Connect source connectors, and non-CDC events by the frontend or backend services. Both types can be appended or merged into the Delta Lake. Non-CDC events can be in any format, but CDC events must be in the standard SOON CDC schema. We implemented Kafka Connect SMTs to transform raw CDC events into this standardized format. SOON unifies all streaming ingestion scenarios such that users only need to learn one onboarding experience and the team only needs to maintain one framework.\n",
       " \n",
       " We care about the ingestion performance. The biggest append-only table onboarded has ingress traffic at hundreds of thousands events per second; the biggest CDC-merge table onboarded has a snapshot size of a few TBs and CDC update traffic at hundreds of events per second. A lot of innovative ideas are incorporated in SOON to improve its performance, such as min-max range merge optimization, KMeans merge optimization, no-update merge for deduplication, generated columns as partitions, etc.</td><td>Title: How Coinbase Built and Optimized SOON, a Streaming Ingestion Framework\n",
       "                Abstract:  Data with low latency is important for real-time incident analysis and metrics. Though we have up-to-date data in OLTP databases, they cannot support those scenarios. Data need to be replicated to a data warehouse to serve queries doing group-bys, and joins across tables from different systems. At Coinbase, we designed SOON (Spark cOntinuOus iNgestion) based on Kafka, Kafka Connect, and Spark as an incremental table replication solution to replicate tables of any size from any database to Delta Lake in a timely manner. It also supports Kafka events ingestion naturally.\n",
       " \n",
       " SOON incrementally ingests Kafka events as appends, updates, and deletes to an existing table on Delta Lake. The events are grouped into two categories: CDC (change data capture) events generated by Kafka Connect source connectors, and non-CDC events by the frontend or backend services. Both types can be appended or merged into the Delta Lake. Non-CDC events can be in any format, but CDC events must be in the standard SOON CDC schema. We implemented Kafka Connect SMTs to transform raw CDC events into this standardized format. SOON unifies all streaming ingestion scenarios such that users only need to learn one onboarding experience and the team only needs to maintain one framework.\n",
       " \n",
       " We care about the ingestion performance. The biggest append-only table onboarded has ingress traffic at hundreds of thousands events per second; the biggest CDC-merge table onboarded has a snapshot size of a few TBs and CDC update traffic at hundreds of events per second. A lot of innovative ideas are incorporated in SOON to improve its performance, such as min-max range merge optimization, KMeans merge optimization, no-update merge for deduplication, generated columns as partitions, etc.</td></tr><tr><td>High Volume Intelligent Streaming with Sub-Minute SLA for Near Real-Time Data Replication</td><td>Come and learn an innovative solution built around Databricks structured streaming and Delta Live Tables (DLT) to replicate thousands of tables from on-premisis to cloud-based relational databases. A highly desirable pattern for many enterprises across the industries to replicate on-premises data to cloud-based data lakes and data stores in near real time for consumption. This powerful architecture can offload legacy platform workloads and accelerate cloud journey. The intelligent cost-efficient solution leverages thread-pools, multi-task jobs, Kafka, spark structured streaming and DLT. This session will go into detail about problems, solutions, lessons-learned and best practices.</td><td>Title: High Volume Intelligent Streaming with Sub-Minute SLA for Near Real-Time Data Replication\n",
       "                Abstract:  Come and learn an innovative solution built around Databricks structured streaming and Delta Live Tables (DLT) to replicate thousands of tables from on-premisis to cloud-based relational databases. A highly desirable pattern for many enterprises across the industries to replicate on-premises data to cloud-based data lakes and data stores in near real time for consumption. This powerful architecture can offload legacy platform workloads and accelerate cloud journey. The intelligent cost-efficient solution leverages thread-pools, multi-task jobs, Kafka, spark structured streaming and DLT. This session will go into detail about problems, solutions, lessons-learned and best practices.</td></tr><tr><td>Using Cisco Spaces Firehose API as a Stream of Data for Real-Time Occupancy Modelling</td><td>Honeywell manages the control of equipment for hundreds of thousands of buildings worldwide. Many of our outcomes relating to energy and comfort rely on knowing where people are in the building at any one time. This is so we can target health and comfort conditions more suitably to areas where are more densely populated. Many of these buildings have Cisco IT infrastructure in them. Using their WIFI points and the RSSI signal strength from people’s laptops and phones, Cisco can calculate the number of people in each area of the building. Cisco Spaces offer this data up as a real-time streaming source. Honeywell HBT has utilised this stream of data by writing delta live table pipelines to consume this data source.\n",
       " Honeywell buildings can now receive this firehose data from hundred of concurrent customers and provide this occupancy data as a service to our vertical offerings in commercial, health, real estate and education. We will discuss the benefits of using DLT to handle this sort of incoming stream data, and illustrate the pain points we had and the resolutions we undertook in successfully receiving the stream of Cisco data. We will illustrate how our DLT pipeline was designed, and how it scaled to deal with huge quantities of real time streaming data.</td><td>Title: Using Cisco Spaces Firehose API as a Stream of Data for Real-Time Occupancy Modelling\n",
       "                Abstract:  Honeywell manages the control of equipment for hundreds of thousands of buildings worldwide. Many of our outcomes relating to energy and comfort rely on knowing where people are in the building at any one time. This is so we can target health and comfort conditions more suitably to areas where are more densely populated. Many of these buildings have Cisco IT infrastructure in them. Using their WIFI points and the RSSI signal strength from people’s laptops and phones, Cisco can calculate the number of people in each area of the building. Cisco Spaces offer this data up as a real-time streaming source. Honeywell HBT has utilised this stream of data by writing delta live table pipelines to consume this data source.\n",
       " Honeywell buildings can now receive this firehose data from hundred of concurrent customers and provide this occupancy data as a service to our vertical offerings in commercial, health, real estate and education. We will discuss the benefits of using DLT to handle this sort of incoming stream data, and illustrate the pain points we had and the resolutions we undertook in successfully receiving the stream of Cisco data. We will illustrate how our DLT pipeline was designed, and how it scaled to deal with huge quantities of real time streaming data.</td></tr><tr><td>Leveraging IoT Data at Scale to Mitigate Global Water Risks Using Apache Spark Streaming and Delta</td><td>Every year, billions of dollars are lost due to water risks from storms, floods, and droughts. Water data scarcity and excess are issues that risk models cannot overcome, creating a world of uncertainty. Divirod is building a platform of water data by normalizing diverse data sources of varying velocity into one unified data asset. In addition to publicly available third-party datasets, we are rapidly deploying our own IoT sensors. \n",
       " \n",
       " These sensors ingest signals at a rate of about 100,000 messages per hour into preprocessing, signal-processing, analytics, and postprocessing workloads in one spark-streaming pipeline to enable critical real-time decision-making processes. By leveraging streaming architecture, we were able to reduce end-to-end latency from 10s of minutes to just a few seconds. \n",
       " \n",
       " We are leveraging Delta to provide a single query interface across multiple tables of this continuously changing data. This enables data science and analytics workloads to always use the most current and comprehensive information available. In addition to the obvious schema transformations, we implement data-quality metrics and datum conversions to provide a trustworthy unified dataset.</td><td>Title: Leveraging IoT Data at Scale to Mitigate Global Water Risks Using Apache Spark Streaming and Delta\n",
       "                Abstract:  Every year, billions of dollars are lost due to water risks from storms, floods, and droughts. Water data scarcity and excess are issues that risk models cannot overcome, creating a world of uncertainty. Divirod is building a platform of water data by normalizing diverse data sources of varying velocity into one unified data asset. In addition to publicly available third-party datasets, we are rapidly deploying our own IoT sensors. \n",
       " \n",
       " These sensors ingest signals at a rate of about 100,000 messages per hour into preprocessing, signal-processing, analytics, and postprocessing workloads in one spark-streaming pipeline to enable critical real-time decision-making processes. By leveraging streaming architecture, we were able to reduce end-to-end latency from 10s of minutes to just a few seconds. \n",
       " \n",
       " We are leveraging Delta to provide a single query interface across multiple tables of this continuously changing data. This enables data science and analytics workloads to always use the most current and comprehensive information available. In addition to the obvious schema transformations, we implement data-quality metrics and datum conversions to provide a trustworthy unified dataset.</td></tr><tr><td>Unlocking Near Real-Time Data Replication with CDC, Apache Spark Streaming, and Delta Lake</td><td>Tune into DoorDash's journey to migrate from a flaky ETL system with 24-hour data delays, to standardizing a CDC streaming pattern across 150+ databases to produce near real-time data in a scalable, configurable, and reliable manner. During this journey, understand how we use Delta Lake to build a self-serve, read-optimized data lake with data latencies of 15 minutes, whilst reducing operational overhead. Furthermore, understand how certain tradeoffs like conceding to a non-real-time system allow for multiple optimizations but still permit for OLTP query use-cases, and the benefits it provides.</td><td>Title: Unlocking Near Real-Time Data Replication with CDC, Apache Spark Streaming, and Delta Lake\n",
       "                Abstract:  Tune into DoorDash's journey to migrate from a flaky ETL system with 24-hour data delays, to standardizing a CDC streaming pattern across 150+ databases to produce near real-time data in a scalable, configurable, and reliable manner. During this journey, understand how we use Delta Lake to build a self-serve, read-optimized data lake with data latencies of 15 minutes, whilst reducing operational overhead. Furthermore, understand how certain tradeoffs like conceding to a non-real-time system allow for multiple optimizations but still permit for OLTP query use-cases, and the benefits it provides.</td></tr><tr><td>Streaming Schema Drift Discovery and Controlled Mitigation</td><td>\"When creating streaming work loads with Databricks, it can sometimes be difficult to capture and understand the current structure of your source data. For example, what happens if you are ingesting JSON events from a vendor, and the keys are very sparsely populated, or contain dynamic content? Ideally, data engineers want to \"\"lock in\"\" a target schema in order to minimize complexity and maximize performance for known access patterns. What do you do when your data sources just don't cooperate with that vision?</td><td>Title: Streaming Schema Drift Discovery and Controlled Mitigation\n",
       "                Abstract:  \"When creating streaming work loads with Databricks, it can sometimes be difficult to capture and understand the current structure of your source data. For example, what happens if you are ingesting JSON events from a vendor, and the keys are very sparsely populated, or contain dynamic content? Ideally, data engineers want to \"\"lock in\"\" a target schema in order to minimize complexity and maximize performance for known access patterns. What do you do when your data sources just don't cooperate with that vision?</td></tr><tr><td> \n",
       " The first step is to quantify how far your current source data is drifting from your established Delta table. But how? In this talk I will demonstrate a way to capture and visual drift across all of your streaming tables. The next question is</td><td> \"\"Now that I see all of the data I'm missing</td><td>Title:  \n",
       " The first step is to quantify how far your current source data is drifting from your established Delta table. But how? In this talk I will demonstrate a way to capture and visual drift across all of your streaming tables. The next question is\n",
       "                Abstract:   \"\"Now that I see all of the data I'm missing</td></tr><tr><td>Real-Time Streaming Solution for a Call Center Analytics: Business Challenges and Technical Enablement</td><td>A large international client with a business footprint in North America, Europe and Africa, reached out to us with an interest in having a real-time streaming solution designed and implemented for its call center handling incoming and outgoing client calls. The client had a previous bad experience with another vendor, who overpromised and underdelivered on the latency of the streaming solution. The previous vendor delivered an over-complex streaming data pipeline resulting in the data taking over 5 minutes to reach a visualization layer. The client felt that architecture was too complex and involved many services integrated together. Our immediate challenges involved gaining the client's trust and proving that our design and implementation quality would supercede a previous experience. To resolve an immediate challenge of the overly complicated pipeline design, we deployed a Databricks Lakehouse architecture with Azure Databricks at the center of the solution. Our reference archiitecture integrated Genesys Cloud -> App Services -> Event Hub -> Databricks <-> Data Lake -> Power BI. The streaming solution proved to be low latency (seconds) during the POV stage, which led to subsequent productionalization of the pipeline with deployment of jobs, DLTs pipeline, including multi-notebook workflow and business and performance metrics dashboarding relied on by the call center staff for a day-to-day performance monitoring and improvements.</td><td>Title: Real-Time Streaming Solution for a Call Center Analytics: Business Challenges and Technical Enablement\n",
       "                Abstract:  A large international client with a business footprint in North America, Europe and Africa, reached out to us with an interest in having a real-time streaming solution designed and implemented for its call center handling incoming and outgoing client calls. The client had a previous bad experience with another vendor, who overpromised and underdelivered on the latency of the streaming solution. The previous vendor delivered an over-complex streaming data pipeline resulting in the data taking over 5 minutes to reach a visualization layer. The client felt that architecture was too complex and involved many services integrated together. Our immediate challenges involved gaining the client's trust and proving that our design and implementation quality would supercede a previous experience. To resolve an immediate challenge of the overly complicated pipeline design, we deployed a Databricks Lakehouse architecture with Azure Databricks at the center of the solution. Our reference archiitecture integrated Genesys Cloud -> App Services -> Event Hub -> Databricks <-> Data Lake -> Power BI. The streaming solution proved to be low latency (seconds) during the POV stage, which led to subsequent productionalization of the pipeline with deployment of jobs, DLTs pipeline, including multi-notebook workflow and business and performance metrics dashboarding relied on by the call center staff for a day-to-day performance monitoring and improvements.</td></tr><tr><td>Top Mistakes to Avoid in Streaming Applications</td><td>Nowadays, streaming use cases become very important in almost all companies. When you think of streaming, Apache Spark™ Structured Streaming is at the top of the list in the technology. As we are dealing with many customer streaming use cases, we learned many do’s and don’t of the streaming application. I will be discussing the top mistakes to avoid when working with streaming applications with regard to different sources and sinks like DLT, Kafka, Delta, and so on. If you are avoiding these mistakes while architecting/running/restarting the application, then you will avoid the costly mistakes at a later point in time.</td><td>Title: Top Mistakes to Avoid in Streaming Applications\n",
       "                Abstract:  Nowadays, streaming use cases become very important in almost all companies. When you think of streaming, Apache Spark™ Structured Streaming is at the top of the list in the technology. As we are dealing with many customer streaming use cases, we learned many do’s and don’t of the streaming application. I will be discussing the top mistakes to avoid when working with streaming applications with regard to different sources and sinks like DLT, Kafka, Delta, and so on. If you are avoiding these mistakes while architecting/running/restarting the application, then you will avoid the costly mistakes at a later point in time.</td></tr><tr><td>Embracing the Future of Data Engineering: The Serverless, Real-Time Lakehouse in Action</td><td>As we venture into the future of data engineering, streaming and serverless technologies take center stage. In this fun, hands-on, in-depth and interactive session you can learn about the essence of future data engineering today. In this session we will tackle the challenge of processing streaming events created by hundreds of sensors in the conference room from a serverless web app (bring your phone and be a part of it!). The focus is on the system architecture and the involved products. Which Databricks product, capability and settings will be most useful for our scenario? What does streaming really mean and why does it make our life easier? What are the exact benefits of serverless and how much serverless is a particular solution?\n",
       "Leveraging the power of the Databricks Lakehouse Platform, I will demonstrate how to create a streaming data pipeline with delta live tables ingesting data from AWS Kinesis. Further, I’ll utilize advanced Databricks workflows triggers for efficient orchestration and real-time alerts feeding into a real-time dashboard. And since I don’t want you to leave with empty hands - I will use Delta Sharing to share the results of the demo we built with every participant in the room.\n",
       "Join me in this hands-on exploration of cutting-edge data engineering techniques and witness the future in action!</td><td>Title: Embracing the Future of Data Engineering: The Serverless, Real-Time Lakehouse in Action\n",
       "                Abstract:  As we venture into the future of data engineering, streaming and serverless technologies take center stage. In this fun, hands-on, in-depth and interactive session you can learn about the essence of future data engineering today. In this session we will tackle the challenge of processing streaming events created by hundreds of sensors in the conference room from a serverless web app (bring your phone and be a part of it!). The focus is on the system architecture and the involved products. Which Databricks product, capability and settings will be most useful for our scenario? What does streaming really mean and why does it make our life easier? What are the exact benefits of serverless and how much serverless is a particular solution?\n",
       "Leveraging the power of the Databricks Lakehouse Platform, I will demonstrate how to create a streaming data pipeline with delta live tables ingesting data from AWS Kinesis. Further, I’ll utilize advanced Databricks workflows triggers for efficient orchestration and real-time alerts feeding into a real-time dashboard. And since I don’t want you to leave with empty hands - I will use Delta Sharing to share the results of the demo we built with every participant in the room.\n",
       "Join me in this hands-on exploration of cutting-edge data engineering techniques and witness the future in action!</td></tr><tr><td>Building and Managing Data Platform for 13+ PB Delta Lake and 1000’s of Users:  AT&T'S Story</td><td>Data runs AT&T’s business, just like it runs most businesses these days. Data can lead to a greater understanding of a business and when translated correctly into information can provide human and business systems valuable insights to make better decisions. Unique to AT&T is the volume of data we support, how much of our work that is driven by AI and the scale at which data and AI drive value for our customers and stakeholders. Our Cloud migration journey includes making data and AI more accessible to employees throughout AT&T so they can use their deep business expertise to leverage data more easily and rapidly. We always had to balance this data democratization and desire for speed with keeping our data private and secure. We loved the open ecosystem model of Lakehouse that enables data , BI and ML tools to be seamlessly integrated on a single pane arena, it just simplifies the architecture and reduces dependencies between technologies in the cloud. Being clear in our architecture guidelines and patterns was very important to us for our success. We are seeing more interest from our business unit partners and continuing to build the AI as a service capabilities to support more citizen data scientists. To scale up our Lakehouse journey , we built a Databricks center of excellence (CoE) function in AT&T which today has ~1400+ active members , further concentrating existing expertise and resources in ML/AI discipline to collaborate on all things Databricks like technical support, trainings , FAQ’s and best practices to attain and sustain world-class performance and drive business value for AT&T. Join us to learn more about how we process and manage 10+ Petabytes of our network Lakehouse with Delta Lake and Databricks.</td><td>Title: Building and Managing Data Platform for 13+ PB Delta Lake and 1000’s of Users:  AT&T'S Story\n",
       "                Abstract:  Data runs AT&T’s business, just like it runs most businesses these days. Data can lead to a greater understanding of a business and when translated correctly into information can provide human and business systems valuable insights to make better decisions. Unique to AT&T is the volume of data we support, how much of our work that is driven by AI and the scale at which data and AI drive value for our customers and stakeholders. Our Cloud migration journey includes making data and AI more accessible to employees throughout AT&T so they can use their deep business expertise to leverage data more easily and rapidly. We always had to balance this data democratization and desire for speed with keeping our data private and secure. We loved the open ecosystem model of Lakehouse that enables data , BI and ML tools to be seamlessly integrated on a single pane arena, it just simplifies the architecture and reduces dependencies between technologies in the cloud. Being clear in our architecture guidelines and patterns was very important to us for our success. We are seeing more interest from our business unit partners and continuing to build the AI as a service capabilities to support more citizen data scientists. To scale up our Lakehouse journey , we built a Databricks center of excellence (CoE) function in AT&T which today has ~1400+ active members , further concentrating existing expertise and resources in ML/AI discipline to collaborate on all things Databricks like technical support, trainings , FAQ’s and best practices to attain and sustain world-class performance and drive business value for AT&T. Join us to learn more about how we process and manage 10+ Petabytes of our network Lakehouse with Delta Lake and Databricks.</td></tr><tr><td>Data Democratization with Lakehouse: An Open Banking Application Case</td><td>\"Banco Bradesco represents one of the largest companies in the financial sector in Latin America. They have more than 99 million customers, 79 years of history, and a legacy of data distributed in hundreds of on-premises systems. With the spread of data-driven approaches and the growth of cloud computing adoption, we needed to innovate and adapt to new trends and enable an analytical environment with democratized data.  We will show how more than 8 business departments have already engaged in using the Lakehouse exploratory environment, with more than 190 use cases mapped and a multi-bank financial manager. Unlike with on-premises, the cost of each process can be isolated and managed in near real-time, allowing quick responses to cost and budget deviations, while increasing the deployment speed of new features 36 times compared to on-premises. The data is now used and shared safely and easily between different areas and companies of the group. Also, the view of dashboards within Databricks allows panels to be efficiently \"\"prototyped\"\" with real data</td><td>Title: Data Democratization with Lakehouse: An Open Banking Application Case\n",
       "                Abstract:  \"Banco Bradesco represents one of the largest companies in the financial sector in Latin America. They have more than 99 million customers, 79 years of history, and a legacy of data distributed in hundreds of on-premises systems. With the spread of data-driven approaches and the growth of cloud computing adoption, we needed to innovate and adapt to new trends and enable an analytical environment with democratized data.  We will show how more than 8 business departments have already engaged in using the Lakehouse exploratory environment, with more than 190 use cases mapped and a multi-bank financial manager. Unlike with on-premises, the cost of each process can be isolated and managed in near real-time, allowing quick responses to cost and budget deviations, while increasing the deployment speed of new features 36 times compared to on-premises. The data is now used and shared safely and easily between different areas and companies of the group. Also, the view of dashboards within Databricks allows panels to be efficiently \"\"prototyped\"\" with real data</td></tr><tr><td>Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying</td><td>Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks’ dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia’s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies,  including success stories that resulted in significant cost savings and efficiency gain.</td><td>Title: Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying\n",
       "                Abstract:  Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks’ dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia’s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies,  including success stories that resulted in significant cost savings and efficiency gain.</td></tr><tr><td>How RecRoom Processes Billions of Events Per Day With Databricks and RudderStack</td><td>Learn how Rec Room, a fast-growing augmented and virtual reality software startup, is saving 50% of their engineering team's time by using Databricks and RudderStack to power real-time analytics and insights for their 85 million gaming customers. In this session, you will walk through a step-by-step explanation of how Rec Room set up efficient processes for ingestion into their data lakehouse, transformation, reverse-ETL and product analytics. You will also see how Rec Room is using incremental materialization of tables to save costs and establish an uptime of close to 100%.</td><td>Title: How RecRoom Processes Billions of Events Per Day With Databricks and RudderStack\n",
       "                Abstract:  Learn how Rec Room, a fast-growing augmented and virtual reality software startup, is saving 50% of their engineering team's time by using Databricks and RudderStack to power real-time analytics and insights for their 85 million gaming customers. In this session, you will walk through a step-by-step explanation of how Rec Room set up efficient processes for ingestion into their data lakehouse, transformation, reverse-ETL and product analytics. You will also see how Rec Room is using incremental materialization of tables to save costs and establish an uptime of close to 100%.</td></tr><tr><td>Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?</td><td>Rust is a unique language whose traits make it very appealing for data engineering. In this session, we'll walk through the different aspects of the language that make it such a good fit for big data processing including: how it improves performance and how it provides greater safety guarantees and compatibility with a wide range of existing tools that make it well positioned to become a major building block for the future of analytics. \n",
       " \n",
       " We will also take a hands-on look through real code examples at a few emerging technologies built on top of Rust that utilize these capabilities, and learn how to apply them to our modern lakehouse architecture.</td><td>Title: Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?\n",
       "                Abstract:  Rust is a unique language whose traits make it very appealing for data engineering. In this session, we'll walk through the different aspects of the language that make it such a good fit for big data processing including: how it improves performance and how it provides greater safety guarantees and compatibility with a wide range of existing tools that make it well positioned to become a major building block for the future of analytics. \n",
       " \n",
       " We will also take a hands-on look through real code examples at a few emerging technologies built on top of Rust that utilize these capabilities, and learn how to apply them to our modern lakehouse architecture.</td></tr><tr><td>Making the Shift to Application-Driven Intelligence</td><td>In the digital economy, application-driven intelligence delivered against live, real-time data will become a core capability of successful enterprises. It has the potential to improve the experience that you provide to your customers and deepen their engagement. But to make application-driven intelligence a reality, you can no longer rely only on copying live application data out of operational systems into analytics stores.\n",
       " Rather, it takes the unique real-time application-serving layer of a MongoDB database combined with the scale and real-time capabilities of a Databricks Lakehouse to automate and operationalize complex and AI-enhanced applications at scale. In this session we will show how it can be seamless for developers and data scientists to automate decisioning and actions on fresh application data and we'll deliver a practical demonstration on how operational data can be integrated in real time to run complex Machine Learning pipelines.</td><td>Title: Making the Shift to Application-Driven Intelligence\n",
       "                Abstract:  In the digital economy, application-driven intelligence delivered against live, real-time data will become a core capability of successful enterprises. It has the potential to improve the experience that you provide to your customers and deepen their engagement. But to make application-driven intelligence a reality, you can no longer rely only on copying live application data out of operational systems into analytics stores.\n",
       " Rather, it takes the unique real-time application-serving layer of a MongoDB database combined with the scale and real-time capabilities of a Databricks Lakehouse to automate and operationalize complex and AI-enhanced applications at scale. In this session we will show how it can be seamless for developers and data scientists to automate decisioning and actions on fresh application data and we'll deliver a practical demonstration on how operational data can be integrated in real time to run complex Machine Learning pipelines.</td></tr><tr><td>Making Travel More Accessible for Customers Bringing Mobility Devices</td><td>American Airlines takes great pride in caring for customers travel, and recognize the importance of supporting the dignity and independence of everyone who travels with us. As we work to improve the customer experience, we're committed to making our airline more accessible to everyone. Our work to ensure that travel that is accessible to all is well underway. We have been particularly focused on making the journey smoother for customers who rely on wheelchairs or other mobility devices. We have implemented the use of a bag tag specifically for wheelchairs and scooters that gives team members more information, like the mobility device’s weight and battery type, or whether it needs to be returned to a customer before a connecting flight.\n",
       " \n",
       " As a data engineering and analytics team, we at American Airlines are building a passenger service request data product that will provide timely insights on expected mobility device traffic at each airport so that the front-line team members can provide seamless travel experience to the passengers.</td><td>Title: Making Travel More Accessible for Customers Bringing Mobility Devices\n",
       "                Abstract:  American Airlines takes great pride in caring for customers travel, and recognize the importance of supporting the dignity and independence of everyone who travels with us. As we work to improve the customer experience, we're committed to making our airline more accessible to everyone. Our work to ensure that travel that is accessible to all is well underway. We have been particularly focused on making the journey smoother for customers who rely on wheelchairs or other mobility devices. We have implemented the use of a bag tag specifically for wheelchairs and scooters that gives team members more information, like the mobility device’s weight and battery type, or whether it needs to be returned to a customer before a connecting flight.\n",
       " \n",
       " As a data engineering and analytics team, we at American Airlines are building a passenger service request data product that will provide timely insights on expected mobility device traffic at each airport so that the front-line team members can provide seamless travel experience to the passengers.</td></tr><tr><td>Improve Apache Spark DS V2 Query Planning Using Column Stats</td><td>When doing the TPC-DS benchmark using external V2 data source, we have observed that for several of the queries, DS V1 has better join plans than Spark. The main reason is that DS V1 uses column stats, especially number of distinct values (NDV) for query optimization. Currently, Spark DS V2 only has interfaces for data sources to report table statistics such as size in bytes and number of rows. In order to use column stats in DS V2, we have added new interfaces to allow external data sources to report column stats to Spark. For a data source with huge data, it’s always challenging to get the column stats, especially the NDV. We plan to calculate NDV using Apache DataSketches Theta sketch and save the serialized compact sketch in the statistics file. The NDV and other column stats will be reported to Spark for query plan optimization.</td><td>Title: Improve Apache Spark DS V2 Query Planning Using Column Stats\n",
       "                Abstract:  When doing the TPC-DS benchmark using external V2 data source, we have observed that for several of the queries, DS V1 has better join plans than Spark. The main reason is that DS V1 uses column stats, especially number of distinct values (NDV) for query optimization. Currently, Spark DS V2 only has interfaces for data sources to report table statistics such as size in bytes and number of rows. In order to use column stats in DS V2, we have added new interfaces to allow external data sources to report column stats to Spark. For a data source with huge data, it’s always challenging to get the column stats, especially the NDV. We plan to calculate NDV using Apache DataSketches Theta sketch and save the serialized compact sketch in the statistics file. The NDV and other column stats will be reported to Spark for query plan optimization.</td></tr><tr><td>Databricks Powered Enterprise Data Platform for Faster Insights with Optimal Cost</td><td>A leading Australian Bank engaged Deloitte to review architecture of current enterprise data platform enabled using Databricks. Bank’s vision is to limit proliferation of diverse tools/technology while enabling “best of breed” capabilities in the bank’s next-gen data platform. To realize this vision, the enterprise data platform will use various Databricks capabilities: DLT (Delta Live tables), Unity Catalog, Delta Sharing, Databricks Dashboards and Databricks Alerts. Using of these Databricks high performance and scalable capabilities allows the bank to avoid proliferation of disparate tools and reduce a need for custom development or integration. \n",
       " \n",
       " During this session, Deloitte, Databricks and Australian Bank will present how Databricks can be leveraged to develop a robust, resilient and scalable data platform which allows an organization to achieve scaled data sharing, reducing lead time to generate insights via reports, dashboards, AI/ML and Data Sharing.</td><td>Title: Databricks Powered Enterprise Data Platform for Faster Insights with Optimal Cost\n",
       "                Abstract:  A leading Australian Bank engaged Deloitte to review architecture of current enterprise data platform enabled using Databricks. Bank’s vision is to limit proliferation of diverse tools/technology while enabling “best of breed” capabilities in the bank’s next-gen data platform. To realize this vision, the enterprise data platform will use various Databricks capabilities: DLT (Delta Live tables), Unity Catalog, Delta Sharing, Databricks Dashboards and Databricks Alerts. Using of these Databricks high performance and scalable capabilities allows the bank to avoid proliferation of disparate tools and reduce a need for custom development or integration. \n",
       " \n",
       " During this session, Deloitte, Databricks and Australian Bank will present how Databricks can be leveraged to develop a robust, resilient and scalable data platform which allows an organization to achieve scaled data sharing, reducing lead time to generate insights via reports, dashboards, AI/ML and Data Sharing.</td></tr><tr><td>Using Databricks to Power Insights and Visualizations on the S&P Global Marketplace</td><td>In this session, we will explain the visualizations that serve to shorten the time to insight for our prospects and encourage potential buyers to take the next step and request more information from our commercial team. The S&P Global Marketplace is a discovery and exploration platform that enables prospective buyers and clients to easily search fundamental and alternative datasets from across S&P Global and curated third-party providers. It serves as a digital storefront that provides transparency into data coverage and use cases, reducing the time and effort for clients to find data for their needs. A key feature of Marketplace is our interactive data visualizations that provide insight into the coverage of a dataset and demonstrate how the dataset can be used to make more informed decisions.   \n",
       " \n",
       "  \n",
       " \n",
       " The S&P Global Marketplace’s interactive visualizations are displayed in Tableau and are powered by Databricks. The Databricks platform allows for easy integration of S&P Global data and provides a collaborative environment where our team of product managers and data engineers can develop the code to generate each visualization. The team utilizes the web interface to develop the queries that perform the heavy lifting of data transformation instead of performing these tasks in Tableau. The final notebook output is saved into a custom datamart (“golden table”) which is the source for Tableau. We also developed an automated process that refreshes the whole process to ensure Marketplace has up to date visualizations.</td><td>Title: Using Databricks to Power Insights and Visualizations on the S&P Global Marketplace\n",
       "                Abstract:  In this session, we will explain the visualizations that serve to shorten the time to insight for our prospects and encourage potential buyers to take the next step and request more information from our commercial team. The S&P Global Marketplace is a discovery and exploration platform that enables prospective buyers and clients to easily search fundamental and alternative datasets from across S&P Global and curated third-party providers. It serves as a digital storefront that provides transparency into data coverage and use cases, reducing the time and effort for clients to find data for their needs. A key feature of Marketplace is our interactive data visualizations that provide insight into the coverage of a dataset and demonstrate how the dataset can be used to make more informed decisions.   \n",
       " \n",
       "  \n",
       " \n",
       " The S&P Global Marketplace’s interactive visualizations are displayed in Tableau and are powered by Databricks. The Databricks platform allows for easy integration of S&P Global data and provides a collaborative environment where our team of product managers and data engineers can develop the code to generate each visualization. The team utilizes the web interface to develop the queries that perform the heavy lifting of data transformation instead of performing these tasks in Tableau. The final notebook output is saved into a custom datamart (“golden table”) which is the source for Tableau. We also developed an automated process that refreshes the whole process to ensure Marketplace has up to date visualizations.</td></tr><tr><td>Self-Service Geospatial Analysis Leveraging Databricks, Apache Sedona, and R</td><td>Geospatial data analysis is critical to understanding the impact of agricultural operations on environmental sustainability with respect to water quality, soil health, greenhouse gasses, and more. Outside of a few specialized software products, however, support for spatial data types is often limited or missing from analytics and visualization platforms. In this session, we show how Truterra is using Databricks, Apache Sedona, and R to analyze spatial data at scale. Additionally, learn how Truterra uses spatial insights to educate and promote practices that optimize profitability, sustainability, and stewardship outcomes at the farm. In this session, you will see how Databricks and Apache Sedona are used to process large spatial datasets including field, watershed, and hydrologic boundaries. You will see dynamic widgets, SQL and R used in tandem to generate map visuals, display them, and enable download all from a Databricks notebook.</td><td>Title: Self-Service Geospatial Analysis Leveraging Databricks, Apache Sedona, and R\n",
       "                Abstract:  Geospatial data analysis is critical to understanding the impact of agricultural operations on environmental sustainability with respect to water quality, soil health, greenhouse gasses, and more. Outside of a few specialized software products, however, support for spatial data types is often limited or missing from analytics and visualization platforms. In this session, we show how Truterra is using Databricks, Apache Sedona, and R to analyze spatial data at scale. Additionally, learn how Truterra uses spatial insights to educate and promote practices that optimize profitability, sustainability, and stewardship outcomes at the farm. In this session, you will see how Databricks and Apache Sedona are used to process large spatial datasets including field, watershed, and hydrologic boundaries. You will see dynamic widgets, SQL and R used in tandem to generate map visuals, display them, and enable download all from a Databricks notebook.</td></tr><tr><td>Processing Delta Lake Tables on AWS Using AWS Glue, Amazon Athena, and Amazon Redshift</td><td>Delta Lake is an open-source project that helps implement modern data lake architectures commonly built on cloud storages. With Delta Lake, you can achieve ACID transactions, time travel queries, CDC, and other common use cases on the cloud. There are a lot of use cases of Delta tables on AWS. AWS has invested a lot in this technology, and now Delta Lake is available with multiple AWS services, such as AWS Glue Spark jobs, Amazon EMR, Amazon Athena, and Amazon Redshift Spectrum. AWS Glue is a serverless, scalable data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources. With AWS Glue, you can easily ingest data from multiple data sources such as on-prem databases, Amazon RDS, DynamoDB, MongoDB into Delta Lake on Amazon S3 even without expertise in coding.\n",
       " \n",
       " This session will demonstrate how to get started with processing Delta Lake tables on Amazon S3 using AWS Glue, and querying from Amazon Athena, and Amazon Redshift. The session also covers recent AWS service updates related to Delta Lake.</td><td>Title: Processing Delta Lake Tables on AWS Using AWS Glue, Amazon Athena, and Amazon Redshift\n",
       "                Abstract:  Delta Lake is an open-source project that helps implement modern data lake architectures commonly built on cloud storages. With Delta Lake, you can achieve ACID transactions, time travel queries, CDC, and other common use cases on the cloud. There are a lot of use cases of Delta tables on AWS. AWS has invested a lot in this technology, and now Delta Lake is available with multiple AWS services, such as AWS Glue Spark jobs, Amazon EMR, Amazon Athena, and Amazon Redshift Spectrum. AWS Glue is a serverless, scalable data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources. With AWS Glue, you can easily ingest data from multiple data sources such as on-prem databases, Amazon RDS, DynamoDB, MongoDB into Delta Lake on Amazon S3 even without expertise in coding.\n",
       " \n",
       " This session will demonstrate how to get started with processing Delta Lake tables on Amazon S3 using AWS Glue, and querying from Amazon Athena, and Amazon Redshift. The session also covers recent AWS service updates related to Delta Lake.</td></tr><tr><td>A Simple SQL Execution API for the Databricks Lakehouse Architecture</td><td>Given the widespread support of REST architectures, data managed by the Databricks Lakehouse Platform can now be accessed from anywhere, making it easy to build data applications tailored to your needs. You can use the SQL Execution API to build web services-based applications, integrate with various applications and computing devices, create generic integration layers for enterprise services, or create custom client libraries for your programming language of choice.\n",
       " \n",
       " In this talk we are going to deep dive into the design and implementation of the SQL Execution API with a focus on:\n",
       " 1. The key API features which can be used to develop data applications.\n",
       " 2. The underlying Databricks architecture such as Cloud Fetch that we developed to enable this API.</td><td>Title: A Simple SQL Execution API for the Databricks Lakehouse Architecture\n",
       "                Abstract:  Given the widespread support of REST architectures, data managed by the Databricks Lakehouse Platform can now be accessed from anywhere, making it easy to build data applications tailored to your needs. You can use the SQL Execution API to build web services-based applications, integrate with various applications and computing devices, create generic integration layers for enterprise services, or create custom client libraries for your programming language of choice.\n",
       " \n",
       " In this talk we are going to deep dive into the design and implementation of the SQL Execution API with a focus on:\n",
       " 1. The key API features which can be used to develop data applications.\n",
       " 2. The underlying Databricks architecture such as Cloud Fetch that we developed to enable this API.</td></tr><tr><td>Best Exploration of Columnar Shuffle Design</td><td>To significantly improve the performance of Spark SQL, there is a trend to offload Spark SQL execution to highly optimized native libraries or accelerators in past several years, like Photon from Databricks, Nvidia's Rapids plug-in, and Intel & Kyligence's initiated open source Gluten project. By the multi-fold performance improvement from these solutions, more and more Spark users have started to adopt the new technology. One characteristics of native libraries is that they all use columnar data format as the basic data format. It's because the columnar data format has the intrinsic affinity to vectorized data processing using SIMD instructions. While vanilla Spark's shuffle is based on spark's internal row data format. The high overhead of the columnar to row and row to columnar conversion during the shuffle makes reusing current shuffle not possible. Due to the importance of shuffle service in Spark, we have to implement an efficient columnar shuffle, which brings couple of new challenges, like the split of columnar data, or the dictionary support during shuffle.\n",
       " \n",
       " In this session, we will share 1) the exploration process of the columnar shuffle design during our Gazelle and Gluten development, and best practices for implementing the columnar shuffle service. We will also share how we learned from the development of vanilla Spark's shuffle, for example, how to address the small files issue then we will propose the new shuffle solution. We will show the performance comparison between Columnar shuffle and vanilla Spark's row-based shuffle. Finally, we will share how the new built-in accelerators like QAT and IAA in the latest Intel processor are used in our columnar shuffle service and boost the performance.</td><td>Title: Best Exploration of Columnar Shuffle Design\n",
       "                Abstract:  To significantly improve the performance of Spark SQL, there is a trend to offload Spark SQL execution to highly optimized native libraries or accelerators in past several years, like Photon from Databricks, Nvidia's Rapids plug-in, and Intel & Kyligence's initiated open source Gluten project. By the multi-fold performance improvement from these solutions, more and more Spark users have started to adopt the new technology. One characteristics of native libraries is that they all use columnar data format as the basic data format. It's because the columnar data format has the intrinsic affinity to vectorized data processing using SIMD instructions. While vanilla Spark's shuffle is based on spark's internal row data format. The high overhead of the columnar to row and row to columnar conversion during the shuffle makes reusing current shuffle not possible. Due to the importance of shuffle service in Spark, we have to implement an efficient columnar shuffle, which brings couple of new challenges, like the split of columnar data, or the dictionary support during shuffle.\n",
       " \n",
       " In this session, we will share 1) the exploration process of the columnar shuffle design during our Gazelle and Gluten development, and best practices for implementing the columnar shuffle service. We will also share how we learned from the development of vanilla Spark's shuffle, for example, how to address the small files issue then we will propose the new shuffle solution. We will show the performance comparison between Columnar shuffle and vanilla Spark's row-based shuffle. Finally, we will share how the new built-in accelerators like QAT and IAA in the latest Intel processor are used in our columnar shuffle service and boost the performance.</td></tr><tr><td>What Happens When Curious and Knowledgeable Business People are Provided Access to the Right Data and Equipped With Simple Tools and Supported by Guidance From In-House Technical Experts</td><td>Too often business decisions in large organizations are based on time consuming and labor-intensive data extracts, fragile excel or access sheets that require significant manual intervention. The teams that prepare these manual reports have invaluable heuristic knowledge that, when combined with meaningful data and tools, can make smart business decisions. \n",
       " \n",
       " \n",
       " Imagine a world where these business teams are empowered with tools that help them build meaningful reports despite their limited technical expertise.\n",
       " \n",
       " \n",
       " In this talk we will discuss : \n",
       " \n",
       " The value derived from investing in developing citizen data personas within a business organization\n",
       " \n",
       " How we successfully built a citizen data analytics culture within Michelin \n",
       " \n",
       " Real examples of the impact of this initiative on - the business and on the people themselves.\n",
       " \n",
       " The audience will walk away with some convincing arguments for building a citizen data culture in their organization and a how-to cookbook that they can use to cultivate citizen data personas.\n",
       " \n",
       " Finally, they will also have the opportunity to interactively uncover key success factors in the case of Michelin that can help drive a similar initiative in their respective companies.</td><td>Title: What Happens When Curious and Knowledgeable Business People are Provided Access to the Right Data and Equipped With Simple Tools and Supported by Guidance From In-House Technical Experts\n",
       "                Abstract:  Too often business decisions in large organizations are based on time consuming and labor-intensive data extracts, fragile excel or access sheets that require significant manual intervention. The teams that prepare these manual reports have invaluable heuristic knowledge that, when combined with meaningful data and tools, can make smart business decisions. \n",
       " \n",
       " \n",
       " Imagine a world where these business teams are empowered with tools that help them build meaningful reports despite their limited technical expertise.\n",
       " \n",
       " \n",
       " In this talk we will discuss : \n",
       " \n",
       " The value derived from investing in developing citizen data personas within a business organization\n",
       " \n",
       " How we successfully built a citizen data analytics culture within Michelin \n",
       " \n",
       " Real examples of the impact of this initiative on - the business and on the people themselves.\n",
       " \n",
       " The audience will walk away with some convincing arguments for building a citizen data culture in their organization and a how-to cookbook that they can use to cultivate citizen data personas.\n",
       " \n",
       " Finally, they will also have the opportunity to interactively uncover key success factors in the case of Michelin that can help drive a similar initiative in their respective companies.</td></tr><tr><td>Modularized Approach to Scale Suite of Advanced Omnichannel Analytics Products</td><td>GSK started the journey of Omnichannel vision a few years ago using static business rules and customer targeting. We kicked off 2022 with conceptualization of a suite of monitoring, measurement and planning products that would help GSK establish a comprehensive omnichannel measurement strategy and plan across its global markets. One of the key guiding principles during conceptualization was deeply and seamlessly embedding these products within GSK’s data transformation and storage architecture built on Azure Databricks. We built the platform powered extensively by Databricks, GitHub and open-source technologies like Python. In this session, we will cover the following aspects of our journey along with some best practices that we have learned over time.</td><td>Title: Modularized Approach to Scale Suite of Advanced Omnichannel Analytics Products\n",
       "                Abstract:  GSK started the journey of Omnichannel vision a few years ago using static business rules and customer targeting. We kicked off 2022 with conceptualization of a suite of monitoring, measurement and planning products that would help GSK establish a comprehensive omnichannel measurement strategy and plan across its global markets. One of the key guiding principles during conceptualization was deeply and seamlessly embedding these products within GSK’s data transformation and storage architecture built on Azure Databricks. We built the platform powered extensively by Databricks, GitHub and open-source technologies like Python. In this session, we will cover the following aspects of our journey along with some best practices that we have learned over time.</td></tr><tr><td>AI to FI with Databricks</td><td>Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\n",
       " 1. Challenges of common pipeline design to support AI-FI \n",
       " 2. Challenges of leveraging data acquired over many acquisitions\n",
       " 3. How is the Lakehouse paradigm lending itself to solving these challenges?\n",
       " 4. Some cool results we can show the world.\n",
       " \n",
       " To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \n",
       " \n",
       " To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying.</td><td>Title: AI to FI with Databricks\n",
       "                Abstract:  Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\n",
       " 1. Challenges of common pipeline design to support AI-FI \n",
       " 2. Challenges of leveraging data acquired over many acquisitions\n",
       " 3. How is the Lakehouse paradigm lending itself to solving these challenges?\n",
       " 4. Some cool results we can show the world.\n",
       " \n",
       " To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \n",
       " \n",
       " To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying.</td></tr><tr><td>How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse</td><td>Don't miss this session where we demonstrate how the Texas Rangers baseball team organized their predictive models by using MLFlow and the MLRegistry inside Databricks. They started using Databricks as a simple solution to centralizing our development on the cloud. This helped lessen the issue of siloed development in our team, and allowed us to leverage the benefits of distributed cloud computing. But we quickly found that Databricks was a perfect solution to another problem that we faced in our data engineering stack. Specifically, cost, complexity, and scalability issues hampered our data architecture development for years, and we decided we needed to modernize our stack by migrating to a lakehouse. With our lakehouse, ad-hoc-analytics, ETL operations, and MLOps all living within Databricks, development at scale has never been easier for our team. Going forward, we hope to fully eliminate the silos of development, and remove the disconnect between our analytics and data engineering teams. From computer vision, pose analytics, and player tracking, to pitch design, base stealing likelihood, and more, come see how the Texas Rangers are using innovative cloud technologies to create action-driven reports from the current sea of Big Data. </td><td>Title: How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse\n",
       "                Abstract:  Don't miss this session where we demonstrate how the Texas Rangers baseball team organized their predictive models by using MLFlow and the MLRegistry inside Databricks. They started using Databricks as a simple solution to centralizing our development on the cloud. This helped lessen the issue of siloed development in our team, and allowed us to leverage the benefits of distributed cloud computing. But we quickly found that Databricks was a perfect solution to another problem that we faced in our data engineering stack. Specifically, cost, complexity, and scalability issues hampered our data architecture development for years, and we decided we needed to modernize our stack by migrating to a lakehouse. With our lakehouse, ad-hoc-analytics, ETL operations, and MLOps all living within Databricks, development at scale has never been easier for our team. Going forward, we hope to fully eliminate the silos of development, and remove the disconnect between our analytics and data engineering teams. From computer vision, pose analytics, and player tracking, to pitch design, base stealing likelihood, and more, come see how the Texas Rangers are using innovative cloud technologies to create action-driven reports from the current sea of Big Data.</td></tr><tr><td>Improving Hospital Operations With Streaming Data and Real Time AI/ML</td><td>Over the past two years, Providence has developed a robust streaming data platform (SDP) leveraging Databricks in Azure. The SDP enables us to ingest and process real-time data reflecting clinical operations across our 52 hospitals and roughly 1000 ambulatory clinics. HL7 messages generated by Epic are parsed using Databricks in our secure cloud environment and used to generate an up-to-the minute picture of exactly what is happening at the point of care. We are already leveraging this information to minimize hospital overcrowding and have been actively integrating AI/ML to accurately forecast future conditions (e.g., arrivals, length of stay, acuity, and discharge requirements). This allows us to both improve resource utilization (e.g., nurse staffing levels) and to optimize patient throughput. The result is both improved patient care and operational efficiency. In this session we will share how these outcomes are only possible with the power and elegance afforded by our investments in Azure, Databricks, and increasingly Lakehouse. We will demonstrate Providence's blueprint for enabling real-time analytics which can be generalized to other healthcare providers.</td><td>Title: Improving Hospital Operations With Streaming Data and Real Time AI/ML\n",
       "                Abstract:  Over the past two years, Providence has developed a robust streaming data platform (SDP) leveraging Databricks in Azure. The SDP enables us to ingest and process real-time data reflecting clinical operations across our 52 hospitals and roughly 1000 ambulatory clinics. HL7 messages generated by Epic are parsed using Databricks in our secure cloud environment and used to generate an up-to-the minute picture of exactly what is happening at the point of care. We are already leveraging this information to minimize hospital overcrowding and have been actively integrating AI/ML to accurately forecast future conditions (e.g., arrivals, length of stay, acuity, and discharge requirements). This allows us to both improve resource utilization (e.g., nurse staffing levels) and to optimize patient throughput. The result is both improved patient care and operational efficiency. In this session we will share how these outcomes are only possible with the power and elegance afforded by our investments in Azure, Databricks, and increasingly Lakehouse. We will demonstrate Providence's blueprint for enabling real-time analytics which can be generalized to other healthcare providers.</td></tr><tr><td>AI Regulation is Coming: The EU AI Act and How Databricks Can Help with Compliance</td><td>With the heightened attention on LLMs and what they can do, and the widening impact of AI on day-to-day life, the push by regulators across the globe to regulate AI is intensifying. As with GDPR in the privacy realm, the EU is leading the way with the EU Artificial Intelligence Act (AIA). Regulators everywhere will be looking to the AIA as precedent, and understanding the requirements imposed by the AIA is important for all players in the AI channel. Although not finalized, the basic framework regarding how the AIA will work is becoming clearer. The impact on developers and deployers of AI (‘providers’ and ‘users’ under the AIA) will be substantial. Although the AIA will probably not go into effect until early 2025, AI applications developed today will likely be affected, and design and development decisions made now should take the future regulations into account.\n",
       "\n",
       "In this session, Matteo Quattrocchi, Brussels-based Director, Policy – EMEA, for BSA (the Software Alliance – the leading advocacy organization representing the enterprise software sector), will present an overview of the current proposed requirements under the AIA and give an update on the ongoing deliberations and likely timing for enactment. We will also highlight some of the ways the Lakehouse platform, including Managed MLflow, can help providers and users of ML-based applications meet the requirements of the AIA and other upcoming AI regulations.\n",
       "</td><td>Title: AI Regulation is Coming: The EU AI Act and How Databricks Can Help with Compliance\n",
       "                Abstract:  With the heightened attention on LLMs and what they can do, and the widening impact of AI on day-to-day life, the push by regulators across the globe to regulate AI is intensifying. As with GDPR in the privacy realm, the EU is leading the way with the EU Artificial Intelligence Act (AIA). Regulators everywhere will be looking to the AIA as precedent, and understanding the requirements imposed by the AIA is important for all players in the AI channel. Although not finalized, the basic framework regarding how the AIA will work is becoming clearer. The impact on developers and deployers of AI (‘providers’ and ‘users’ under the AIA) will be substantial. Although the AIA will probably not go into effect until early 2025, AI applications developed today will likely be affected, and design and development decisions made now should take the future regulations into account.\n",
       "\n",
       "In this session, Matteo Quattrocchi, Brussels-based Director, Policy – EMEA, for BSA (the Software Alliance – the leading advocacy organization representing the enterprise software sector), will present an overview of the current proposed requirements under the AIA and give an update on the ongoing deliberations and likely timing for enactment. We will also highlight some of the ways the Lakehouse platform, including Managed MLflow, can help providers and users of ML-based applications meet the requirements of the AIA and other upcoming AI regulations.</td></tr><tr><td>Using Databricks to Develop Statistical and Mathematical Models to Forecast the Monkeypox Outbreak in Washington State</td><td>In the spring and summer of 2022, monkeypox was detected in the United States and quickly spread throughout the country. To contain and mitigate the spread of the disease in Washington State, the Washington Department of Health data science team used the Databricks platform to develop a modeling pipeline that employed statistical and mathematical techniques to forecast the course of the monkeypox outbreak throughout the state. These models provided actionable information that helped inform decision making and guide the public health response to the outbreak. \n",
       " \n",
       " We used contact-tracing data, standard line-lists, and published parameters to train a variety of time-series forecasting models, including an ARIMA model, a Poisson regression, and an SEIR compartmental model. We also calculated the daily R-effective rate as an additional output. The compartmental model best fit the reported cases when tested out of sample, but the statistical models were quicker and easier to deploy and helped inform initial decision-making. The R-effective rate was particularly useful throughout the effort. \n",
       " \n",
       " Overall, these efforts highlighted the importance of rapidly deployable and scalable infectious disease modeling pipelines. Public health data science is still a nascent field, however, so common best practices in other industries are often-times novel approaches in public health. The need for stable, generalizable pipelines is crucial. Using the Databricks platform has allowed us to more quickly scale and iteratively improve our modeling pipelines to include other infectious diseases, such as influenza and RSV. Further development of scalable and standardized approaches to disease forecasting at the state and local level is vital to better informing future public health response efforts.</td><td>Title: Using Databricks to Develop Statistical and Mathematical Models to Forecast the Monkeypox Outbreak in Washington State\n",
       "                Abstract:  In the spring and summer of 2022, monkeypox was detected in the United States and quickly spread throughout the country. To contain and mitigate the spread of the disease in Washington State, the Washington Department of Health data science team used the Databricks platform to develop a modeling pipeline that employed statistical and mathematical techniques to forecast the course of the monkeypox outbreak throughout the state. These models provided actionable information that helped inform decision making and guide the public health response to the outbreak. \n",
       " \n",
       " We used contact-tracing data, standard line-lists, and published parameters to train a variety of time-series forecasting models, including an ARIMA model, a Poisson regression, and an SEIR compartmental model. We also calculated the daily R-effective rate as an additional output. The compartmental model best fit the reported cases when tested out of sample, but the statistical models were quicker and easier to deploy and helped inform initial decision-making. The R-effective rate was particularly useful throughout the effort. \n",
       " \n",
       " Overall, these efforts highlighted the importance of rapidly deployable and scalable infectious disease modeling pipelines. Public health data science is still a nascent field, however, so common best practices in other industries are often-times novel approaches in public health. The need for stable, generalizable pipelines is crucial. Using the Databricks platform has allowed us to more quickly scale and iteratively improve our modeling pipelines to include other infectious diseases, such as influenza and RSV. Further development of scalable and standardized approaches to disease forecasting at the state and local level is vital to better informing future public health response efforts.</td></tr><tr><td>Accelerating the Development of Viewership Personas With a Unified Feature Store</td><td>With the proliferation of video content and flourishing consumer demand, there is an enormous opportunity for customer-centric video entertainment companies to use data & analytics to understand what their viewers want and deliver more of the content that that meets their needs.\n",
       " \n",
       " At DIRECTV, our Data Science Center of Excellence is constantly looking to push the boundary of innovation in how we can better and more quickly understand the needs of our customers and leverage those actionable insights to deliver business impact. One way in which we do so is through the development of Viewership Personas with cluster analysis at scale to group our customers by the types of content they enjoy watching. This process is significantly accelerated by a unified feature store which contain a wide array of features that captures key information on viewing preferences.\n",
       " \n",
       " This presentation will focus on how the DIRECTV Data Science team utilizes Databricks to help: (1) develop a unified feature store and (2) how we leverage the feature store to accelerate the process of running machine learning algorithms to find meaningful viewership clusters.</td><td>Title: Accelerating the Development of Viewership Personas With a Unified Feature Store\n",
       "                Abstract:  With the proliferation of video content and flourishing consumer demand, there is an enormous opportunity for customer-centric video entertainment companies to use data & analytics to understand what their viewers want and deliver more of the content that that meets their needs.\n",
       " \n",
       " At DIRECTV, our Data Science Center of Excellence is constantly looking to push the boundary of innovation in how we can better and more quickly understand the needs of our customers and leverage those actionable insights to deliver business impact. One way in which we do so is through the development of Viewership Personas with cluster analysis at scale to group our customers by the types of content they enjoy watching. This process is significantly accelerated by a unified feature store which contain a wide array of features that captures key information on viewing preferences.\n",
       " \n",
       " This presentation will focus on how the DIRECTV Data Science team utilizes Databricks to help: (1) develop a unified feature store and (2) how we leverage the feature store to accelerate the process of running machine learning algorithms to find meaningful viewership clusters.</td></tr><tr><td>How You Can Audit A Language Model</td><td>Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\n",
       " \n",
       " Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\n",
       " \n",
       " * Studying past incidents using the AI Incident Database and using this information to guide debugging.\n",
       " \n",
       " * Finding & fixing common data quality issues.\n",
       " \n",
       " * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\n",
       " \n",
       " * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\n",
       " \n",
       " * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\n",
       " \n",
       " * Testing performance, sentiment & toxicity across different & less common languages.\n",
       " \n",
       " * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\n",
       " \n",
       " * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\n",
       " \n",
       " * Engaging stakeholders to help find problems system designers & developers cannot see.\n",
       " \n",
       " It's now time to figure out how to live with AI, and that means audits, risk management & regulation.</td><td>Title: How You Can Audit A Language Model\n",
       "                Abstract:  Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\n",
       " \n",
       " Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\n",
       " \n",
       " * Studying past incidents using the AI Incident Database and using this information to guide debugging.\n",
       " \n",
       " * Finding & fixing common data quality issues.\n",
       " \n",
       " * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\n",
       " \n",
       " * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\n",
       " \n",
       " * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\n",
       " \n",
       " * Testing performance, sentiment & toxicity across different & less common languages.\n",
       " \n",
       " * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\n",
       " \n",
       " * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\n",
       " \n",
       " * Engaging stakeholders to help find problems system designers & developers cannot see.\n",
       " \n",
       " It's now time to figure out how to live with AI, and that means audits, risk management & regulation.</td></tr><tr><td>JetBlue’s real-time AI & ML digital twin journey using Databricks</td><td>JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \n",
       " \n",
       " The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code.</td><td>Title: JetBlue’s real-time AI & ML digital twin journey using Databricks\n",
       "                Abstract:  JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \n",
       " \n",
       " The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code.</td></tr><tr><td>Intermittent Service Part Demand Forecasting at John Deere</td><td>John Deere customers demand reliability and uptime for their equipment and John Deere dealers must fulfill that need by stocking the right mix of service parts in the right quantities to keep them up and running. John Deere dealers operate on six continents with geographically dispersed customers, products, and equipment. This presents a significant operations and logistics challenge to maintain best-in-industry service levels. Improved demand forecasting enables better retail stocking decisions and inventory planning, however, the variety of equipment serviced, diversity of customer segments, and total number of SKUs at dealer retail locations presents a complex demand forecasting challenge.\n",
       " \n",
       " The John Deere Aftermarket Analytics team took on this opportunity and developed forecasts and stocking recommendations for difficult low volume and intermittent demand service parts that comprise a large percentage of dealers' part sales. Building and iterating the solution required development of data pipelines, exploratory analysis and investigation, model training with multiple R and Python packages, experiment tracking in MLFlow, and model deployment. All of this was enabled with John Deere's Enterprise Data Lake paired with Databricks.\n",
       " \n",
       " In this session, learn about:\n",
       " \n",
       " • Practical approaches for intermittent demand forecasting \n",
       " • Leveraging Workspaces and Repos for efficient team collaboration\n",
       " • Developing scalable data pipelines and features for ML models with Delta Lake\n",
       " • Rapid model experimentation with AutoML and MLFlow\n",
       "  • Job orchestration using Workflows</td><td>Title: Intermittent Service Part Demand Forecasting at John Deere\n",
       "                Abstract:  John Deere customers demand reliability and uptime for their equipment and John Deere dealers must fulfill that need by stocking the right mix of service parts in the right quantities to keep them up and running. John Deere dealers operate on six continents with geographically dispersed customers, products, and equipment. This presents a significant operations and logistics challenge to maintain best-in-industry service levels. Improved demand forecasting enables better retail stocking decisions and inventory planning, however, the variety of equipment serviced, diversity of customer segments, and total number of SKUs at dealer retail locations presents a complex demand forecasting challenge.\n",
       " \n",
       " The John Deere Aftermarket Analytics team took on this opportunity and developed forecasts and stocking recommendations for difficult low volume and intermittent demand service parts that comprise a large percentage of dealers' part sales. Building and iterating the solution required development of data pipelines, exploratory analysis and investigation, model training with multiple R and Python packages, experiment tracking in MLFlow, and model deployment. All of this was enabled with John Deere's Enterprise Data Lake paired with Databricks.\n",
       " \n",
       " In this session, learn about:\n",
       " \n",
       " • Practical approaches for intermittent demand forecasting \n",
       " • Leveraging Workspaces and Repos for efficient team collaboration\n",
       " • Developing scalable data pipelines and features for ML models with Delta Lake\n",
       " • Rapid model experimentation with AutoML and MLFlow\n",
       "  • Job orchestration using Workflows</td></tr><tr><td>Scaling AI Applications With Databricks, HuggingFace and Pinecone</td><td>The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\n",
       " In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution. </td><td>Title: Scaling AI Applications With Databricks, HuggingFace and Pinecone\n",
       "                Abstract:  The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\n",
       " In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution.</td></tr><tr><td>Hyperparameter Tuning Via Apache Spark and Ray</td><td>Model selection through hyperparameter tuning (HPT) is a computationally intensive task. Development of specialized hardware, distributed computation, and modern model selection algorithms has allowed a wide range of potential solutions to be evaluated by the data science community. In this session, you will learn more about how Marks and Spencer tackled this task. In examining the tools, you see that Spark UDF and Ray have received considerable attention from the data science community. Spark UDF allows parallelized execution of custom workloads on distributed datasets in a fault tolerant and scalable manner. Ray is an open-source framework for distributed model selection. It features a well-designed, narrow waist API that seamlessly integrates a wide range of hyperparameter search algorithms. Ray creates trials and controls their execution with the help of a scheduler equipped with well-known HPT algorithms allowing Bayesian optimization and Population-based training. </td><td>Title: Hyperparameter Tuning Via Apache Spark and Ray\n",
       "                Abstract:  Model selection through hyperparameter tuning (HPT) is a computationally intensive task. Development of specialized hardware, distributed computation, and modern model selection algorithms has allowed a wide range of potential solutions to be evaluated by the data science community. In this session, you will learn more about how Marks and Spencer tackled this task. In examining the tools, you see that Spark UDF and Ray have received considerable attention from the data science community. Spark UDF allows parallelized execution of custom workloads on distributed datasets in a fault tolerant and scalable manner. Ray is an open-source framework for distributed model selection. It features a well-designed, narrow waist API that seamlessly integrates a wide range of hyperparameter search algorithms. Ray creates trials and controls their execution with the help of a scheduler equipped with well-known HPT algorithms allowing Bayesian optimization and Population-based training.</td></tr><tr><td>Large Scale Multi-Task Learning Recommender Service at Verizon</td><td>Millions of customers visit the Verizon company website and stores monthly to shop, from plans to products and accessories. Numerous task-specific recommender models have been built at Verizon to enhance customer experience. Motivated by the latest development in deep learning, multi-task learning architectures can better understand customer behavior therefore greatly improve model performance, and also save time to learn new tasks. However, it’s challenging to build such a model that digests heterogeneous data from purchases, online clicks, store visits to customer services and many more, while also  accomplishing various tasks where some of them might contradict with each other. Motivated by the Pathways vision, we built a novel multi-task learning recommender service called Pathways Recommender Service (PaRS), which directly handles multiple abstract forms of data and is able to generalize across multiple recommendation tasks at Verizon. You will see the that we explored built-in bias mitigation in PaRS to promote diverse, relevant and fair recommendations.</td><td>Title: Large Scale Multi-Task Learning Recommender Service at Verizon\n",
       "                Abstract:  Millions of customers visit the Verizon company website and stores monthly to shop, from plans to products and accessories. Numerous task-specific recommender models have been built at Verizon to enhance customer experience. Motivated by the latest development in deep learning, multi-task learning architectures can better understand customer behavior therefore greatly improve model performance, and also save time to learn new tasks. However, it’s challenging to build such a model that digests heterogeneous data from purchases, online clicks, store visits to customer services and many more, while also  accomplishing various tasks where some of them might contradict with each other. Motivated by the Pathways vision, we built a novel multi-task learning recommender service called Pathways Recommender Service (PaRS), which directly handles multiple abstract forms of data and is able to generalize across multiple recommendation tasks at Verizon. You will see the that we explored built-in bias mitigation in PaRS to promote diverse, relevant and fair recommendations.</td></tr><tr><td>Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases</td><td>Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases.</td><td>Title: Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases\n",
       "                Abstract:  Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases.</td></tr><tr><td>International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale</td><td>International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \n",
       "In this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.</td><td>Title: International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale\n",
       "                Abstract:  International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \n",
       "In this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.</td></tr><tr><td>How MLOps on Databricks helped adidas to gain speed in productionising ML projects</td><td>MLOps enables Data science teams to bring ML models into production environments in an efficient manner while making sure that the model pipeline is maintained and model is monitored continuously. When drift is detected, models are retrained promptly and teams are notified with the availability of new version of model. Our team at adidas has created a ML template that uses Databricks, Jenkins and MLflow, which facilitates in the faster deployment process for our ML models in production. We employ a hybrid deployment strategy to minimize compute costs for large models. This presentation will demonstrate how the template was developed, how our teams are utilizing it for model deployment and how it helped us to gain speed to bring or use cases live.</td><td>Title: How MLOps on Databricks helped adidas to gain speed in productionising ML projects\n",
       "                Abstract:  MLOps enables Data science teams to bring ML models into production environments in an efficient manner while making sure that the model pipeline is maintained and model is monitored continuously. When drift is detected, models are retrained promptly and teams are notified with the availability of new version of model. Our team at adidas has created a ML template that uses Databricks, Jenkins and MLflow, which facilitates in the faster deployment process for our ML models in production. We employ a hybrid deployment strategy to minimize compute costs for large models. This presentation will demonstrate how the template was developed, how our teams are utilizing it for model deployment and how it helped us to gain speed to bring or use cases live.</td></tr><tr><td>How We Built a Unified Talent Solution Using Databricks Machine Learning</td><td>Using Databricks, we built a “Unified Talent Solution”, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta’s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data</td><td>Title: How We Built a Unified Talent Solution Using Databricks Machine Learning\n",
       "                Abstract:  Using Databricks, we built a “Unified Talent Solution”, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta’s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data</td></tr><tr><td>Meet LOLA: The Innovation Engine Brewing Models at Scale for AB-InBev</td><td>Today the world's largest brewer, AB-InBev, is disrupting the industry with BEES and LOLA. LOLA is our machine learning platform that enables us to brew models at scale. The foundation of LOLA rests on Databricks Lakehouse's platform and is now being extended globally with Unity Catalog. However, three years ago, things were different. Back then, LOLA was just a simple recommender with less than 60% account coverage in the US. Now it is evolving into an ML platform that supports more models, like our new recommendation engine DEAR, with 95%+ account coverage and integration with BEES in the US.\n",
       "As LOLA evolves, the foundation laid by Lakehouse paved the way for the cornerstone of our platform, our platinum data layer, the Feature Store. The Feature Store is the hub for all feature engineering in LOLA, using Databricks Workflows to support over 250+ production features. But, more importantly, it has become the abstraction layer that helps build out our growing number of ML pipelines.\n",
       "  \n",
       " </td><td>Title: Meet LOLA: The Innovation Engine Brewing Models at Scale for AB-InBev\n",
       "                Abstract:  Today the world's largest brewer, AB-InBev, is disrupting the industry with BEES and LOLA. LOLA is our machine learning platform that enables us to brew models at scale. The foundation of LOLA rests on Databricks Lakehouse's platform and is now being extended globally with Unity Catalog. However, three years ago, things were different. Back then, LOLA was just a simple recommender with less than 60% account coverage in the US. Now it is evolving into an ML platform that supports more models, like our new recommendation engine DEAR, with 95%+ account coverage and integration with BEES in the US.\n",
       "As LOLA evolves, the foundation laid by Lakehouse paved the way for the cornerstone of our platform, our platinum data layer, the Feature Store. The Feature Store is the hub for all feature engineering in LOLA, using Databricks Workflows to support over 250+ production features. But, more importantly, it has become the abstraction layer that helps build out our growing number of ML pipelines.</td></tr><tr><td>Scaling MLOps for a demand forecasting across multiple markets for a large CPG</td><td>In this session, we look at how one of the world’s largest CPG company setup a scalable MLOps pipeline for a Demand Forecasting use case that predicted demand at 100k+ DFUs (demand forecasting units) on a weekly basis across 20+ markets. This implementation resulted in significant cost savings in terms of improved productivity, reduced cloud usage and faster time to value amongst other benefits.\n",
       " The attendees of this session will leave this session with a clearer picture on the following:\n",
       " 1. Best practices in scaling mlops with Databricks and Azure for a demand forecasting use case with a multi-market and multi-region roll-out.\n",
       " 2. Best practices related to model re-factoring and setting up standard CI-CD pipelines for MLOps\n",
       " 3. What are some of the pitfalls to avoid in such scenarios?</td><td>Title: Scaling MLOps for a demand forecasting across multiple markets for a large CPG\n",
       "                Abstract:  In this session, we look at how one of the world’s largest CPG company setup a scalable MLOps pipeline for a Demand Forecasting use case that predicted demand at 100k+ DFUs (demand forecasting units) on a weekly basis across 20+ markets. This implementation resulted in significant cost savings in terms of improved productivity, reduced cloud usage and faster time to value amongst other benefits.\n",
       " The attendees of this session will leave this session with a clearer picture on the following:\n",
       " 1. Best practices in scaling mlops with Databricks and Azure for a demand forecasting use case with a multi-market and multi-region roll-out.\n",
       " 2. Best practices related to model re-factoring and setting up standard CI-CD pipelines for MLOps\n",
       " 3. What are some of the pitfalls to avoid in such scenarios?</td></tr><tr><td>MLOps at Gucci: From Zero to Hero</td><td>In recent years, similar principles to those of DevOps in software development have been applied to Machine Learning projects with the goal of productionizing automated solutions. However, Machine Learning Operations (MLOps) practices have proven to be of difficult implementation, as they often require putting together several different tools in a complex architecture. In this session, we introduce MLOps concepts and describe how we implement MLOps principles in our projects at Gucci by leveraging Databricks functionalities. A use case of deploying a data science tool for supporting media budget allocation decisions is presented. After the development of a POC to experiment and effectively address the business problem, the code was versioned and refactored. Code reviews were executed to ensure quality and readability. Within Databricks, we rapidly moved the project to the production stage by achieving an automated solution including unit tests, environment configuration, registration and versioning of models, monitoring of model performances as well as model serving and a dashboard to visualize results. This template is being successfully replicated for other projects and is allowing our new Data Science team to quickly bring value to different business areas of the company. These best practices and insights can be used as an example of how this can be beneficial to you or other practitioners.</td><td>Title: MLOps at Gucci: From Zero to Hero\n",
       "                Abstract:  In recent years, similar principles to those of DevOps in software development have been applied to Machine Learning projects with the goal of productionizing automated solutions. However, Machine Learning Operations (MLOps) practices have proven to be of difficult implementation, as they often require putting together several different tools in a complex architecture. In this session, we introduce MLOps concepts and describe how we implement MLOps principles in our projects at Gucci by leveraging Databricks functionalities. A use case of deploying a data science tool for supporting media budget allocation decisions is presented. After the development of a POC to experiment and effectively address the business problem, the code was versioned and refactored. Code reviews were executed to ensure quality and readability. Within Databricks, we rapidly moved the project to the production stage by achieving an automated solution including unit tests, environment configuration, registration and versioning of models, monitoring of model performances as well as model serving and a dashboard to visualize results. This template is being successfully replicated for other projects and is allowing our new Data Science team to quickly bring value to different business areas of the company. These best practices and insights can be used as an example of how this can be beneficial to you or other practitioners.</td></tr><tr><td>Streamlining API Deployment for ML Models Across Multiple Brands: Ahold Delhaize's Experience on Serverless</td><td>At Ahold Delhaize, we have 19 local brands. Most of our brands have common goals, such as providing personalized offers to their customers, a better search engine on e-commerce websites, and forecasting models to reduce food waste and ensure availability. As a central team, our goal is to standardize the way of working across all of these brands, including the deployment of machine learning models. To this end, we have adopted Databricks as our standard platform for our batch inference models. \n",
       "\n",
       "However, API deployment for real time inference models remained challenging due to the varying capabilities of our brands. Our attempts to standardize API deployments with different tools failed due to complexity of our organization. Fortunately, Databricks has recently introduced a new feature: serverless API deployment. Since all our brands already use Databricks, this feature was easy to adopt. It allows us to easily reuse API deployment across all of our brands, significantly reducing time to market (from 6-12 months to 1 month), increasing efficiency, and reducing the costs. In this session, you will see the solution architecture, sample use case specifically used to cross-sell model deployed to 4 different brands, and API deployment using Databricks Serverless API with custom model.</td><td>Title: Streamlining API Deployment for ML Models Across Multiple Brands: Ahold Delhaize's Experience on Serverless\n",
       "                Abstract:  At Ahold Delhaize, we have 19 local brands. Most of our brands have common goals, such as providing personalized offers to their customers, a better search engine on e-commerce websites, and forecasting models to reduce food waste and ensure availability. As a central team, our goal is to standardize the way of working across all of these brands, including the deployment of machine learning models. To this end, we have adopted Databricks as our standard platform for our batch inference models. \n",
       "\n",
       "However, API deployment for real time inference models remained challenging due to the varying capabilities of our brands. Our attempts to standardize API deployments with different tools failed due to complexity of our organization. Fortunately, Databricks has recently introduced a new feature: serverless API deployment. Since all our brands already use Databricks, this feature was easy to adopt. It allows us to easily reuse API deployment across all of our brands, significantly reducing time to market (from 6-12 months to 1 month), increasing efficiency, and reducing the costs. In this session, you will see the solution architecture, sample use case specifically used to cross-sell model deployed to 4 different brands, and API deployment using Databricks Serverless API with custom model.</td></tr><tr><td>Monetizing Data Assets: Sharing Data, Models and Features</td><td>Exec Summary:\n",
       " \n",
       " - Data is an asset and selling/sharing data has (largely) been solved\n",
       " - Hosted models exist (example: ChatGPT) but moving sensitive data across the public internet or across clouds is problematic\n",
       " - Sharing features (the result of feature engineering) can be monetized for new potential revenue streams\n",
       " - Sharing models can also be monetized while avoiding the transfer of sensitive data\n",
       " - This presentation will walk through a few examples of how to share models and features to generate new revenue streams using Delta Sharing, MLflow, and Databricks</td><td>Title: Monetizing Data Assets: Sharing Data, Models and Features\n",
       "                Abstract:  Exec Summary:\n",
       " \n",
       " - Data is an asset and selling/sharing data has (largely) been solved\n",
       " - Hosted models exist (example: ChatGPT) but moving sensitive data across the public internet or across clouds is problematic\n",
       " - Sharing features (the result of feature engineering) can be monetized for new potential revenue streams\n",
       " - Sharing models can also be monetized while avoiding the transfer of sensitive data\n",
       " - This presentation will walk through a few examples of how to share models and features to generate new revenue streams using Delta Sharing, MLflow, and Databricks</td></tr><tr><td>An API for DL Inferencing on Spark</td><td>Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization.\n",
       " \n",
       " In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models.</td><td>Title: An API for DL Inferencing on Spark\n",
       "                Abstract:  Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization.\n",
       " \n",
       " In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models.</td></tr><tr><td>DEIMOS - Optimizing Time-Series Modeling at Scale With Applications in CPG</td><td>At the beginning of a forecasting project, data scientists must make several decisions, each of which impacts the success of a project. Should the model be univariate or multivariate? What features warrant the best model inputs? What model type should you choose? Are the best hyperparameters selected? All of these parts of the modeling process can be quite costly in terms of both time and computational resources. In this session, you will see solution we've created called DEIMOS (Deep Experimental Integrated Multivariate Optimized Series). DEIMOS is built to handle many of the critical problems in time-series modeling automatically and at scale. This package is designed to be simple and straightforward to use, with easily interpretable results to both technical and non-technical audiences. Moreover, we’ve designed it to be flexible so that it can be easily applied to many use cases.</td><td>Title: DEIMOS - Optimizing Time-Series Modeling at Scale With Applications in CPG\n",
       "                Abstract:  At the beginning of a forecasting project, data scientists must make several decisions, each of which impacts the success of a project. Should the model be univariate or multivariate? What features warrant the best model inputs? What model type should you choose? Are the best hyperparameters selected? All of these parts of the modeling process can be quite costly in terms of both time and computational resources. In this session, you will see solution we've created called DEIMOS (Deep Experimental Integrated Multivariate Optimized Series). DEIMOS is built to handle many of the critical problems in time-series modeling automatically and at scale. This package is designed to be simple and straightforward to use, with easily interpretable results to both technical and non-technical audiences. Moreover, we’ve designed it to be flexible so that it can be easily applied to many use cases.</td></tr><tr><td>Comparing Databricks and Snowflake for Machine Learning</td><td>Snowflake and Databricks both aim to provide data science toolkits for machine learning workflows, albeit with different approaches and resources. While developing ML models is technically possible using either platform, the Hitachi Solutions Empower team tested which solution will be easier, faster, and cheaper to work with in terms of both user experience and business outcomes for our customers. To do this, we designed and conducted a series of experiments with use cases from the TPCx-AI benchmark standard. We developed both single-node and multi-node versions of these experiments, which sometimes required us to set up separate compute infrastructure outside of the platform, in the case of Snowflake. We also built datasets of various sizes (1GB, 10GB, and 100GB), to assess how each platform/node setup handles scale. Based on our findings. On the average, Databricks is faster, cheaper, and easier to use for developing machine learning models, and we use it exclusively for data science on the Empower platform. Snowflake’s reliance on third party resources for distributed training is a major drawback, and the need to use multiple compute environments to scale up training is complex and, in our view, an unnecessary complication to achieve best results.</td><td>Title: Comparing Databricks and Snowflake for Machine Learning\n",
       "                Abstract:  Snowflake and Databricks both aim to provide data science toolkits for machine learning workflows, albeit with different approaches and resources. While developing ML models is technically possible using either platform, the Hitachi Solutions Empower team tested which solution will be easier, faster, and cheaper to work with in terms of both user experience and business outcomes for our customers. To do this, we designed and conducted a series of experiments with use cases from the TPCx-AI benchmark standard. We developed both single-node and multi-node versions of these experiments, which sometimes required us to set up separate compute infrastructure outside of the platform, in the case of Snowflake. We also built datasets of various sizes (1GB, 10GB, and 100GB), to assess how each platform/node setup handles scale. Based on our findings. On the average, Databricks is faster, cheaper, and easier to use for developing machine learning models, and we use it exclusively for data science on the Empower platform. Snowflake’s reliance on third party resources for distributed training is a major drawback, and the need to use multiple compute environments to scale up training is complex and, in our view, an unnecessary complication to achieve best results.</td></tr><tr><td>Testing Generative AI Models - What You Need to Know</td><td>Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\n",
       " \n",
       " But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\n",
       " \n",
       " In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.</td><td>Title: Testing Generative AI Models - What You Need to Know\n",
       "                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\n",
       " \n",
       " But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\n",
       " \n",
       " In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.</td></tr><tr><td>Simplifying Real-Time Machine Learning: A Look at Feature Platforms and Modern Real-time ML Architectures Using MLflow & Tecton</td><td>Are you struggling to keep up with the demands of real-time machine learning? Like most organizations building real-time ML, you’re probably looking for a better way to: Manage the lifecycle of ML models and features, Implement batch, streaming, and real-time data pipelines, Generate accurate training datasets and Serve models and data online with strict SLAs, supporting millisecond latencies and high query volumes. Look no further! In this session, we will unveil a modern technical architecture that simplifies the process of managing real-time ML models and features. Using MLFlow and Tecton, we’ll show you how to build a robust MLOps platform on Databricks that can easily handle the unique challenges of real-time data processing. Join us to discover how to streamline the lifecycle of ML models and features, implement data pipelines with ease, and generate accurate training datasets with minimal effort. See how to serve models and data online with mission-critical speed and reliability, supporting millisecond latencies and high query volumes. Take a firsthand look at how FanDuel uses this solution to power their real-time ML applications, from responsible gaming to content recommendations and marketing optimization. See for yourself how this system can be used to define features, train models, process streaming data, and serve both models and features online for real-time inference with a live demo. Join us to learn how to build a modern MLOps platform for your real-time ML use cases.</td><td>Title: Simplifying Real-Time Machine Learning: A Look at Feature Platforms and Modern Real-time ML Architectures Using MLflow & Tecton\n",
       "                Abstract:  Are you struggling to keep up with the demands of real-time machine learning? Like most organizations building real-time ML, you’re probably looking for a better way to: Manage the lifecycle of ML models and features, Implement batch, streaming, and real-time data pipelines, Generate accurate training datasets and Serve models and data online with strict SLAs, supporting millisecond latencies and high query volumes. Look no further! In this session, we will unveil a modern technical architecture that simplifies the process of managing real-time ML models and features. Using MLFlow and Tecton, we’ll show you how to build a robust MLOps platform on Databricks that can easily handle the unique challenges of real-time data processing. Join us to discover how to streamline the lifecycle of ML models and features, implement data pipelines with ease, and generate accurate training datasets with minimal effort. See how to serve models and data online with mission-critical speed and reliability, supporting millisecond latencies and high query volumes. Take a firsthand look at how FanDuel uses this solution to power their real-time ML applications, from responsible gaming to content recommendations and marketing optimization. See for yourself how this system can be used to define features, train models, process streaming data, and serve both models and features online for real-time inference with a live demo. Join us to learn how to build a modern MLOps platform for your real-time ML use cases.</td></tr><tr><td>Building a Real-Time Model Monitoring Pipeline on Databricks</td><td>Model deployment is almost never the final step in any ML lifecycle. ML models can degrade over time due to a variety of influencing factors. In this technical deep dive, we will build a real-time ML model monitoring pipeline on Databricks. We need to own and monitor our models for drifts like feature drift, concept drift, distribution drift, and so on. We must constantly monitor the models and issue alerts or trigger retraining when necessary. With so many open source tools and frameworks available, it can be difficult to figure out how to make everything work. In this tutorial, we will create a high-quality Model Monitoring Pipeline. Everything will be built from the ground up using Spark on Databricks. In this session we will introduce a use case in which we set up a model serving pipeline and log the predictions to a stream in real time. We will then configure a model metric monitoring pipeline to consume from the stream and aggregate over specific time windows. Then, to see these metrics live on dashboards, we will integrate a model monitoring visualizing pipeline.</td><td>Title: Building a Real-Time Model Monitoring Pipeline on Databricks\n",
       "                Abstract:  Model deployment is almost never the final step in any ML lifecycle. ML models can degrade over time due to a variety of influencing factors. In this technical deep dive, we will build a real-time ML model monitoring pipeline on Databricks. We need to own and monitor our models for drifts like feature drift, concept drift, distribution drift, and so on. We must constantly monitor the models and issue alerts or trigger retraining when necessary. With so many open source tools and frameworks available, it can be difficult to figure out how to make everything work. In this tutorial, we will create a high-quality Model Monitoring Pipeline. Everything will be built from the ground up using Spark on Databricks. In this session we will introduce a use case in which we set up a model serving pipeline and log the predictions to a stream in real time. We will then configure a model metric monitoring pipeline to consume from the stream and aggregate over specific time windows. Then, to see these metrics live on dashboards, we will integrate a model monitoring visualizing pipeline.</td></tr><tr><td>Using NLP to evaluate 100 Million global webpages daily to contextually target consumers</td><td>This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\n",
       "\n",
       "TTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\n",
       "\n",
       "The Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\n",
       "</td><td>Title: Using NLP to evaluate 100 Million global webpages daily to contextually target consumers\n",
       "                Abstract:  This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\n",
       "\n",
       "TTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\n",
       "\n",
       "The Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.</td></tr><tr><td>How Office Leverages Deep Graph Learning to Improve Productivity Products</td><td>We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale.</td><td>Title: How Office Leverages Deep Graph Learning to Improve Productivity Products\n",
       "                Abstract:  We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale.</td></tr><tr><td>Stable Diffusion: The Future of Generative AI</td><td>Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering.</td><td>Title: Stable Diffusion: The Future of Generative AI\n",
       "                Abstract:  Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering.</td></tr><tr><td>Vector Data Lakes</td><td>Vector databases such as ElasticSearch and Pinecone offer fast ingestion and querying on vector embeddings with ANNs. However, they typically do not decouple compute and storage, making them hard to integrate in production data stacks. Because data storage in these databases is expensive and not easily accessible, data teams typically maintain ETL pipelines to offload historical embedding data to blob stores. When that data needs to be queried, they get loaded back into the vector database in another ETL process. This is reminiscent of loading data from OLTP database to cloud storage, then loading said data into an OLAP warehouse for offline analytics. Recently, “lakehouse” offerings allow direct OLAP querying on cloud storage, removing the need for the second ETL step. The same could be done for embedding data. While embedding storage in blob stores cannot satisfy the high TPS requirements in online settings, we argue it’s sufficient for offline analytics use cases like slicing and dicing data based on embedding clusters. Instead of loading the embedding data back into the vector database for offline analytics, we propose direct processing on embeddings stored in Parquet files in Delta Lake. You will see that offline embedding workloads typically touch a large portion of the stored embeddings without the need for random access. As a result, the workload is entirely bound by network throughput instead of latency, making it quite suitable for blob storage backends. On a test 1 billion vector dataset, ETL into cloud storage takes around 1 hour on a dedicated GPU instance, while batched nearest neighbor search can be done in under one minute with four CPU instances. We believe future “lakehouses” will ship with native support for these embedding workloads.</td><td>Title: Vector Data Lakes\n",
       "                Abstract:  Vector databases such as ElasticSearch and Pinecone offer fast ingestion and querying on vector embeddings with ANNs. However, they typically do not decouple compute and storage, making them hard to integrate in production data stacks. Because data storage in these databases is expensive and not easily accessible, data teams typically maintain ETL pipelines to offload historical embedding data to blob stores. When that data needs to be queried, they get loaded back into the vector database in another ETL process. This is reminiscent of loading data from OLTP database to cloud storage, then loading said data into an OLAP warehouse for offline analytics. Recently, “lakehouse” offerings allow direct OLAP querying on cloud storage, removing the need for the second ETL step. The same could be done for embedding data. While embedding storage in blob stores cannot satisfy the high TPS requirements in online settings, we argue it’s sufficient for offline analytics use cases like slicing and dicing data based on embedding clusters. Instead of loading the embedding data back into the vector database for offline analytics, we propose direct processing on embeddings stored in Parquet files in Delta Lake. You will see that offline embedding workloads typically touch a large portion of the stored embeddings without the need for random access. As a result, the workload is entirely bound by network throughput instead of latency, making it quite suitable for blob storage backends. On a test 1 billion vector dataset, ETL into cloud storage takes around 1 hour on a dedicated GPU instance, while batched nearest neighbor search can be done in under one minute with four CPU instances. We believe future “lakehouses” will ship with native support for these embedding workloads.</td></tr><tr><td>JoinBoost: In-DB ML for Tree-Models</td><td>Data and Machine Learning (ML) are crucial for enterprise operations. Enterprises store data in databases for management and use ML to gain business insights. However, there is a mismatch between the way ML expects data to be organized (a single table) and the way data is organized in databases (a join graph of multiple tables) and leads to inefficiencies when joining and materializing tables. In this session, you will see how we successfully address this issue. We introduce JoinBoost, a lightweight python library that trains tree models (such as random forests and gradient boosting) for join graphs in databases. JoinBoost acts as a query rewriting layer that is compatible with cloud databases, and eliminates the need for costly join materialization.</td><td>Title: JoinBoost: In-DB ML for Tree-Models\n",
       "                Abstract:  Data and Machine Learning (ML) are crucial for enterprise operations. Enterprises store data in databases for management and use ML to gain business insights. However, there is a mismatch between the way ML expects data to be organized (a single table) and the way data is organized in databases (a join graph of multiple tables) and leads to inefficiencies when joining and materializing tables. In this session, you will see how we successfully address this issue. We introduce JoinBoost, a lightweight python library that trains tree models (such as random forests and gradient boosting) for join graphs in databases. JoinBoost acts as a query rewriting layer that is compatible with cloud databases, and eliminates the need for costly join materialization.</td></tr><tr><td>Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP</td><td>In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.</td><td>Title: Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP\n",
       "                Abstract:  In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.</td></tr><tr><td>Data Caching Strategies for Data Analytics and AI</td><td>The increasing popularity of data analytics and artificial intelligence (AI) has led to a dramatic increase in the volume of data being used in these fields, creating a growing need for an enhanced computational capability. Cache plays a crucial role as an accelerator for data and AI computations, but it is important to note that these domains have different data access patterns, requiring different cache strategies. In this session, you will see our observations on data access patterns in the analytical SQL and AI training domains based on practical experience with large-scale systems. We will discuss the evaluation results of various caching strategies for analytical SQL and AI and provide caching recommendations for different use cases. Over the years, we have learned some best practices from big internet companies about the following aspects of our journey: 1. Traffic pattern for analytical SQL and cache strategy recommendation, 2. Traffic pattern for AI training and how we can measure the cache efficiency for different AI training process, 3. Cache capacity planning based on real-time metrics of the working set, and 4. Adaptive caching admission and eviction for uncertain traffic patterns</td><td>Title: Data Caching Strategies for Data Analytics and AI\n",
       "                Abstract:  The increasing popularity of data analytics and artificial intelligence (AI) has led to a dramatic increase in the volume of data being used in these fields, creating a growing need for an enhanced computational capability. Cache plays a crucial role as an accelerator for data and AI computations, but it is important to note that these domains have different data access patterns, requiring different cache strategies. In this session, you will see our observations on data access patterns in the analytical SQL and AI training domains based on practical experience with large-scale systems. We will discuss the evaluation results of various caching strategies for analytical SQL and AI and provide caching recommendations for different use cases. Over the years, we have learned some best practices from big internet companies about the following aspects of our journey: 1. Traffic pattern for analytical SQL and cache strategy recommendation, 2. Traffic pattern for AI training and how we can measure the cache efficiency for different AI training process, 3. Cache capacity planning based on real-time metrics of the working set, and 4. Adaptive caching admission and eviction for uncertain traffic patterns</td></tr><tr><td>If a Duck Quacks in the Forrest and Everyone Hears, Should you Care?</td><td>\"YES! \"\"Duck posting\"\" has become an internet meme for praising DuckDB on twitter. Nearly every quack using DuckDB has done it once or twice. But</td><td>Title: If a Duck Quacks in the Forrest and Everyone Hears, Should you Care?\n",
       "                Abstract:  \"YES! \"\"Duck posting\"\" has become an internet meme for praising DuckDB on twitter. Nearly every quack using DuckDB has done it once or twice. But</td></tr><tr><td>Python with Spark Connect</td><td>PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python.\n",
       " \n",
       " With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today’s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example.\n",
       " \n",
       " In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications.\n",
       " \n",
       " In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what’s next beyond Apache Spark 3.4.</td><td>Title: Python with Spark Connect\n",
       "                Abstract:  PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python.\n",
       " \n",
       " With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today’s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example.\n",
       " \n",
       " In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications.\n",
       " \n",
       " In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what’s next beyond Apache Spark 3.4.</td></tr><tr><td>Databricks Connect powered by Spark Connect: Develop and Debug Spark from any developer tool</td><td>Spark developers want to develop and debug their code using their tools of choice and development best practices while ensuring high-production fidelity on the target remote cluster. However, Spark's driver architecture is monolithic, with no built-in capability to directly connect to a remote Spark cluster from languages other than SQL. This makes it hard to enable such interactive developer experiences from a user’s local IDE of choice. Spark Connect’s decoupled client-server architecture introduces remote connectivity to Spark clusters and with that, enables interactive development experience - Spark and its open ecosystem can be leveraged from everywhere! \n",
       " \n",
       " In this talk, we show how we leverage Spark Connect to build a completely redesigned version of Databricks Connect, a first-class IDE-based developer experience that offers interactive debugging from any IDE. We show how developers can easily ensure consistency between their local and remote environments. We walk the audience through real-live examples of how to locally debug code running on Databrick. We also show how Databricks Connect integrates into the Databricks Visual Studio Code extension for an even better developer experience.</td><td>Title: Databricks Connect powered by Spark Connect: Develop and Debug Spark from any developer tool\n",
       "                Abstract:  Spark developers want to develop and debug their code using their tools of choice and development best practices while ensuring high-production fidelity on the target remote cluster. However, Spark's driver architecture is monolithic, with no built-in capability to directly connect to a remote Spark cluster from languages other than SQL. This makes it hard to enable such interactive developer experiences from a user’s local IDE of choice. Spark Connect’s decoupled client-server architecture introduces remote connectivity to Spark clusters and with that, enables interactive development experience - Spark and its open ecosystem can be leveraged from everywhere! \n",
       " \n",
       " In this talk, we show how we leverage Spark Connect to build a completely redesigned version of Databricks Connect, a first-class IDE-based developer experience that offers interactive debugging from any IDE. We show how developers can easily ensure consistency between their local and remote environments. We walk the audience through real-live examples of how to locally debug code running on Databrick. We also show how Databricks Connect integrates into the Databricks Visual Studio Code extension for an even better developer experience.</td></tr><tr><td>Ray on Spark</td><td>Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale.\n",
       " \n",
       " This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. \n",
       " \n",
       " In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. \n",
       " We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. \n",
       " During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change!\n",
       " We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases.</td><td>Title: Ray on Spark\n",
       "                Abstract:  Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale.\n",
       " \n",
       " This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. \n",
       " \n",
       " In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. \n",
       " We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. \n",
       " During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change!\n",
       " We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases.</td></tr><tr><td>Why Delta Lake is the best storage format for pandas analyses</td><td>pandas analyses are often limited by file formats like CSV and Parquet. CSV doesn't allow for column pruning, which is an important performance optimization. Parquet doesn't allow for critical features like ACID transactions, time travel, and schema enforcement. This talk discusses why Delta Lake is the fastest file format for pandas users and how it provides users with great features.</td><td>Title: Why Delta Lake is the best storage format for pandas analyses\n",
       "                Abstract:  pandas analyses are often limited by file formats like CSV and Parquet. CSV doesn't allow for column pruning, which is an important performance optimization. Parquet doesn't allow for critical features like ACID transactions, time travel, and schema enforcement. This talk discusses why Delta Lake is the fastest file format for pandas users and how it provides users with great features.</td></tr><tr><td>Fine tuning & scaling Hugging Face with Ray AIR</td><td>Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n",
       " \n",
       " The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\n",
       " \n",
       " In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models’ training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n",
       " \n",
       " Key Takeaways: \n",
       " * Python developers and machine learning engineers can use Transformers and scale their language models\n",
       " * Get exposed to Ray AIR’s Python APIs for end-to-end Hugging Face and ML workflow\n",
       " * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads</td><td>Title: Fine tuning & scaling Hugging Face with Ray AIR\n",
       "                Abstract:  Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n",
       " \n",
       " The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\n",
       " \n",
       " In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models’ training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n",
       " \n",
       " Key Takeaways: \n",
       " * Python developers and machine learning engineers can use Transformers and scale their language models\n",
       " * Get exposed to Ray AIR’s Python APIs for end-to-end Hugging Face and ML workflow\n",
       " * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads</td></tr><tr><td>Breaking Barriers with Databricks Lakehouse - How Blackberry is Revolutionizing Cybersecurity Services Worldwide. AI-Driven Cybersecurity that Works Smarter, Not Harder.\n",
       "</td><td>Cybersecurity incidents are costly, and using an Endpoint Detection and Response (EDR) solution enables the detection of cybersecurity incidents as quickly as possible. To effectively detect cybersecurity incidences requires the collection of millions of data points, and the storing/querying of endpoints data presents considerable engineering challenges. This includes quickly moving local data from endpoints to a single table in the cloud and enabling performant querying against it. The need to avoid internal data siloing within BlackBerry was paramount as multiple teams required access to the data to deliver an effective EDR solution for the present and the future. Databricks tooling enabled us to break down our data silos and iteratively improve our EDR pipeline to ingest data faster and reduce querying latency by more than 20% while reducing costs by more than 30%.\n",
       "In this session, BlackBerry Engineers Srinivasa Kanamatha and Justin Lai will share the journey, lessons learned, and the future for collecting, storing, governing, and sharing data from endpoints in Databricks. The result of building EDR using Databricks helped us accelerate the deployment of our data platform.\"\"</td><td>Title: Breaking Barriers with Databricks Lakehouse - How Blackberry is Revolutionizing Cybersecurity Services Worldwide. AI-Driven Cybersecurity that Works Smarter, Not Harder.\n",
       "\n",
       "                Abstract:  Cybersecurity incidents are costly, and using an Endpoint Detection and Response (EDR) solution enables the detection of cybersecurity incidents as quickly as possible. To effectively detect cybersecurity incidences requires the collection of millions of data points, and the storing/querying of endpoints data presents considerable engineering challenges. This includes quickly moving local data from endpoints to a single table in the cloud and enabling performant querying against it. The need to avoid internal data siloing within BlackBerry was paramount as multiple teams required access to the data to deliver an effective EDR solution for the present and the future. Databricks tooling enabled us to break down our data silos and iteratively improve our EDR pipeline to ingest data faster and reduce querying latency by more than 20% while reducing costs by more than 30%.\n",
       "In this session, BlackBerry Engineers Srinivasa Kanamatha and Justin Lai will share the journey, lessons learned, and the future for collecting, storing, governing, and sharing data from endpoints in Databricks. The result of building EDR using Databricks helped us accelerate the deployment of our data platform.\"\"</td></tr><tr><td>Scaling Deep Learning using Delta Lake storage format on Databricks</td><td>Delta Lake is an open-source storage format that can be ideally used for storing large-scale datasets, which can be used for single-node and distributed training of deep learning models. Delta Lake storage format gives deep learning practitioners unique data management capabilities for working with their datasets. \n",
       " The challenge is that, as of now, it’s not possible to use Delta Lake to train PyTorch models directly. PyTorch community has recently introduced a Torchdata library for efficient data loading. This library supports many formats out of the box, but not Delta Lake. This talk will demonstrate using the Delta Lake storage format for single-node and distributed PyTorch training using the torchdata framework and standalone delta-rs Delta Lake implementation.</td><td>Title: Scaling Deep Learning using Delta Lake storage format on Databricks\n",
       "                Abstract:  Delta Lake is an open-source storage format that can be ideally used for storing large-scale datasets, which can be used for single-node and distributed training of deep learning models. Delta Lake storage format gives deep learning practitioners unique data management capabilities for working with their datasets. \n",
       " The challenge is that, as of now, it’s not possible to use Delta Lake to train PyTorch models directly. PyTorch community has recently introduced a Torchdata library for efficient data loading. This library supports many formats out of the box, but not Delta Lake. This talk will demonstrate using the Delta Lake storage format for single-node and distributed PyTorch training using the torchdata framework and standalone delta-rs Delta Lake implementation.</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse",
         "Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\n \n Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \n \n These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\n - Modular and composable code\n - Unit testing framework\n - Incremental event processing with spark structured streaming \n - Granular resource tuning for better performance and cost efficacy\n \n Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse.",
         "Title: Nebula: The Journey of Scaling Instacart’s Data Pipelines with Apache Spark™ and Lakehouse\n                Abstract:  Instacart has gone through immense growth during the pandemic and the trend continues. Instacart ads is no exception in this growth story. We have launched many new product lines including display and video ads covering the full advertising funnel to address the increasing demand of our retail partners. We have built advanced models to auto-suggest optimal bidding to increase the ROI for our CPG partners. Advertisers’ trust is the utmost priority and thus the quest to build a top-class ads measurement platform.\n \n Ads data processing requires complex data verifications to update ads serving stats. In ETL pipelines these were implemented through files containing thousands of lines of raw SQL which were hard to scale, test, and iterate upon. Our data engineers used to spend hours testing small changes due to a lack of local testing mechanisms. \n \n These pain points stress our need for better tools. After some research, we chose Apache Spark™ as our preferred tool to rebuild ETLs, and the Databricks platform made this move easier. In this presentation, I will share our journey to move our pipelines to Spark and Delta Lake on Databricks. With spark, scala, and delta we solved many problems which were slowing the team’s productivity. Some key areas I will cover include:\n - Modular and composable code\n - Unit testing framework\n - Incremental event processing with spark structured streaming \n - Granular resource tuning for better performance and cost efficacy\n \n Other than the domain business logic, the problems discussed here are quite common for performing data processing at scale. I would be glad to have the opportunity to share our learning and am hopeful that it will benefit others who are going through similar growth challenges or migrating to Lakehouse."
        ],
        [
         "Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System",
         "Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community.",
         "Title: Satellite Imaginary Data Processing Using Apache Spark™ and H3 Geospatial Indexing System\n                Abstract:  Agriculture is a complex ecosystem. Understanding Ag data around soil metrics, weather and historical crop production is a key to adopt sustainable practices which help farmers to enhance their profitability and soil health. As these datasets are huge and disparate in nature, finding a standard unit of analysis and bringing all the data to a common granularity is challenging. By leveraging the distributed data processing capabilities of Apache Spark™ and h3 geospatial indexing system, created a hexagonal grid and mapped all the data sets using h3 index. This gave us an ability to join all the datasets together and helped us in deriving more insights. This session will share our learnings and experiences with the Spark community."
        ],
        [
         "From Snowflake to Enterprise-Scale Apache Spark™",
         "Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers.\n \n The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™  solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs.\n \n In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to:\n \n * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation\n * Handle X100 queries per second on a single Spark application with sub-second query latency\n * Protect Spark application from misbehaving users \n * Optimize SQL-based queries",
         "Title: From Snowflake to Enterprise-Scale Apache Spark™\n                Abstract:  Akamai mPulse is a real user monitoring (RUM) solution that delivers real-time web performance analytics to Akamai customers through dashboards, alerting, reporting and data science. The architecture of mPulse relies on a combination of public and private cloud-based services, such as Amazon AWS, Microsoft Azure and the Snowflake data warehouse. Snowflake has provided the core data warehousing needs as the product has grown at scale along with Akamai’s customers.\n \n The engineering team at mPulse has been re-architecting the system to migrate away from Snowflake to an internal enterprise-scale Apache Spark™  solution that Akamai has been developing in-house to improve performance and save on cost. In the first half of the talk, we’ll discuss how the mPulse team made the decision to migrate, the challenges we’ve seen and how Spark is suiting the product's needs.\n \n In the second half of the talk, we’ll discuss the details of the Spark-based infrastructure. Akamai data warehouse (a.k.a Asgard) is a Spark-based solution running on the Azure cloud. We will describe the internal and unique technologies and characteristics of the solution that enable it to outperform Snowflake's offering both from a cost and performance perspective. We will share our experience on how to:\n \n * Run Spark on K8s at scale while supporting multi-tenancy and resource isolation\n * Handle X100 queries per second on a single Spark application with sub-second query latency\n * Protect Spark application from misbehaving users \n * Optimize SQL-based queries"
        ],
        [
         "The Future of Data Orchestration: Asset-Based Orchestration",
         "Data orchestration is a core component for any batch data processing platform and we’ve been using patterns that haven't changed since the 1980s. \n \n In this session, I’ll be introducing a new pattern and way of thinking for data orchestration known as asset-based orchestration, with data freshness sensors to trigger pipelines. I will demo this new pattern using popular tools of the modern data stack - dbt, airbyte, dagster, and databricks.",
         "Title: The Future of Data Orchestration: Asset-Based Orchestration\n                Abstract:  Data orchestration is a core component for any batch data processing platform and we’ve been using patterns that haven't changed since the 1980s. \n \n In this session, I’ll be introducing a new pattern and way of thinking for data orchestration known as asset-based orchestration, with data freshness sensors to trigger pipelines. I will demo this new pattern using popular tools of the modern data stack - dbt, airbyte, dagster, and databricks."
        ],
        [
         "Photon for Dummies: How Does this New Execution Engine Actually Work?",
         "Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it.\n \n If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. \n \n Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as:\n - Why is a Java Virtual Machine the current bottleneck for spark enhancements?\n - What does vectorized even mean? And how was it done before?\n - Why is the relationship status between Spark and Photon 'It’s complicated'?\n \n In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. \n \n I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to. \n \n ",
         "Title: Photon for Dummies: How Does this New Execution Engine Actually Work?\n                Abstract:  Did you finish the Photon whitepaper and think, wait, what? I know I did. Unfortunately, it’s my job to understand it, explain it, and then use it.\n \n If your role involves using Apache Spark™ on Databricks, then you need to know about Photon and where to use it. \n \n Join me, chief dummy, nay *supreme* dummy, as I break down this whitepaper into easy to understand explanations that don’t require a computer science degree. Together we will unravel mysteries such as:\n - Why is a Java Virtual Machine the current bottleneck for spark enhancements?\n - What does vectorized even mean? And how was it done before?\n - Why is the relationship status between Spark and Photon 'It’s complicated'?\n \n In this seession, we’ll start with the basics of Apache Spark, the details we pretend to know and where those performance cracks were starting to show through. Only then will we start to look at Photon, how it’s different, where the clever design choices are and how you can make the most of this in your own workloads. \n \n I’ve spent over 50 hours going over the paper in excruciating detail, every reference, and in some instances, the references of the references so that you don’t have to."
        ],
        [
         "Monitoring Delta Live Tables",
         "In this session we will share how Volvo Group monitors Databricks jobs and DLTs to stay ahead of the curve. Volvo Group uses Databricks for - amongst others - material planning. Based on live input from warehouse systems, the system predicts which orders to place with the suppliers to ensure the availability of spare parts and keep the trucks running. Since this is a critical component, with reverse ETL feeding the recommendation back into the warehouse systems, strict monitoring to avoid pipeline congestion is required. In this talk I will:\n * Explain a vision of data quality (deliver trustworthy data in time); which metrics to set up to monitor data quality\n * How it helps Volvo Group; make the SLA, jointly use with FinOps to improve ingestion pipelines, avoid congestion\n * Setup; use Jobs API and Databricks SQL workspace to build and consult the monitoring dashboard\n * Demo\n * Get notebook from GitHub",
         "Title: Monitoring Delta Live Tables\n                Abstract:  In this session we will share how Volvo Group monitors Databricks jobs and DLTs to stay ahead of the curve. Volvo Group uses Databricks for - amongst others - material planning. Based on live input from warehouse systems, the system predicts which orders to place with the suppliers to ensure the availability of spare parts and keep the trucks running. Since this is a critical component, with reverse ETL feeding the recommendation back into the warehouse systems, strict monitoring to avoid pipeline congestion is required. In this talk I will:\n * Explain a vision of data quality (deliver trustworthy data in time); which metrics to set up to monitor data quality\n * How it helps Volvo Group; make the SLA, jointly use with FinOps to improve ingestion pipelines, avoid congestion\n * Setup; use Jobs API and Databricks SQL workspace to build and consult the monitoring dashboard\n * Demo\n * Get notebook from GitHub"
        ],
        [
         "Data Quality: Fast and Slow",
         "Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \n \n In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\n \n This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical.",
         "Title: Data Quality: Fast and Slow\n                Abstract:  Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \n \n In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\n \n This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical."
        ],
        [
         "Taking Your Cloud Vendor to the Next Level: Solving Complex Challenges with Azure Databricks",
         "Akamai's content delivery network (CDN) processes about 30% of the internet's daily traffic, resulting in a massive amount of data that presents engineering challenges, both internally and with cloud vendors. In this session, we will discuss the barriers faced while building a data infrastructure on Azure, Databricks, and Kafka to meet strict SLAs, hitting the limits of some of our cloud vendors’ services. We will describe the iterative process of re-architecting a massive-scale data platform using the aforementioned technologies. We will also delve into how today, Akamai is able to quickly ingest and make available to customers TBs of data, as well as efficiently query PBs of data and return results within 10 seconds for most queries. This discussion will provide valuable insights for attendees and organizations seeking to effectively process and analyze large amounts of data.",
         "Title: Taking Your Cloud Vendor to the Next Level: Solving Complex Challenges with Azure Databricks\n                Abstract:  Akamai's content delivery network (CDN) processes about 30% of the internet's daily traffic, resulting in a massive amount of data that presents engineering challenges, both internally and with cloud vendors. In this session, we will discuss the barriers faced while building a data infrastructure on Azure, Databricks, and Kafka to meet strict SLAs, hitting the limits of some of our cloud vendors’ services. We will describe the iterative process of re-architecting a massive-scale data platform using the aforementioned technologies. We will also delve into how today, Akamai is able to quickly ingest and make available to customers TBs of data, as well as efficiently query PBs of data and return results within 10 seconds for most queries. This discussion will provide valuable insights for attendees and organizations seeking to effectively process and analyze large amounts of data."
        ],
        [
         "Unleashing the Power of Interactive Analytics at Scale with Databricks and Delta Lake: Lessons Learned from Building Akamai's Web Security Analytics Product",
         "Akamai is a leading content delivery network (CDN) and cybersecurity company, operating hundreds of thousands of servers in more than 135 countries worldwide.\n In this talk, we will share our experiences and lessons learned from building and maintaining the Web Security Analytics (WSA) product, an interactive analytics platform powered by Databricks and Delta Lake, that enables customers to efficiently analyze and take informed action on a high volume of streaming security events. \n \n The WSA platform must be able to serve hundreds of queries per minute, scanning hundreds of terabytes of data from a six petabyte data lake, with most queries returning results within ten seconds (for both aggregation queries and needle in a haystack queries).\n The talk will cover how to use Databricks SQL warehouses and job clusters cost-effectively, and how to improve query performance through the use of tools and techniques such as Delta Lake, Databricks Photon, and partitioning. \n This talk will be valuable for anyone looking to build and operate a high-performance analytics platform.",
         "Title: Unleashing the Power of Interactive Analytics at Scale with Databricks and Delta Lake: Lessons Learned from Building Akamai's Web Security Analytics Product\n                Abstract:  Akamai is a leading content delivery network (CDN) and cybersecurity company, operating hundreds of thousands of servers in more than 135 countries worldwide.\n In this talk, we will share our experiences and lessons learned from building and maintaining the Web Security Analytics (WSA) product, an interactive analytics platform powered by Databricks and Delta Lake, that enables customers to efficiently analyze and take informed action on a high volume of streaming security events. \n \n The WSA platform must be able to serve hundreds of queries per minute, scanning hundreds of terabytes of data from a six petabyte data lake, with most queries returning results within ten seconds (for both aggregation queries and needle in a haystack queries).\n The talk will cover how to use Databricks SQL warehouses and job clusters cost-effectively, and how to improve query performance through the use of tools and techniques such as Delta Lake, Databricks Photon, and partitioning. \n This talk will be valuable for anyone looking to build and operate a high-performance analytics platform."
        ],
        [
         "ABN Story: Migrating to Future Proof Data Platform",
         "ABN AMRO Bank is one of the top leading banks in the Netherlands; it is the 3rd largest bank in the Netherlands by revenue and number when it comes to mortgages within the Netherlands. We have an objective to become a fully data-driven bank and this goal is well supported by our top management.\n \n ABN AMRO started its data journey almost seven years ago and has built a data platform off-premises with Hadoop technologies. This data platform has been used by more than 200 data providers, 150 data consumers, and overall more than 3000 Datasets.\n \n To become a fully digital bank and address the limitation of the on-premises platform, we needed a future-proof data platform DIAL (digital integration and access layer.) ABN AMRO decided to build an Azure cloud-native data platform with the help of Microsoft and Databricks. Last year this cloud-native platform was ready for our data providers and data consumers. Six months ago we started the journey of migrating all the content from the on-premises data platform to the Azure data platform, this was a very large-scale migration and was achieved in 6 months.\n \n In this session, we will focus on two things :\n 1. Share our strategy for migration from on-premises to a cloud-native platform. \n 2. Share how we used Databricks products in our data platform and how the Databricks team helped us in the overall migration.\n \n ",
         "Title: ABN Story: Migrating to Future Proof Data Platform\n                Abstract:  ABN AMRO Bank is one of the top leading banks in the Netherlands; it is the 3rd largest bank in the Netherlands by revenue and number when it comes to mortgages within the Netherlands. We have an objective to become a fully data-driven bank and this goal is well supported by our top management.\n \n ABN AMRO started its data journey almost seven years ago and has built a data platform off-premises with Hadoop technologies. This data platform has been used by more than 200 data providers, 150 data consumers, and overall more than 3000 Datasets.\n \n To become a fully digital bank and address the limitation of the on-premises platform, we needed a future-proof data platform DIAL (digital integration and access layer.) ABN AMRO decided to build an Azure cloud-native data platform with the help of Microsoft and Databricks. Last year this cloud-native platform was ready for our data providers and data consumers. Six months ago we started the journey of migrating all the content from the on-premises data platform to the Azure data platform, this was a very large-scale migration and was achieved in 6 months.\n \n In this session, we will focus on two things :\n 1. Share our strategy for migration from on-premises to a cloud-native platform. \n 2. Share how we used Databricks products in our data platform and how the Databricks team helped us in the overall migration."
        ],
        [
         "Simon + Denny Live: Ask Us Anything",
         "Simon and Denny have been discussing and debating all things Delta, Lakehouse and Apache Spark™ on their regular webshow. Whether you want advice on lake structures, want to hear their opinions on the latest trends and hype in the data world, or you simply have a tech implementation question to throw at two seasoned experts, these two will have something to say on the matter. In their previous shows, Simon and Denny focused on building out a sample lakehouse architecture, refactoring and tinkering as new features came out, but now we're throwing the doors open for any and every question you might have.\n \n So if you've had a niggling question and these two can help, this is the session for you. There will be a question submission form shared prior to the event, so the team will be prepped with a whole bunch of topics to talk through. Simon and Denny want to hear YOUR questions, which they can field from a wealth of industry experience, wide ranging community engagement and their differing perspectives as external consultant and internal Databricks respectively. There's also a chance they'll get distracted and go way off track talking about coffee, sci-fi, nerdery or the English weather. It happens.",
         "Title: Simon + Denny Live: Ask Us Anything\n                Abstract:  Simon and Denny have been discussing and debating all things Delta, Lakehouse and Apache Spark™ on their regular webshow. Whether you want advice on lake structures, want to hear their opinions on the latest trends and hype in the data world, or you simply have a tech implementation question to throw at two seasoned experts, these two will have something to say on the matter. In their previous shows, Simon and Denny focused on building out a sample lakehouse architecture, refactoring and tinkering as new features came out, but now we're throwing the doors open for any and every question you might have.\n \n So if you've had a niggling question and these two can help, this is the session for you. There will be a question submission form shared prior to the event, so the team will be prepped with a whole bunch of topics to talk through. Simon and Denny want to hear YOUR questions, which they can field from a wealth of industry experience, wide ranging community engagement and their differing perspectives as external consultant and internal Databricks respectively. There's also a chance they'll get distracted and go way off track talking about coffee, sci-fi, nerdery or the English weather. It happens."
        ],
        [
         "Sometimes Less is More: A Deep Dive into the Evolution of a Large, Complex, Real-Time Data Pipeline",
         "This session will dive into technical detail around one of my complex cybersecurity endpoint detection and response (EDR) data pipelines - which spans multiple industries - that is being joined with an indicator of compromise (IoC) feed. I'll detail the stages we went through in the initial evolution of the pipeline and why it ultimately made more sense to apply less schema structure to it and focus on using a non-obvious partitioning strategy when working with this data source. All code is in Python/Pyspark and uses Delta Live Tables.",
         "Title: Sometimes Less is More: A Deep Dive into the Evolution of a Large, Complex, Real-Time Data Pipeline\n                Abstract:  This session will dive into technical detail around one of my complex cybersecurity endpoint detection and response (EDR) data pipelines - which spans multiple industries - that is being joined with an indicator of compromise (IoC) feed. I'll detail the stages we went through in the initial evolution of the pipeline and why it ultimately made more sense to apply less schema structure to it and focus on using a non-obvious partitioning strategy when working with this data source. All code is in Python/Pyspark and uses Delta Live Tables."
        ],
        [
         "Use Apache Spark™  from Anywhere: Remote connectivity with Spark Connect",
         "Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark™. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\n \n However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\n \n Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages.\n \n This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere.",
         "Title: Use Apache Spark™  from Anywhere: Remote connectivity with Spark Connect\n                Abstract:  Over the past decade, developers, researchers, and the community at large have successfully built tens of thousands of data applications using Apache Spark™. Since then, use cases and requirements of data applications have evolved. Today, every application, from web services that run in application servers, interactive environments such as notebooks and IDEs, to phones and edge devices such as smart home devices, want to leverage the power of data.\n \n However, Spark's driver architecture is monolithic, running client applications on top of a scheduler, optimizer and analyzer. This architecture makes it hard to address these new requirements as there is no built-in capability to remotely connect to a Spark cluster from languages other than SQL.\n \n Spark Connect introduces a decoupled client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications, in IDEs, notebooks and programming languages.\n \n This session highlights how simple it is to connect to Spark using Spark Connect from any data applications or IDEs. We will do a deep dive into the architecture of Spark Connect and provide an outlook on how the community can participate in the extension of Spark Connect for new programming languages and frameworks bringing the power of Spark everywhere."
        ],
        [
         "Seven Things You Didn't Know You Can Do with Databricks Workflows",
         "Databricks workflows has come a long way since the initial days of orchestrating simple notebooks and jar/wheel files. Now we can orchestrate multi-task jobs and create a chain of tasks with lineage and DAG with either fan-in or fan-out among multiple other patterns or even run another Databricks job directly inside another job. Databricks workflows takes its tag: “orchestrate anything anywhere” pretty seriously and is a truly fully-managed, cloud-native orchestrator to orchestrate diverse workloads like Delta Live Tables, SQL, Notebooks, Jars, Python Wheels, dbt, SQL, Apache Spark™, ML pipelines with excellent monitoring, alerting and observability capabilities as well. Basically it is a one-stop product for all orchestration needs for an efficient Lakehouse. And what is even better is, it gives full flexibility of running your jobs in a cloud-agnostic and cloud-independent way and is available across AWS, Azure and GCP. In this talk we will discuss and deep dive on some of the very interesting features and will showcase end-to-end demos of the features which will allow you to take full advantage of Databricks workflows for orchestrating the Lakehouse.",
         "Title: Seven Things You Didn't Know You Can Do with Databricks Workflows\n                Abstract:  Databricks workflows has come a long way since the initial days of orchestrating simple notebooks and jar/wheel files. Now we can orchestrate multi-task jobs and create a chain of tasks with lineage and DAG with either fan-in or fan-out among multiple other patterns or even run another Databricks job directly inside another job. Databricks workflows takes its tag: “orchestrate anything anywhere” pretty seriously and is a truly fully-managed, cloud-native orchestrator to orchestrate diverse workloads like Delta Live Tables, SQL, Notebooks, Jars, Python Wheels, dbt, SQL, Apache Spark™, ML pipelines with excellent monitoring, alerting and observability capabilities as well. Basically it is a one-stop product for all orchestration needs for an efficient Lakehouse. And what is even better is, it gives full flexibility of running your jobs in a cloud-agnostic and cloud-independent way and is available across AWS, Azure and GCP. In this talk we will discuss and deep dive on some of the very interesting features and will showcase end-to-end demos of the features which will allow you to take full advantage of Databricks workflows for orchestrating the Lakehouse."
        ],
        [
         "Using DMS and DLT for Change Data Capture ",
         "[Abstract TBD]",
         "Title: Using DMS and DLT for Change Data Capture \n                Abstract:  [Abstract TBD]"
        ],
        [
         "Deep Dive into the New Features of Apache Spark™  3.4",
         "In 2022, Apache Spark™ was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. \n \n We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling.",
         "Title: Deep Dive into the New Features of Apache Spark™  3.4\n                Abstract:  In 2022, Apache Spark™ was awarded the prestigious SIGMOD Systems Award, because Spark is the de facto standard for data processing. In this talk, we want to share the latest progress in Apache Spark community. With tremendous contribution from the open-source community, Spark 3.4 managed to resolve in excess of 2,400 Jira tickets. \n \n We will talk about the major features and improvements in Spark 3.4. The major updates are Spark Connect, numerous PySpark and SQL language features, engine performance enhancements, as well as operational improvements in Spark UX and error handling."
        ],
        [
         "Delta Live Tables: A Modern Approach to Data Pipelines",
         "Data engineers have the difficult task of cleansing complex, diverse data, and transforming it into a usable source to drive data analytics, data science, and machine learning. They need to know the data infrastructure platform in depth, build complex queries in various languages and stitch them together for production. Join this session to learn how Delta Live Tables (DLT) simplifies the complexity of data transformation and ETL. DLT is the first ETL framework to use modern software engineering practices to deliver reliable and trusted data pipelines at any scale. Discover how analysts and data engineers can innovate rapidly with simple pipeline development and maintenance, how to remove operational complexity by automating administrative tasks and gaining visibility into pipeline operations, how built-in quality controls and monitoring ensure accurate BI, data science, and ML, and how simplified batch and streaming can be implemented with self-optimizing and auto-scaling data pipelines.",
         "Title: Delta Live Tables: A Modern Approach to Data Pipelines\n                Abstract:  Data engineers have the difficult task of cleansing complex, diverse data, and transforming it into a usable source to drive data analytics, data science, and machine learning. They need to know the data infrastructure platform in depth, build complex queries in various languages and stitch them together for production. Join this session to learn how Delta Live Tables (DLT) simplifies the complexity of data transformation and ETL. DLT is the first ETL framework to use modern software engineering practices to deliver reliable and trusted data pipelines at any scale. Discover how analysts and data engineers can innovate rapidly with simple pipeline development and maintenance, how to remove operational complexity by automating administrative tasks and gaining visibility into pipeline operations, how built-in quality controls and monitoring ensure accurate BI, data science, and ML, and how simplified batch and streaming can be implemented with self-optimizing and auto-scaling data pipelines."
        ],
        [
         "Journey Towards Uniting Metastores",
         "This session will provide a brief overview about Nationwide’s journey towards implementing Unity Catalog at an Enterprise Level. We will cover the following topics.\n - Identity management structure\n - Compute framework\n - Naming standards and usage best practices\n - And a little bit about how Delta Sharing will help us ingest 3rd party data\n Unity Catalog has been a core feature towards strengthening Data Lakehouse architecture for multiple business units.",
         "Title: Journey Towards Uniting Metastores\n                Abstract:  This session will provide a brief overview about Nationwide’s journey towards implementing Unity Catalog at an Enterprise Level. We will cover the following topics.\n - Identity management structure\n - Compute framework\n - Naming standards and usage best practices\n - And a little bit about how Delta Sharing will help us ingest 3rd party data\n Unity Catalog has been a core feature towards strengthening Data Lakehouse architecture for multiple business units."
        ],
        [
         "How CIBC Integrated Purview and Unity Catalog for Our Data Governance Within a Data Mesh Architecture Leveraging the Lakehouse",
         "This session will focus on our journey to integrate Purview and Unity Catalog for our data governance within data mesh architecture, while leveraging the lakehouse. Our approach is to build four foundational pillars of data mesh that will uphold data management practices based on consumer-driven and domain-centric data sources. \n Principle 1: decentralized ownership\n Principle 2: federated governance\n Principle 3: data as a product\n Principle 4: self-service infrastructure\n Native access controls within the self-service infrastructure are enhanced by pushing policy defined access to unity to enable fine-grain control across the data landscape.",
         "Title: How CIBC Integrated Purview and Unity Catalog for Our Data Governance Within a Data Mesh Architecture Leveraging the Lakehouse\n                Abstract:  This session will focus on our journey to integrate Purview and Unity Catalog for our data governance within data mesh architecture, while leveraging the lakehouse. Our approach is to build four foundational pillars of data mesh that will uphold data management practices based on consumer-driven and domain-centric data sources. \n Principle 1: decentralized ownership\n Principle 2: federated governance\n Principle 3: data as a product\n Principle 4: self-service infrastructure\n Native access controls within the self-service infrastructure are enhanced by pushing policy defined access to unity to enable fine-grain control across the data landscape."
        ],
        [
         "The Future of Data Access Control: Booz Allen Hamilton’s Approach to Securing our Databricks Lakehouse with Immuta",
         "In this session, I’ll review how we utilize Attribute-Based Access Control (ABAC) to enforce policy via Immuta. I’ll discuss the differences between the ABAC and legacy Role-Based Access Control (RBAC) approaches to control access and how the RBAC approach is not sufficient to keep up with today’s growing big data market. With so much data available, there also comes substantial risk. Data can contain many sensitive data elements, including PII and PHI. Industry leaders like Databricks are pushing the boundaries of data technology, which leads to constantly evolving data use cases. And that’s a good thing! However, the RBAC approach is struggling to keep up with those advancements.\n So what is RBAC? It’s an approach to data access that permits system access based on the end-user’s role. For legacy systems, it’s meant as a simple but effective approach to securing data. Are you a manager? Then you’ll get access to data meant for managers. This is great for small deployments with clearly defined roles. Here at Booz Allen, we invested in Databricks because we have an environment of over 30k users and billions of rows of data.\n To mitigate this problem and align with our forward-thinking company standard, we introduced Immuta into our stack. Immuta uses ABAC to allow for dynamic data access control. Users are automatically assigned certain attributes, and access is based on those attributes instead of just their role. This allows for more flexibility and allows data access control to easily scale without the need to constantly map a user to their role. Using attributes, we can write policies in one place and have them applied across all of our data platforms. This makes for a truly holistic data governance approach and provides immediate ROI and time savings for the company.",
         "Title: The Future of Data Access Control: Booz Allen Hamilton’s Approach to Securing our Databricks Lakehouse with Immuta\n                Abstract:  In this session, I’ll review how we utilize Attribute-Based Access Control (ABAC) to enforce policy via Immuta. I’ll discuss the differences between the ABAC and legacy Role-Based Access Control (RBAC) approaches to control access and how the RBAC approach is not sufficient to keep up with today’s growing big data market. With so much data available, there also comes substantial risk. Data can contain many sensitive data elements, including PII and PHI. Industry leaders like Databricks are pushing the boundaries of data technology, which leads to constantly evolving data use cases. And that’s a good thing! However, the RBAC approach is struggling to keep up with those advancements.\n So what is RBAC? It’s an approach to data access that permits system access based on the end-user’s role. For legacy systems, it’s meant as a simple but effective approach to securing data. Are you a manager? Then you’ll get access to data meant for managers. This is great for small deployments with clearly defined roles. Here at Booz Allen, we invested in Databricks because we have an environment of over 30k users and billions of rows of data.\n To mitigate this problem and align with our forward-thinking company standard, we introduced Immuta into our stack. Immuta uses ABAC to allow for dynamic data access control. Users are automatically assigned certain attributes, and access is based on those attributes instead of just their role. This allows for more flexibility and allows data access control to easily scale without the need to constantly map a user to their role. Using attributes, we can write policies in one place and have them applied across all of our data platforms. This makes for a truly holistic data governance approach and provides immediate ROI and time savings for the company."
        ],
        [
         "Increasing Trust in Your Data: Enabling a Data Governance Program on Databricks Using Unity Catalog and ML-Driven MDM",
         "As part of Comcast Effectv’s transformation into a completely digital advertising agency, it was key to develop an approach to manage and remediate data quality issues related to customer data so that the sales organization is using reliable data to enable data-driven decision making. Like many organizations, Effectv's customer lifecycle processes are spread across many systems utilizing various integrations between them. This results in key challenges like duplicate and redundant customer data that requires rationalization and remediation. Data is at the core of Effectv’s modernization journey with the intended result of winning more business, accelerating order fulfillment, reducing make-goods and identifying revenue.\n  \nIn partnership with Slalom Consulting, Comcast Effectv built a traditional lakehouse on Databricks to ingest data from all of these systems but with a twist; they anchored every engineering decision in how it will enable their data governance program. \n  \nIn this talk, we will touch upon the data transformation journey at Effectv and dive deeper into the implementation of data governance leveraging Databricks solutions such as Delta Lake, Unity Catalog and DB SQL. Key focus areas include how we baked master data management into our pipelines by automating the matching and survivorship process, and bringing it all together for the data consumer via DBSQL to use our certified assets in bronze, silver and gold layers.\n  \nBy making thoughtful decisions about structuring data in Unity Catalog and baking MDM into ETL pipelines, you can greatly increase the quality, reliability, and adoption of single-source-of-truth data so your business users can stop spending cycles on wrangling data and spend more time developing actionable insights for your business.",
         "Title: Increasing Trust in Your Data: Enabling a Data Governance Program on Databricks Using Unity Catalog and ML-Driven MDM\n                Abstract:  As part of Comcast Effectv’s transformation into a completely digital advertising agency, it was key to develop an approach to manage and remediate data quality issues related to customer data so that the sales organization is using reliable data to enable data-driven decision making. Like many organizations, Effectv's customer lifecycle processes are spread across many systems utilizing various integrations between them. This results in key challenges like duplicate and redundant customer data that requires rationalization and remediation. Data is at the core of Effectv’s modernization journey with the intended result of winning more business, accelerating order fulfillment, reducing make-goods and identifying revenue.\n  \nIn partnership with Slalom Consulting, Comcast Effectv built a traditional lakehouse on Databricks to ingest data from all of these systems but with a twist; they anchored every engineering decision in how it will enable their data governance program. \n  \nIn this talk, we will touch upon the data transformation journey at Effectv and dive deeper into the implementation of data governance leveraging Databricks solutions such as Delta Lake, Unity Catalog and DB SQL. Key focus areas include how we baked master data management into our pipelines by automating the matching and survivorship process, and bringing it all together for the data consumer via DBSQL to use our certified assets in bronze, silver and gold layers.\n  \nBy making thoughtful decisions about structuring data in Unity Catalog and baking MDM into ETL pipelines, you can greatly increase the quality, reliability, and adoption of single-source-of-truth data so your business users can stop spending cycles on wrangling data and spend more time developing actionable insights for your business."
        ],
        [
         "Activate Your Lakehouse with Unity Catalog",
         "Building a Lakehouse is straightforward today thanks to many open-source technologies and Databricks. However, it can be taxing to extract value from Lakehouses as they grow in size without robust data operations. Join us to learn how YipitData uses the Unity Catalog to streamline data operations and discover best practices to scale your own Lakehouse.\n \n At YipitData, our 15+ petabyte Lakehouse is a self-service data platform built with Databricks and AWS, supporting analytics for a 250+ data team. We will share how leveraging Unity Catalog accelerates our mission to help financial institutions and corporations leverage alternative data by:\n \n - Enabling clients to universally access our data through a spectrum of channels, including Sigma, Delta Sharing, and multiple clouds \n - Fostering collaboration across internal teams using a data mesh paradigm that yields rich insights\n - Strengthening the integrity and security of data assets through ACLs, data lineage, audit logs, and further isolation of AWS resources\n - Reducing the cost of large tables without downtime through automated data expiration and ETL optimizations on managed delta tables\n \n Through our migration to Unity Catalog, we have gained tactics and philosophies to seamlessly flow our data assets internally and externally. Data platforms need to be value-generating, secure, and cost-effective in today's world. We are excited to share how Unity Catalog delivers on this and helps you get the most out of your Lakehouse.",
         "Title: Activate Your Lakehouse with Unity Catalog\n                Abstract:  Building a Lakehouse is straightforward today thanks to many open-source technologies and Databricks. However, it can be taxing to extract value from Lakehouses as they grow in size without robust data operations. Join us to learn how YipitData uses the Unity Catalog to streamline data operations and discover best practices to scale your own Lakehouse.\n \n At YipitData, our 15+ petabyte Lakehouse is a self-service data platform built with Databricks and AWS, supporting analytics for a 250+ data team. We will share how leveraging Unity Catalog accelerates our mission to help financial institutions and corporations leverage alternative data by:\n \n - Enabling clients to universally access our data through a spectrum of channels, including Sigma, Delta Sharing, and multiple clouds \n - Fostering collaboration across internal teams using a data mesh paradigm that yields rich insights\n - Strengthening the integrity and security of data assets through ACLs, data lineage, audit logs, and further isolation of AWS resources\n - Reducing the cost of large tables without downtime through automated data expiration and ETL optimizations on managed delta tables\n \n Through our migration to Unity Catalog, we have gained tactics and philosophies to seamlessly flow our data assets internally and externally. Data platforms need to be value-generating, secure, and cost-effective in today's world. We are excited to share how Unity Catalog delivers on this and helps you get the most out of your Lakehouse."
        ],
        [
         "Self Service Data Analytics and governance at enterprise scale with unity catalog",
         "This session focuses on one of the first Unity Catalog implementations for a large-scale enterprise. In this scenario, a cloud scale analytics platform with 7500 active users based on the lakehouse approach is used. In addition, there is potential for 1500 further users who are subject to special governance rules. They are consuming 600+TB of data stored in Delta Lake - continuously growing at more than 1TB per day. This might grow due to local country data.\n Therefore, the existing data platform must be extended to enable users to combine global and local data from their countries. A new data management was required, which reflects the strict information security rules at a need to know base. Core requirements are: read only from global data, write into local and share the results.\n Due to a very pronounced information security awareness and a lack of the technological possibilities it was not possible to interdisciplinary analyze and exchange data so easy or at all so far. Therefore, a lot of business potential and gains could not be identified and realized.\n With the new developments in the technology used and the basis of the Lakehouse approach, thanks to Unity Catalog, we were able to develop a solution that could meet high requirements for security and process. And enables globally secured interdisciplinary data exchange and analysis at scale.\n This solution enables the democratization of the data. This results not only in the ability to gain better insights for business management, but also to generate entirely new business cases or products that require a higher degree of data integration and encourage the culture to change.\n We highlight technical challenges and solutions, present best practices and point out benefits of implementing Unity catalog for enterprises.",
         "Title: Self Service Data Analytics and governance at enterprise scale with unity catalog\n                Abstract:  This session focuses on one of the first Unity Catalog implementations for a large-scale enterprise. In this scenario, a cloud scale analytics platform with 7500 active users based on the lakehouse approach is used. In addition, there is potential for 1500 further users who are subject to special governance rules. They are consuming 600+TB of data stored in Delta Lake - continuously growing at more than 1TB per day. This might grow due to local country data.\n Therefore, the existing data platform must be extended to enable users to combine global and local data from their countries. A new data management was required, which reflects the strict information security rules at a need to know base. Core requirements are: read only from global data, write into local and share the results.\n Due to a very pronounced information security awareness and a lack of the technological possibilities it was not possible to interdisciplinary analyze and exchange data so easy or at all so far. Therefore, a lot of business potential and gains could not be identified and realized.\n With the new developments in the technology used and the basis of the Lakehouse approach, thanks to Unity Catalog, we were able to develop a solution that could meet high requirements for security and process. And enables globally secured interdisciplinary data exchange and analysis at scale.\n This solution enables the democratization of the data. This results not only in the ability to gain better insights for business management, but also to generate entirely new business cases or products that require a higher degree of data integration and encourage the culture to change.\n We highlight technical challenges and solutions, present best practices and point out benefits of implementing Unity catalog for enterprises."
        ],
        [
         "PII Detection at Scale on the Lakehouse",
         "SEEK is Australia’s largest online employment marketplace and a market leader spanning ten countries across Asia Pacific and Latin America. SEEK provides employment opportunities for roughly 16 million monthly active users and process 25 million candidate applications to listings.\n \n Processing millions of resumès involves handling and managing highly sensitive candidate information, usually inputted in a highly unstructured format. With recent high-profile data leaks in Australia, personally identifiable information (PII) protection has become a major focus area for large digital organizations. The first step is detection, and SEEK has developed a custom framework built using HuggingFace transformers fine tuned with nuances around employment. For example, “Software Engineer at Databricks” is not PII, but “CEO at Databricks” is PII. \n \n After identifying and anonymizing PII in stream and batch data, SEEK uses Unity Catalog’s data lineage to track PII through their reporting, ETL, and other downstream ML use-cases and govern access control achieving an organization-wide data management capability driven by deep learning and enforcement using Databricks.",
         "Title: PII Detection at Scale on the Lakehouse\n                Abstract:  SEEK is Australia’s largest online employment marketplace and a market leader spanning ten countries across Asia Pacific and Latin America. SEEK provides employment opportunities for roughly 16 million monthly active users and process 25 million candidate applications to listings.\n \n Processing millions of resumès involves handling and managing highly sensitive candidate information, usually inputted in a highly unstructured format. With recent high-profile data leaks in Australia, personally identifiable information (PII) protection has become a major focus area for large digital organizations. The first step is detection, and SEEK has developed a custom framework built using HuggingFace transformers fine tuned with nuances around employment. For example, “Software Engineer at Databricks” is not PII, but “CEO at Databricks” is PII. \n \n After identifying and anonymizing PII in stream and batch data, SEEK uses Unity Catalog’s data lineage to track PII through their reporting, ETL, and other downstream ML use-cases and govern access control achieving an organization-wide data management capability driven by deep learning and enforcement using Databricks."
        ],
        [
         "Essential Data Security Strategies for the Modern Enterprise Data Architecture",
         "Balancing critical data requirements is a 24-7 task for enterprise-level organizations that must straddle the need to open specific gates to enable self-service data access while closing other access points to maintain internal and external compliance. Data breaches can cost U.S. businesses an average of $9.4 million per occurrence; ignoring this leaves organizations vulnerable to severe losses and crippling costs. \n \nThe 2022 Gartner Hype Cycle for Data Security reports that more and more enterprises are modernizing their data architecture with cloud and technology partners to help them collect, store and manage business data; a trend that does not appear to be letting up.\n \nAccording to Gartner®, “by 2025, 30% of enterprises will have adopted bDSP (Broad Data Security Platform), up from less than 10% in 2021, due to the pent-up demand for higher levels of data security and the rapid increase in product capabilities.\"",
         "Title: Essential Data Security Strategies for the Modern Enterprise Data Architecture\n                Abstract:  Balancing critical data requirements is a 24-7 task for enterprise-level organizations that must straddle the need to open specific gates to enable self-service data access while closing other access points to maintain internal and external compliance. Data breaches can cost U.S. businesses an average of $9.4 million per occurrence; ignoring this leaves organizations vulnerable to severe losses and crippling costs. \n \nThe 2022 Gartner Hype Cycle for Data Security reports that more and more enterprises are modernizing their data architecture with cloud and technology partners to help them collect, store and manage business data; a trend that does not appear to be letting up.\n \nAccording to Gartner®, “by 2025, 30% of enterprises will have adopted bDSP (Broad Data Security Platform), up from less than 10% in 2021, due to the pent-up demand for higher levels of data security and the rapid increase in product capabilities.\""
        ],
        [
         " \nMoving to both a modern data architecture and data-driven culture sets enterprises on the right trajectory for growth",
         " but it’s important to keep in mind individual public cloud platforms are not guaranteed to protect and secure data. To solve this",
         "Title:  \nMoving to both a modern data architecture and data-driven culture sets enterprises on the right trajectory for growth\n                Abstract:   but it’s important to keep in mind individual public cloud platforms are not guaranteed to protect and secure data. To solve this"
        ],
        [
         "Using Open-Source Tools to Build Privacy-Conscious Data Systems",
         "With the rapid proliferation of consumer data privacy laws across the world, it is becoming a strict requirement for data organizations to be mindful of data privacy risks. Privacy violation fines are reaching record highs and will only get higher as governments continue to crack down on the runaway abuse of user data. To continue producing value without becoming a liability, data systems must include privacy protections at a foundational level.\n \n The most practical way to do this is to enable privacy as code, shifting privacy left and including it as a foundational part of the organization's software development life cycle. The promise of privacy as code is that data organizations can be liberated from inefficient, manual workflows for producing the compliance deliverables their legal teams need, and instead ship at speed with pre-defined privacy guardrails built into the structure of their preferred workflows.\n \n Despite being an emerging and complex problem, there are already powerful open source tools available designed to help organizations of all sizes achieve this outcome. Fides is an open source privacy as code tool, written in Python and Typescript, that is engineered to tackle a variety of privacy problems throughout the application lifecycle. The most relevant feature for data organizations is the ability to annotate systems and their datasets with data privacy metadata, thus enabling automatic rejection of dangerous or illegal uses. Fides empowers data organizations to be proactive, not reactive, in terms of protecting user privacy and reducing organizational risk. Moving forward data privacy will need to be top of mind for data teams.",
         "Title: Using Open-Source Tools to Build Privacy-Conscious Data Systems\n                Abstract:  With the rapid proliferation of consumer data privacy laws across the world, it is becoming a strict requirement for data organizations to be mindful of data privacy risks. Privacy violation fines are reaching record highs and will only get higher as governments continue to crack down on the runaway abuse of user data. To continue producing value without becoming a liability, data systems must include privacy protections at a foundational level.\n \n The most practical way to do this is to enable privacy as code, shifting privacy left and including it as a foundational part of the organization's software development life cycle. The promise of privacy as code is that data organizations can be liberated from inefficient, manual workflows for producing the compliance deliverables their legal teams need, and instead ship at speed with pre-defined privacy guardrails built into the structure of their preferred workflows.\n \n Despite being an emerging and complex problem, there are already powerful open source tools available designed to help organizations of all sizes achieve this outcome. Fides is an open source privacy as code tool, written in Python and Typescript, that is engineered to tackle a variety of privacy problems throughout the application lifecycle. The most relevant feature for data organizations is the ability to annotate systems and their datasets with data privacy metadata, thus enabling automatic rejection of dangerous or illegal uses. Fides empowers data organizations to be proactive, not reactive, in terms of protecting user privacy and reducing organizational risk. Moving forward data privacy will need to be top of mind for data teams."
        ],
        [
         "Leveraging Unity Catalog for Data Governance for Grab’s Use Case",
         "Grab has been looking for a tool that provides data access to over 2000 users to support our daily operations. This tool should provide granular access control and easily scale to over thousands of users and tables, with data governance and security in mind.\n \n Through various evaluations, Grab concluded that Unity Catalog is a good fit that addresses all the requirements of data governance and security. For example, it provides row-level-security (RLS) and dynamic-data-masking (DDM) capabilities (through dynamic views), centralized governance and security at both table and object level. \n This ensures sensitive PII data is well protected, and we are able to open up tables to a wider audience ensuring only entitled data is shown. More importantly, we are able to meet the required auditing standards.\n \n With Unity catalog, we managed to:\n 1. Build a custom solution that allows users to seamlessly access data with UC.\n 2. Easily interface and integrate into existing Grab’s in-house entitlement system to translate access requirements, unifying the end-to-end user experience for requesting data access permission. \n 3. Extend UC’s capability by leveraging existing UC APIs and Databricks workflows to easily automate administrative tasks. For example, we have fully automated the creation of RLS and DDM views on-the-fly.\n 4. Provide out of the box, comprehensive auditing capabilities easily.\n \n As a result, we managed to achieve this in a short period of time to support over 2000 users to achieve better governance and significant cost savings with an ROI of about 2.7x",
         "Title: Leveraging Unity Catalog for Data Governance for Grab’s Use Case\n                Abstract:  Grab has been looking for a tool that provides data access to over 2000 users to support our daily operations. This tool should provide granular access control and easily scale to over thousands of users and tables, with data governance and security in mind.\n \n Through various evaluations, Grab concluded that Unity Catalog is a good fit that addresses all the requirements of data governance and security. For example, it provides row-level-security (RLS) and dynamic-data-masking (DDM) capabilities (through dynamic views), centralized governance and security at both table and object level. \n This ensures sensitive PII data is well protected, and we are able to open up tables to a wider audience ensuring only entitled data is shown. More importantly, we are able to meet the required auditing standards.\n \n With Unity catalog, we managed to:\n 1. Build a custom solution that allows users to seamlessly access data with UC.\n 2. Easily interface and integrate into existing Grab’s in-house entitlement system to translate access requirements, unifying the end-to-end user experience for requesting data access permission. \n 3. Extend UC’s capability by leveraging existing UC APIs and Databricks workflows to easily automate administrative tasks. For example, we have fully automated the creation of RLS and DDM views on-the-fly.\n 4. Provide out of the box, comprehensive auditing capabilities easily.\n \n As a result, we managed to achieve this in a short period of time to support over 2000 users to achieve better governance and significant cost savings with an ROI of about 2.7x"
        ],
        [
         "Unity Catalog: Flexibility to Fit Your Organization",
         "Databricks Unity Catalog provides the flexibility to meet the needs of any-sized organization, from startups to companies the size of Nike. \n \n Unity Catalog and lakehouse architecture have been chosen by Nike to store and process all of Nike's data. Unity Catalog has resolved systemic challenges in the Data Lake, resulting in 1) a simpler and transparent Data Lake, and, 2) allowing for scaled data and analytics use cases with greater efficiency. \n \n Designing Unity Catalog for the scale of Nike met the following goals:\n \n * Ownership of data: how all data produced in the lake has clear team ownership, communication channels, and accountability\n * Data environments: accelerated development of pipelines against real Production Data with zero risk\n * Consistent team semantics: all teams on Lakehouse follow the same semantics, allowing for scaled onboarding and consistent development expectations\n * Consistent governed data access: a single simplified, governed, data access process for all data in the Lake\n * Monitoring of unused data and data pipelines: simplified observability through Unity Catalog Data Lineage to quickly identify any unused data or data pipelines, resulting in less storage and compute costs\n \n These capabilities are powering Nike as a data driven organization, allowing the company to quickly make informed decisions.",
         "Title: Unity Catalog: Flexibility to Fit Your Organization\n                Abstract:  Databricks Unity Catalog provides the flexibility to meet the needs of any-sized organization, from startups to companies the size of Nike. \n \n Unity Catalog and lakehouse architecture have been chosen by Nike to store and process all of Nike's data. Unity Catalog has resolved systemic challenges in the Data Lake, resulting in 1) a simpler and transparent Data Lake, and, 2) allowing for scaled data and analytics use cases with greater efficiency. \n \n Designing Unity Catalog for the scale of Nike met the following goals:\n \n * Ownership of data: how all data produced in the lake has clear team ownership, communication channels, and accountability\n * Data environments: accelerated development of pipelines against real Production Data with zero risk\n * Consistent team semantics: all teams on Lakehouse follow the same semantics, allowing for scaled onboarding and consistent development expectations\n * Consistent governed data access: a single simplified, governed, data access process for all data in the Lake\n * Monitoring of unused data and data pipelines: simplified observability through Unity Catalog Data Lineage to quickly identify any unused data or data pipelines, resulting in less storage and compute costs\n \n These capabilities are powering Nike as a data driven organization, allowing the company to quickly make informed decisions."
        ],
        [
         "Automating Sensitive Data (PII/PHI) Detection and Quarantining to a Databricks Clean Room",
         "Healthcare datasets contain both personally identifiable information (PII) and personal health information (PHI) that needs to be de-identified in order to protect patient confidentiality and ensure HIPAA compliance. This privacy data is easily detected when it’s provided in columns  labeled with names such as “SSN,” First Name,” “Full Name,” and “DOB;” however, it is much harder to detect when it is hidden within columns labeled “Doctor Notes,” “Diagnoses,” or “Comments.” HealthVerity, a leader in the HIPAA-compliant exchange of real-world data (RWD) to uncover patient, payer and genomic insights and power innovation for the healthcare industry, ensures healthcare datasets are de-identified from PII and PHI using elaborate privacy procedures. During this presentation, we will demonstrate how to use a low-code/no-code platform to simplify and automate data pipelines that leverage prebuilt ML models to scan data for PHI/PII leakage and quarantine those rows in Unity Catalog when leakage is identified and move them to a Databricks clean room for analysis.",
         "Title: Automating Sensitive Data (PII/PHI) Detection and Quarantining to a Databricks Clean Room\n                Abstract:  Healthcare datasets contain both personally identifiable information (PII) and personal health information (PHI) that needs to be de-identified in order to protect patient confidentiality and ensure HIPAA compliance. This privacy data is easily detected when it’s provided in columns  labeled with names such as “SSN,” First Name,” “Full Name,” and “DOB;” however, it is much harder to detect when it is hidden within columns labeled “Doctor Notes,” “Diagnoses,” or “Comments.” HealthVerity, a leader in the HIPAA-compliant exchange of real-world data (RWD) to uncover patient, payer and genomic insights and power innovation for the healthcare industry, ensures healthcare datasets are de-identified from PII and PHI using elaborate privacy procedures. During this presentation, we will demonstrate how to use a low-code/no-code platform to simplify and automate data pipelines that leverage prebuilt ML models to scan data for PHI/PII leakage and quarantine those rows in Unity Catalog when leakage is identified and move them to a Databricks clean room for analysis."
        ],
        [
         "Cross-Platform Data Lineage with OpenLineage",
         "There are more data tools available than ever before, and it's easier to build a pipeline than it's ever been. This has resulted in an explosion of innovation, but it also means that data within today's organizations has become increasingly distributed. It can't be contained within a single brain, a single team, or a single platform. Data lineage can help by tracing the relationships between datasets and providing a map of your entire data universe. OpenLineage provides a standard for lineage collection that spans multiple platforms, including Apache Airflow, Apache Spark™, Flink®, and dbt. This empowers teams to diagnose and address widespread data quality and efficiency issues in real time. In this session, Julien Le Dem from Astronomer will show how to trace data lineage across Apache Spark and Apache Airflow. He will walk through the OpenLineage architecture and provide a live demo of a running pipeline with real-time data lineage.",
         "Title: Cross-Platform Data Lineage with OpenLineage\n                Abstract:  There are more data tools available than ever before, and it's easier to build a pipeline than it's ever been. This has resulted in an explosion of innovation, but it also means that data within today's organizations has become increasingly distributed. It can't be contained within a single brain, a single team, or a single platform. Data lineage can help by tracing the relationships between datasets and providing a map of your entire data universe. OpenLineage provides a standard for lineage collection that spans multiple platforms, including Apache Airflow, Apache Spark™, Flink®, and dbt. This empowers teams to diagnose and address widespread data quality and efficiency issues in real time. In this session, Julien Le Dem from Astronomer will show how to trace data lineage across Apache Spark and Apache Airflow. He will walk through the OpenLineage architecture and provide a live demo of a running pipeline with real-time data lineage."
        ],
        [
         "Ahold Delhaize's Journey to Implementing Unity Catalog at Scale",
         "In this session we will explore the steps we took to implement Unity Catalog at Ahold Delhaize's data platform. This powerful tool has completely transformed the way we access and manage data objects, making it easier than ever for our entire organization to find and use the data we need. The catalog brings us a governance layer that allows us to more securely and easily share restricted or confidential data by providing row and column level security per project. But that's not all, we're also setting up the Unity Catalog as a self-service tool, allowing users to easily customize and set up their own catalogs, schema's, views or user groups using simple YAML configuration files. We will cover topics such as:\n * Architecture where we came from and the improvement Unity Catalog has made\n * Our self-service way of working by showing code snippets\n * Our fully automatic deployments by explaining our flow\n By the end of this talk, you will have a better understanding of the process of implementing Unity Catalog at scale.",
         "Title: Ahold Delhaize's Journey to Implementing Unity Catalog at Scale\n                Abstract:  In this session we will explore the steps we took to implement Unity Catalog at Ahold Delhaize's data platform. This powerful tool has completely transformed the way we access and manage data objects, making it easier than ever for our entire organization to find and use the data we need. The catalog brings us a governance layer that allows us to more securely and easily share restricted or confidential data by providing row and column level security per project. But that's not all, we're also setting up the Unity Catalog as a self-service tool, allowing users to easily customize and set up their own catalogs, schema's, views or user groups using simple YAML configuration files. We will cover topics such as:\n * Architecture where we came from and the improvement Unity Catalog has made\n * Our self-service way of working by showing code snippets\n * Our fully automatic deployments by explaining our flow\n By the end of this talk, you will have a better understanding of the process of implementing Unity Catalog at scale."
        ],
        [
         "Multi-Cloud Data Governance on the Databricks Lakehouse",
         "Across industries, a multi-cloud setup has quickly become the reality for large organizations. Multi-cloud introduces new governance challenges as permissions models often do not translate from one cloud to the other and if they do, are insufficiently granular to accommodate privacy requirements and principles of least privilege. This problem can be especially acute for Data and AI workloads that rely on sharing and aggregating large and diverse data sources across business unit boundaries and where governance models need to incorporate assets such as table rows/columns and ML features and models. \n \n In this session we will provide guidelines on how best to overcome these challenges for companies that have adopted the Databricks Lakehouse as their collaborative space for data teams across the organization, by exploiting some of the unique product features of the Databricks platform.\n We will focus on a common scenario: a data platform team providing data assets to two different ML teams, one using the same cloud and the other one using a different cloud. We will explain the step-by-step setup of a unified governance model by leveraging the following components and conventions:\n \n - Unity Catalog for implementing fine-grained access control across all data assets: files in cloud storage, rows and columns in tables and ML features and models\n - The Databricks Terraform provider to automatically enforce guardrails and permissions across clouds \n - Account level SSO Integration and identity federation to centralize administer access across workspaces\n - Delta sharing to seamlessly propagate changes in provider data sets to consumers in near real-time\n - Centralized audit logging for a unified view on what asset was accessed by whom",
         "Title: Multi-Cloud Data Governance on the Databricks Lakehouse\n                Abstract:  Across industries, a multi-cloud setup has quickly become the reality for large organizations. Multi-cloud introduces new governance challenges as permissions models often do not translate from one cloud to the other and if they do, are insufficiently granular to accommodate privacy requirements and principles of least privilege. This problem can be especially acute for Data and AI workloads that rely on sharing and aggregating large and diverse data sources across business unit boundaries and where governance models need to incorporate assets such as table rows/columns and ML features and models. \n \n In this session we will provide guidelines on how best to overcome these challenges for companies that have adopted the Databricks Lakehouse as their collaborative space for data teams across the organization, by exploiting some of the unique product features of the Databricks platform.\n We will focus on a common scenario: a data platform team providing data assets to two different ML teams, one using the same cloud and the other one using a different cloud. We will explain the step-by-step setup of a unified governance model by leveraging the following components and conventions:\n \n - Unity Catalog for implementing fine-grained access control across all data assets: files in cloud storage, rows and columns in tables and ML features and models\n - The Databricks Terraform provider to automatically enforce guardrails and permissions across clouds \n - Account level SSO Integration and identity federation to centralize administer access across workspaces\n - Delta sharing to seamlessly propagate changes in provider data sets to consumers in near real-time\n - Centralized audit logging for a unified view on what asset was accessed by whom"
        ],
        [
         "How Comcast Effectv Drives Data Observability with Databricks and Monte Carlo",
         "Comcast Effectv, the 2,000-employee advertising wing of Comcast, America’s largest telecommunications company, provides custom video ad solutions powered by aggregated viewership data. As a global technology and media company connecting millions of customers to personalized experiences and processing billions of transactions, Comcast Effectv was challenged with handling massive loads of data, monitoring hundreds of data pipelines, and managing timely coordination across data teams. In this talk, Robinson Creighton, Sr. DataOps Lead of Advanced Analytics at Effectv, and Lior Gavish, CTO & co-founder at Monte Carlo, will discuss Comcast Effectv’s journey to building a more scalable, reliable lakehouse and driving data observability at scale with Monte Carlo. This has enabled Effectv to have a single pane of glass view of their entire data environment to ensure consumer data trust across their entire AWS, Databricks, and Looker environment.",
         "Title: How Comcast Effectv Drives Data Observability with Databricks and Monte Carlo\n                Abstract:  Comcast Effectv, the 2,000-employee advertising wing of Comcast, America’s largest telecommunications company, provides custom video ad solutions powered by aggregated viewership data. As a global technology and media company connecting millions of customers to personalized experiences and processing billions of transactions, Comcast Effectv was challenged with handling massive loads of data, monitoring hundreds of data pipelines, and managing timely coordination across data teams. In this talk, Robinson Creighton, Sr. DataOps Lead of Advanced Analytics at Effectv, and Lior Gavish, CTO & co-founder at Monte Carlo, will discuss Comcast Effectv’s journey to building a more scalable, reliable lakehouse and driving data observability at scale with Monte Carlo. This has enabled Effectv to have a single pane of glass view of their entire data environment to ensure consumer data trust across their entire AWS, Databricks, and Looker environment."
        ],
        [
         "Unity Catalog and Marketplace: Door to a Data Driven Organization",
         "Being data-driven can help organizations make better decisions, improve efficiency, increase competitiveness, and reduce compliance risks.\n \n Organizations expect the workforce to embrace data but at the same time want the process to be governance-driven, auditable, traceable and cost-center based for back charging.\n \n Unity Catalog is a tool that helps organizations manage and organize their data assets in a unified manner. By using Unity Catalog, you can create a central repository for all of your data assets, including structured and unstructured data, and provide access to this repository to authorized users within your organization. This can help you build a data-driven organization by enabling easy access to data for analysis and decision-making, and by promoting data governance and data management best practices.\n When Unity Catalog is made the front door for an organization’s data estate then it enables the organization to unlock the following:\n Governance-driven data discovery\n Governance-driven data access\n Governance-driven ephemeral workspace access\n Cost center based back charging\n Auditable and traceable\n \n Internal marketplace will help organizations publish and provide access to data within the organization’s workspaces via an out-of-the-box lightweight solution powered by Unity Catalog. \n \n Depending on the level of complexity and back charging, isolation requirements, organizations can choose to use the out-of-the-box internal marketplace or build the end-to-end process of publishing, provisioning, destroying, back charging process via Unity Catalog plus Workspace APIs.",
         "Title: Unity Catalog and Marketplace: Door to a Data Driven Organization\n                Abstract:  Being data-driven can help organizations make better decisions, improve efficiency, increase competitiveness, and reduce compliance risks.\n \n Organizations expect the workforce to embrace data but at the same time want the process to be governance-driven, auditable, traceable and cost-center based for back charging.\n \n Unity Catalog is a tool that helps organizations manage and organize their data assets in a unified manner. By using Unity Catalog, you can create a central repository for all of your data assets, including structured and unstructured data, and provide access to this repository to authorized users within your organization. This can help you build a data-driven organization by enabling easy access to data for analysis and decision-making, and by promoting data governance and data management best practices.\n When Unity Catalog is made the front door for an organization’s data estate then it enables the organization to unlock the following:\n Governance-driven data discovery\n Governance-driven data access\n Governance-driven ephemeral workspace access\n Cost center based back charging\n Auditable and traceable\n \n Internal marketplace will help organizations publish and provide access to data within the organization’s workspaces via an out-of-the-box lightweight solution powered by Unity Catalog. \n \n Depending on the level of complexity and back charging, isolation requirements, organizations can choose to use the out-of-the-box internal marketplace or build the end-to-end process of publishing, provisioning, destroying, back charging process via Unity Catalog plus Workspace APIs."
        ],
        [
         "Data Mesh Realization on Databricks: Making Data Engineering and Consumption Self-Service Driven for Data Platforms",
         "\"Our client, a consulting giant, as part of the famous \"\"Big 4\"\" is creating tax",
         "Title: Data Mesh Realization on Databricks: Making Data Engineering and Consumption Self-Service Driven for Data Platforms\n                Abstract:  \"Our client, a consulting giant, as part of the famous \"\"Big 4\"\" is creating tax"
        ],
        [
         "Distributing Data Governance: How Unity Catalog Allows for a Collaborative Approach",
         "As one of the world’s largest content delivery network (CDN) and security solutions provider, Akamai owns thousands of data assets of various shapes and sizes, some even go up to multiple PBs.\n Several departments within the company leverage Databricks for their data and AI workloads, which means we have over a hundred Databricks workspaces within a single Databricks account, where some of the assets are shared across products, and some are product-specific.\n  \n In this presentation, we will describe how to use the capabilities of Unity Catalog to distribute the administration burden between departments, while still maintaining a unified governance model.\n \n We will also share the benefits we’ve found in using Unity Catalog, beyond just access management, such as: \n * Visibility into which data assets we have in the organization\n * Ability to identify and potentially eliminate duplicate data workloads between departments \n * Removing boilerplate code for accessing external sources\n * Increasing innovation of product teams by exposing the data assets in a better, more efficient way",
         "Title: Distributing Data Governance: How Unity Catalog Allows for a Collaborative Approach\n                Abstract:  As one of the world’s largest content delivery network (CDN) and security solutions provider, Akamai owns thousands of data assets of various shapes and sizes, some even go up to multiple PBs.\n Several departments within the company leverage Databricks for their data and AI workloads, which means we have over a hundred Databricks workspaces within a single Databricks account, where some of the assets are shared across products, and some are product-specific.\n  \n In this presentation, we will describe how to use the capabilities of Unity Catalog to distribute the administration burden between departments, while still maintaining a unified governance model.\n \n We will also share the benefits we’ve found in using Unity Catalog, beyond just access management, such as: \n * Visibility into which data assets we have in the organization\n * Ability to identify and potentially eliminate duplicate data workloads between departments \n * Removing boilerplate code for accessing external sources\n * Increasing innovation of product teams by exposing the data assets in a better, more efficient way"
        ],
        [
         "Unity Catalog at Scale in Retail Data Engineering and Data Science",
         "Retail data science, insights, and media company 84.51° helps Kroger and other organizations create more personalized and valuable experiences for shoppers. Powered by cutting edge science, we leverage first-party retail data from nearly one out of two US households totaling over 2B transactions a year. Databricks is integral to the data science and machine learning work at 84.51° and manages 35+ PB of first-party customer data across 150+ Databricks workspaces. Before Unity Catalog, this data was structured in a decentralized data mesh which had issues including problematic data sharing, minimal data governance, and no auditing of data extracts. In this session, we will discuss how 84.51° incrementally rolled out Unity Catalog company-wide in a way that involved minimal disruption of existing processes and how UC-enabled solutions solved several real-world problems including:\n • Being able to migrate an on-prem solution to a Databricks SQL Warehouse with HIPAA auditing and data governance which resulted in reduced on-prem costs and enabled new solutions/products to bring significant new revenue sources. \n • Leveraging Delta Sharing to eliminate duplication of data and processing and enabling new insights to provide a deeper, more personal approach to customer engagement.\nDatabricks has also been key to the 84.51° Collaborative Cloud, which provides our clients' data scientists a platform that provides clean and ready-to-use transaction and UPC-level data from 60 million households empowering companies to get value out of data science, regardless of where they sit on the readiness spectrum. Unity Catalog will be key to extend the collaborative cloud functionality and cleanroom technology with improved security and governance.",
         "Title: Unity Catalog at Scale in Retail Data Engineering and Data Science\n                Abstract:  Retail data science, insights, and media company 84.51° helps Kroger and other organizations create more personalized and valuable experiences for shoppers. Powered by cutting edge science, we leverage first-party retail data from nearly one out of two US households totaling over 2B transactions a year. Databricks is integral to the data science and machine learning work at 84.51° and manages 35+ PB of first-party customer data across 150+ Databricks workspaces. Before Unity Catalog, this data was structured in a decentralized data mesh which had issues including problematic data sharing, minimal data governance, and no auditing of data extracts. In this session, we will discuss how 84.51° incrementally rolled out Unity Catalog company-wide in a way that involved minimal disruption of existing processes and how UC-enabled solutions solved several real-world problems including:\n • Being able to migrate an on-prem solution to a Databricks SQL Warehouse with HIPAA auditing and data governance which resulted in reduced on-prem costs and enabled new solutions/products to bring significant new revenue sources. \n • Leveraging Delta Sharing to eliminate duplication of data and processing and enabling new insights to provide a deeper, more personal approach to customer engagement.\nDatabricks has also been key to the 84.51° Collaborative Cloud, which provides our clients' data scientists a platform that provides clean and ready-to-use transaction and UPC-level data from 60 million households empowering companies to get value out of data science, regardless of where they sit on the readiness spectrum. Unity Catalog will be key to extend the collaborative cloud functionality and cleanroom technology with improved security and governance."
        ],
        [
         "Lakehouse as a FAIR Platform",
         "FAIR (findable, accessible, interoperable, reusable) data and data platforms are becoming more and more important in public sector. Lakehouse platform is strongly aligned with these principles. Lakehouse provides tools required to both adhere to FAIR but also to FAIRify data that isn't FAIR compliant. In this session we will cover parts of the lakehouse that enable end users to FAIRify data products, how to build good robust data products and which parts of Lakehouse align to which principles in FAIR. We'll demonstrate how DLT is crucial for data transformations on nonFAIR data, how Unity Catalog unlocks discoverability (F) and governed data access (A), and how marketplace, cleanrooms and Delta Sharing unlock interoperability and data exchange (I and R). All of these concepts are massive enablers for highly regulated industries such as Public Sector. It undeniably important to align Lakehouse to standards that are widely adopted by standards and policy makers and regulators. These principles transcend all industries and all use cases.",
         "Title: Lakehouse as a FAIR Platform\n                Abstract:  FAIR (findable, accessible, interoperable, reusable) data and data platforms are becoming more and more important in public sector. Lakehouse platform is strongly aligned with these principles. Lakehouse provides tools required to both adhere to FAIR but also to FAIRify data that isn't FAIR compliant. In this session we will cover parts of the lakehouse that enable end users to FAIRify data products, how to build good robust data products and which parts of Lakehouse align to which principles in FAIR. We'll demonstrate how DLT is crucial for data transformations on nonFAIR data, how Unity Catalog unlocks discoverability (F) and governed data access (A), and how marketplace, cleanrooms and Delta Sharing unlock interoperability and data exchange (I and R). All of these concepts are massive enablers for highly regulated industries such as Public Sector. It undeniably important to align Lakehouse to standards that are widely adopted by standards and policy makers and regulators. These principles transcend all industries and all use cases."
        ],
        [
         "Engineers Shouldn't Write Data Governance Policies",
         "Controlling permissions for accessing data assets can be messy, time consuming, and usually a combination of both. The teams responsible for creating the business rules that govern who should have access to what data are usually different from the teams responsible for administering the grants to achieve that access. On the other side of the equation, the end user who needs access to a data asset may be left waiting for grants to be made as the decision is passed between teams. That is, if they even know the correct path to getting access in the first place.\n  \n Separating the concerns of managing data governance at a business level and implementing data governance at an engineering level is the best way to clarify data access permissions. In practice, this involves building systems to enable data governance enforcement based on business rules, with little to no understanding of the individual system where the data lives. \n \n In practice, with a concrete business rule, such as “only users from the finance team should have access to critical financial data,” we want a system that deals only with those constituent concepts. For example, “the data is marked as critical financial” and “the user is a part of the finance team”). By abstracting away any source system components, such as “the tables in the finance schema” and “someone who’s a member of the finance databricks group,” the access policies applied will then model the business rules as closely as possible.\n \n This talk will focus on how to establish and align the processes, policies, and stakeholders involved in making this type of system work seamlessly. Sharing the experience and learnings of our team at Instacart, we will aim to help attendees streamline and simplify their data security and access strategies.",
         "Title: Engineers Shouldn't Write Data Governance Policies\n                Abstract:  Controlling permissions for accessing data assets can be messy, time consuming, and usually a combination of both. The teams responsible for creating the business rules that govern who should have access to what data are usually different from the teams responsible for administering the grants to achieve that access. On the other side of the equation, the end user who needs access to a data asset may be left waiting for grants to be made as the decision is passed between teams. That is, if they even know the correct path to getting access in the first place.\n  \n Separating the concerns of managing data governance at a business level and implementing data governance at an engineering level is the best way to clarify data access permissions. In practice, this involves building systems to enable data governance enforcement based on business rules, with little to no understanding of the individual system where the data lives. \n \n In practice, with a concrete business rule, such as “only users from the finance team should have access to critical financial data,” we want a system that deals only with those constituent concepts. For example, “the data is marked as critical financial” and “the user is a part of the finance team”). By abstracting away any source system components, such as “the tables in the finance schema” and “someone who’s a member of the finance databricks group,” the access policies applied will then model the business rules as closely as possible.\n \n This talk will focus on how to establish and align the processes, policies, and stakeholders involved in making this type of system work seamlessly. Sharing the experience and learnings of our team at Instacart, we will aim to help attendees streamline and simplify their data security and access strategies."
        ],
        [
         "Managing Data Encryption in Apache Spark™",
         "Sensitive data sets can be encrypted directly by new Apache Spark™  versions (3.2 and higher). Setting a number of configuration parameters and dataframe options will trigger the Apache Parquet modular encryption mechanism that protects select columns with column-specific keys. The upcoming Spark 3.4 version will also support uniform encryption, where all dataframe columns are encrypted with the same key.\n \n Spark data encryption is already leveraged by a number of companies to protect personal or business confidential data in their production environments. The main integration effort is focused on key access control and on building a Spark/Parquet plug-in code that can interact with company’s key management service (KMS).\n \n In this talk, we will briefly cover the basics of Spark/Parquet encryption usage, and dive into the details of encryption key management that will help in integrating this Spark data protection mechanism in your deployment. You will learn how to run a HelloWorld encryption sample, and how to extend it into a real world production code integrated with your organization’s KMS and access control policies. We will talk about the standard envelope encryption approach to big data protection, the performance-vs-security trade-offs between single and double envelope wrapping, internal and external key metadata storage. We will see a demo, and discuss the new features such as uniform encryption and two-tier management of encryption keys.",
         "Title: Managing Data Encryption in Apache Spark™\n                Abstract:  Sensitive data sets can be encrypted directly by new Apache Spark™  versions (3.2 and higher). Setting a number of configuration parameters and dataframe options will trigger the Apache Parquet modular encryption mechanism that protects select columns with column-specific keys. The upcoming Spark 3.4 version will also support uniform encryption, where all dataframe columns are encrypted with the same key.\n \n Spark data encryption is already leveraged by a number of companies to protect personal or business confidential data in their production environments. The main integration effort is focused on key access control and on building a Spark/Parquet plug-in code that can interact with company’s key management service (KMS).\n \n In this talk, we will briefly cover the basics of Spark/Parquet encryption usage, and dive into the details of encryption key management that will help in integrating this Spark data protection mechanism in your deployment. You will learn how to run a HelloWorld encryption sample, and how to extend it into a real world production code integrated with your organization’s KMS and access control policies. We will talk about the standard envelope encryption approach to big data protection, the performance-vs-security trade-offs between single and double envelope wrapping, internal and external key metadata storage. We will see a demo, and discuss the new features such as uniform encryption and two-tier management of encryption keys."
        ],
        [
         "Enabling Data Governance at Enterprise Scale Using Unity Catalog",
         "For some time, Amgen has been building multiple enterprise platforms on the latest technology with a key focus on tech rationalization, data democratization, overall user experience, increase reusability and cost-effectiveness. One of these platforms is the enterprise data fabric platform. This platform specifically focuses on pulling in data across functions into a single platform building capabilities around the connectedness of data, and access governance.\n For a while, we have been trying to setup robust data governance capabilities which are simple, yet easy to manage through Databricks. As part of this process we evaluated products like privacera, Immuta, and Okera. While these tools could solve a few immediate needs like managing tables and database level, for the use cases like maintaining governance on highly restricted data domains like EDW(Finance) and Workday(HR), we felt that for the long term we will require a more Databricks native solution because of below reasons:\n - There were ways to bypass these tools on databricks cluster\n - Unsupported DBR runtime\n - Complexity of fine-grained security\n - Policy management – AWS IAM + Intool policies\n  To address these challenges, and for large-scale enterprise adoption of our governance capability, we started working on UC integration with our governance processes. Following tech benefits we realized:\n - Independent of Databricks runtime\n - Easy fine-grained access control\n - Eliminated management of IAM roles\n - Dynamic access control using UC and dynamic views\n As a result of these technical benefits, we were able to implement fine-grained and complex governance policies around the restricted data set of Amgen including Finance and Workday. At this point we have around 100K objects mapped in the Unity Catalog and growing rapidly.",
         "Title: Enabling Data Governance at Enterprise Scale Using Unity Catalog\n                Abstract:  For some time, Amgen has been building multiple enterprise platforms on the latest technology with a key focus on tech rationalization, data democratization, overall user experience, increase reusability and cost-effectiveness. One of these platforms is the enterprise data fabric platform. This platform specifically focuses on pulling in data across functions into a single platform building capabilities around the connectedness of data, and access governance.\n For a while, we have been trying to setup robust data governance capabilities which are simple, yet easy to manage through Databricks. As part of this process we evaluated products like privacera, Immuta, and Okera. While these tools could solve a few immediate needs like managing tables and database level, for the use cases like maintaining governance on highly restricted data domains like EDW(Finance) and Workday(HR), we felt that for the long term we will require a more Databricks native solution because of below reasons:\n - There were ways to bypass these tools on databricks cluster\n - Unsupported DBR runtime\n - Complexity of fine-grained security\n - Policy management – AWS IAM + Intool policies\n  To address these challenges, and for large-scale enterprise adoption of our governance capability, we started working on UC integration with our governance processes. Following tech benefits we realized:\n - Independent of Databricks runtime\n - Easy fine-grained access control\n - Eliminated management of IAM roles\n - Dynamic access control using UC and dynamic views\n As a result of these technical benefits, we were able to implement fine-grained and complex governance policies around the restricted data set of Amgen including Finance and Workday. At this point we have around 100K objects mapped in the Unity Catalog and growing rapidly."
        ],
        [
         "Lineage System Table in Unity Catalog",
         "Unity Catalog provides fully automated data lineage for all workloads in SQL, R, Python, Scala and across all asset types at Databricks. The aggregated view has been available to end users through data explorer and API. In this session, we are excited to share that lineage is available via delta table in their UC metastore. It stores full history of recent lineage records and it is near real time. Additionally, customers can query it through standard SQL interface. With that, customers can get significant operational insights about their workload for impact analysis, troubleshooting, quality assurance, data discovery, and data governance. Together with the system table platform effort, which provides query history, job run operational data, audit logs and more, lineage table will be a critical piece to link all the data asset and entity asset together. It will provide better Lakehouse observability and unification to customers.",
         "Title: Lineage System Table in Unity Catalog\n                Abstract:  Unity Catalog provides fully automated data lineage for all workloads in SQL, R, Python, Scala and across all asset types at Databricks. The aggregated view has been available to end users through data explorer and API. In this session, we are excited to share that lineage is available via delta table in their UC metastore. It stores full history of recent lineage records and it is near real time. Additionally, customers can query it through standard SQL interface. With that, customers can get significant operational insights about their workload for impact analysis, troubleshooting, quality assurance, data discovery, and data governance. Together with the system table platform effort, which provides query history, job run operational data, audit logs and more, lineage table will be a critical piece to link all the data asset and entity asset together. It will provide better Lakehouse observability and unification to customers."
        ],
        [
         "Combining Privacy Solutions to Solve Data Access at Scale",
         "The trend that has made data easier to collect and analyze, has only aggravated privacy risks. Luckily, a range of privacy technologies have emerged to enable private data management (differential privacy, synthetic data, confidential computing).  In isolation, those technologies have had a limited impact because they did not always bring the 10x improvement expected by data leaders.\n \n Combining these privacy technologies has been the real game changer. We will demonstrate that the right mix of technologies brings the optimal balance of privacy and flexibility at the scale of the data warehouse.  We will illustrate this by real-life applications of Sarus in three domains:\nHealthcare: how to make hospital data available for research at scale in full compliance\nFinance: how to pool data between several banks to fight criminal transactions\nMarketing: how to build insights on combined data from partners and distributors\n \n The examples will be illustrated using data stored in Databricks and queried using Sarus differential privacy engine.",
         "Title: Combining Privacy Solutions to Solve Data Access at Scale\n                Abstract:  The trend that has made data easier to collect and analyze, has only aggravated privacy risks. Luckily, a range of privacy technologies have emerged to enable private data management (differential privacy, synthetic data, confidential computing).  In isolation, those technologies have had a limited impact because they did not always bring the 10x improvement expected by data leaders.\n \n Combining these privacy technologies has been the real game changer. We will demonstrate that the right mix of technologies brings the optimal balance of privacy and flexibility at the scale of the data warehouse.  We will illustrate this by real-life applications of Sarus in three domains:\nHealthcare: how to make hospital data available for research at scale in full compliance\nFinance: how to pool data between several banks to fight criminal transactions\nMarketing: how to build insights on combined data from partners and distributors\n \n The examples will be illustrated using data stored in Databricks and queried using Sarus differential privacy engine."
        ],
        [
         "Map Your Lakehouse Content with DiscoverX",
         "An enterprise lakehouse contains many different datasets which are related to different sources and might belong to different business units. \n These datasets can span across hundreds of tables, and each table has a different schema, and those schemas evolve over time. The Cyber security domain is a good example where datasets come from many different source systems and land in the lakehouse.\n With such a complex dataset ecosystem, answers to simple questions like “Have we ever detected this IP address?” or “Which columns contain IP addresses?” can become impractical and expensive.\n DiscoverX can automate the discovery of all columns that might contain specific patterns, (e.g. IP addresses, MAC addresses, fully qualified domain names, etc.) and automatically generate search and indexing queries that span across multiple tables and columns.",
         "Title: Map Your Lakehouse Content with DiscoverX\n                Abstract:  An enterprise lakehouse contains many different datasets which are related to different sources and might belong to different business units. \n These datasets can span across hundreds of tables, and each table has a different schema, and those schemas evolve over time. The Cyber security domain is a good example where datasets come from many different source systems and land in the lakehouse.\n With such a complex dataset ecosystem, answers to simple questions like “Have we ever detected this IP address?” or “Which columns contain IP addresses?” can become impractical and expensive.\n DiscoverX can automate the discovery of all columns that might contain specific patterns, (e.g. IP addresses, MAC addresses, fully qualified domain names, etc.) and automatically generate search and indexing queries that span across multiple tables and columns."
        ],
        [
         "Post-Merger: Implementing Unity Catalog Across Multiple Accounts",
         "Warner Media and Discovery have recently merged to form Warner Bros Discovery. Owning two Databricks accounts and wanting to maintain their separation, our data governance team has successfully implemented Unity Catalog as our data governance solution across both accounts, allowing our teams to collaboratively and securely use the data assets of two organizations. This session is aimed at sharing that success story, including initial challenges, our approach, our architecture, the actual implementation, and user success post-implementation.",
         "Title: Post-Merger: Implementing Unity Catalog Across Multiple Accounts\n                Abstract:  Warner Media and Discovery have recently merged to form Warner Bros Discovery. Owning two Databricks accounts and wanting to maintain their separation, our data governance team has successfully implemented Unity Catalog as our data governance solution across both accounts, allowing our teams to collaboratively and securely use the data assets of two organizations. This session is aimed at sharing that success story, including initial challenges, our approach, our architecture, the actual implementation, and user success post-implementation."
        ],
        [
         "Advanced Governance with Collibra on Databricks",
         "Define security policies in Collibra that are seamlessly enforced on Databricks",
         "Title: Advanced Governance with Collibra on Databricks\n                Abstract:  Define security policies in Collibra that are seamlessly enforced on Databricks"
        ],
        [
         "How Nestle is Leveraging Unity Catalog for Governance At Scale",
         "Governance with Unity Catalog across multiple regions, markets, teams and environments at Nestle. Establish data isolation at all levels",
         "Title: How Nestle is Leveraging Unity Catalog for Governance At Scale\n                Abstract:  Governance with Unity Catalog across multiple regions, markets, teams and environments at Nestle. Establish data isolation at all levels"
        ],
        [
         "Wrangling Your Security Data: Cybersecurity as a Data Management Problem",
         "Today, the average Fortune 500 company manages 100+ cybersecurity vendors. The sheer number of enterprise cybersecurity toolsets, each capturing and generating its own data, creates significant data management and data engineering challenges for all enterprises. Because each of these security tools has separate logs, outputs, and databases, it prevents any security team from meaningfully understanding their overall security posture and using their own data effectively. We believe that cybersecurity is fundamentally a data management problem. In this presentation, we will show how treating cybersecurity as a data management problem, and subsequently opening disparate cybersecurity data to queries can help organizations secure their systems and data more effectively. In this presentation, practitioners will walk away understanding the value of modeled and organized security on a data warehouse. We will leverage a real-world use case that shows how Databricks can help solve vulnerability management and end-to-end workflows for security practitioners.",
         "Title: Wrangling Your Security Data: Cybersecurity as a Data Management Problem\n                Abstract:  Today, the average Fortune 500 company manages 100+ cybersecurity vendors. The sheer number of enterprise cybersecurity toolsets, each capturing and generating its own data, creates significant data management and data engineering challenges for all enterprises. Because each of these security tools has separate logs, outputs, and databases, it prevents any security team from meaningfully understanding their overall security posture and using their own data effectively. We believe that cybersecurity is fundamentally a data management problem. In this presentation, we will show how treating cybersecurity as a data management problem, and subsequently opening disparate cybersecurity data to queries can help organizations secure their systems and data more effectively. In this presentation, practitioners will walk away understanding the value of modeled and organized security on a data warehouse. We will leverage a real-world use case that shows how Databricks can help solve vulnerability management and end-to-end workflows for security practitioners."
        ],
        [
         "Lakehouse Architecture to Advance Security Analytics at the Department of State",
         "In 2023, the Department of State surged forward on implementing a Lakehouse architecture to get faster, smarter, and more effective on cybersecurity log monitoring and incident response. In addition to getting us ahead of federal mandates, this approach promises to enable advanced analytics and machine learning across our highly federated global IT environment while minimizing costs associated with data retention and aggregation. \n \n This session will include a high-level overview of the technical and policy challenge and a technical deeper dive on the tactical implementation choices made. We’ll share lessons learned related to governance and securing organizational support, connecting between multiple cloud environments, and standardizing data to make it useful for analytics. \n \n And finally, we’ll discuss how the Lakehouse leverages Databricks in multi-cloud environments to promote decentralized ownership of data while enabling strong, centralized data governance practices.",
         "Title: Lakehouse Architecture to Advance Security Analytics at the Department of State\n                Abstract:  In 2023, the Department of State surged forward on implementing a Lakehouse architecture to get faster, smarter, and more effective on cybersecurity log monitoring and incident response. In addition to getting us ahead of federal mandates, this approach promises to enable advanced analytics and machine learning across our highly federated global IT environment while minimizing costs associated with data retention and aggregation. \n \n This session will include a high-level overview of the technical and policy challenge and a technical deeper dive on the tactical implementation choices made. We’ll share lessons learned related to governance and securing organizational support, connecting between multiple cloud environments, and standardizing data to make it useful for analytics. \n \n And finally, we’ll discuss how the Lakehouse leverages Databricks in multi-cloud environments to promote decentralized ownership of data while enabling strong, centralized data governance practices."
        ],
        [
         "Engineering Data Lakehouse Systems for the Next Ten Years of Growth",
         "For most of my professional life, I've dealt with data. As a data practitioner, I developed algorithms to solve real-world problems leveraging machine learning techniques. As an engineer, I led the direction that brought the value of my hands-on machine learning experience into our products and services by building upon cutting-edge and emerging technologies. This experience has taught me that scaling data systems is harder than you might think. \n  \n Supporting the operations of scalable data environments poses a challenge greater than the known application-level support due to the complexity of managing data together with the application. Why is data adding so much complexity? Well, data is big, so all systems are now becoming distributed. Data changes and evolves, and it's hard to create repeatable, automated pipelines. Plus, technology is advancing at an alarming rate and changes are messy.\n  \n Interested in learning about all the most recent updates in the data space? How does that impact our data systems? In this talk, you will learn about the challenges of product quality, delivery velocity, production monitoring, and outage recovery, see how those can be met using best practices in the tech stack, and develop empathy for those who manage scalable data environments.",
         "Title: Engineering Data Lakehouse Systems for the Next Ten Years of Growth\n                Abstract:  For most of my professional life, I've dealt with data. As a data practitioner, I developed algorithms to solve real-world problems leveraging machine learning techniques. As an engineer, I led the direction that brought the value of my hands-on machine learning experience into our products and services by building upon cutting-edge and emerging technologies. This experience has taught me that scaling data systems is harder than you might think. \n  \n Supporting the operations of scalable data environments poses a challenge greater than the known application-level support due to the complexity of managing data together with the application. Why is data adding so much complexity? Well, data is big, so all systems are now becoming distributed. Data changes and evolves, and it's hard to create repeatable, automated pipelines. Plus, technology is advancing at an alarming rate and changes are messy.\n  \n Interested in learning about all the most recent updates in the data space? How does that impact our data systems? In this talk, you will learn about the challenges of product quality, delivery velocity, production monitoring, and outage recovery, see how those can be met using best practices in the tech stack, and develop empathy for those who manage scalable data environments."
        ],
        [
         "Optimize Your Delta Lake",
         "Delta tables in Databricks just work. They're so easy to setup to deliver value in your organization. What about all those features and optimization that you can use to super charge your delta lakehouse? \n \n In this lighting talk we will discuss structuring, partitioning, optimizing, delta log retention of your Delta Tables all in 15 minutes or less!",
         "Title: Optimize Your Delta Lake\n                Abstract:  Delta tables in Databricks just work. They're so easy to setup to deliver value in your organization. What about all those features and optimization that you can use to super charge your delta lakehouse? \n \n In this lighting talk we will discuss structuring, partitioning, optimizing, delta log retention of your Delta Tables all in 15 minutes or less!"
        ],
        [
         "Lakehouses for Data Engineers: What You Need to Consider to Build Efficient ETL Pipelines",
         "ETL pipelines are ubiquitous in data-warehousing and Lakehouse ecosystem to make business decisions by building raw and derived tables from a plethora of fragmented data. As companies are investing in building data applications for richer user experiences, ETL pipelines need to solve some of the modern-day challenges of helping deliver low-latency analytics. \n \n In this lightning talk, we’ll focus on how to build efficient Apache Spark™  ETL pipelines for Lakehouses where you can effectively ingest and utilize streaming data. Will cover details on:\n - leveraging incrementally processing framework to improve compute efficiency\n - index data to deliver low-latency analytics\n - advanced concurrency control mechanisms to improve throughput",
         "Title: Lakehouses for Data Engineers: What You Need to Consider to Build Efficient ETL Pipelines\n                Abstract:  ETL pipelines are ubiquitous in data-warehousing and Lakehouse ecosystem to make business decisions by building raw and derived tables from a plethora of fragmented data. As companies are investing in building data applications for richer user experiences, ETL pipelines need to solve some of the modern-day challenges of helping deliver low-latency analytics. \n \n In this lightning talk, we’ll focus on how to build efficient Apache Spark™  ETL pipelines for Lakehouses where you can effectively ingest and utilize streaming data. Will cover details on:\n - leveraging incrementally processing framework to improve compute efficiency\n - index data to deliver low-latency analytics\n - advanced concurrency control mechanisms to improve throughput"
        ],
        [
         "Bringing Data in-House! Behind the New York Jets' Winning Game Plan to Create a Data Lakehouse on Databricks",
         "On the field, the New York Jets are one of the most recognizable brands in professional sports. Off the field, they’re a small business within a finite market with lots of competition. In order to fuel revenue growth, improve operational efficiencies, and drive fan engagement, the Jets brought its data infrastructure in-house and moved to Databricks. \n \n This 40-minute presentation will include a discussion of the New York Jets data journey including its overall business and technical goals, the organization's multiple attempts at building a data stack, what drove the shift to Databricks, its current architectural approach, and the benefits of leveraging Databricks as its cloud data platform. \n \n Benefits to data practitioners:\n  - Learn about the New York Jets data journey and the similarities to other organizations’ data journeys including challenges, build vs. buy, and the decision \n making processes required to implement a modern data stack\n  - Discussion around the Data Lakehouse architectural pattern and the steps required to implement this on Databricks\n  - Identification of the benefits of a customer 360 analysis and process and technologies required to create this record \n  - Understand the benefits of leveraging Databricks as a cloud data platform \n  - Highlight future opportunities to leverage Databricks for ML / AI use cases to improve upon current baseline analytics",
         "Title: Bringing Data in-House! Behind the New York Jets' Winning Game Plan to Create a Data Lakehouse on Databricks\n                Abstract:  On the field, the New York Jets are one of the most recognizable brands in professional sports. Off the field, they’re a small business within a finite market with lots of competition. In order to fuel revenue growth, improve operational efficiencies, and drive fan engagement, the Jets brought its data infrastructure in-house and moved to Databricks. \n \n This 40-minute presentation will include a discussion of the New York Jets data journey including its overall business and technical goals, the organization's multiple attempts at building a data stack, what drove the shift to Databricks, its current architectural approach, and the benefits of leveraging Databricks as its cloud data platform. \n \n Benefits to data practitioners:\n  - Learn about the New York Jets data journey and the similarities to other organizations’ data journeys including challenges, build vs. buy, and the decision \n making processes required to implement a modern data stack\n  - Discussion around the Data Lakehouse architectural pattern and the steps required to implement this on Databricks\n  - Identification of the benefits of a customer 360 analysis and process and technologies required to create this record \n  - Understand the benefits of leveraging Databricks as a cloud data platform \n  - Highlight future opportunities to leverage Databricks for ML / AI use cases to improve upon current baseline analytics"
        ],
        [
         "Databricks-As-Code: How to Effectively Automate a Secure Lakehouse Using Terraform for Resource Provisioning",
         "At Rivian, we have automated >95% of our Databricks resource provisioning workflows using an in-house Terraform module, affording us a lean admin team to manage 750+ users. In this talk, we will cover the following elements of our approach and how others can benefit from improved team efficiency.\n \n User and service principal management\n Our permission model on Unity Catalog for data governance\n Workspace and secrets resource management\n Managing internal package dependencies using init scripts\n Facilitating dashboards, SQL queries and their associated permissions\n Scaling source of truth Petabyte scale Delta Lake table ingestion jobs and workflows",
         "Title: Databricks-As-Code: How to Effectively Automate a Secure Lakehouse Using Terraform for Resource Provisioning\n                Abstract:  At Rivian, we have automated >95% of our Databricks resource provisioning workflows using an in-house Terraform module, affording us a lean admin team to manage 750+ users. In this talk, we will cover the following elements of our approach and how others can benefit from improved team efficiency.\n \n User and service principal management\n Our permission model on Unity Catalog for data governance\n Workspace and secrets resource management\n Managing internal package dependencies using init scripts\n Facilitating dashboards, SQL queries and their associated permissions\n Scaling source of truth Petabyte scale Delta Lake table ingestion jobs and workflows"
        ],
        [
         "Real-Time Reporting and Analytics for Construction Data Powered by Delta Lake and DBSQL",
         "Procore is a construction project management software that helps construction professionals efficiently manage their projects and collaborate with their teams. Our mission is to connect everyone in construction on a global platform. \n \n Procore is the system of record for all construction projects. Our customers need to access the data in near real-time for construction insights. Enhanced reporting is a self-service operational reporting module that allows quick data access with consistency to thousands of tables and reports.\n \n Procore Data platform rebuilt the module (originally built on the relational database) using Databricks and Delta lake. We used Apache Spark™  streaming to maintain the consistent state on the ingestion side from Kafka and plan to leverage the fully capable functionalities of DBSQL using the serverless SQL warehouse to read the medallion models (built via DBT) in Delta Lake. In addition, the Unity Catalog and the Delta share features helped us share the data across regions seamlessly. This design enabled us to improve the p95 and p99 read time by x% (which were initially timing out).\n \n  To learn about the learnings and experience of building a Data Lakehouse architecture, attend this talk.",
         "Title: Real-Time Reporting and Analytics for Construction Data Powered by Delta Lake and DBSQL\n                Abstract:  Procore is a construction project management software that helps construction professionals efficiently manage their projects and collaborate with their teams. Our mission is to connect everyone in construction on a global platform. \n \n Procore is the system of record for all construction projects. Our customers need to access the data in near real-time for construction insights. Enhanced reporting is a self-service operational reporting module that allows quick data access with consistency to thousands of tables and reports.\n \n Procore Data platform rebuilt the module (originally built on the relational database) using Databricks and Delta lake. We used Apache Spark™  streaming to maintain the consistent state on the ingestion side from Kafka and plan to leverage the fully capable functionalities of DBSQL using the serverless SQL warehouse to read the medallion models (built via DBT) in Delta Lake. In addition, the Unity Catalog and the Delta share features helped us share the data across regions seamlessly. This design enabled us to improve the p95 and p99 read time by x% (which were initially timing out).\n \n  To learn about the learnings and experience of building a Data Lakehouse architecture, attend this talk."
        ],
        [
         "Made in Italy: How Barilla Uses Databricks Lakehouse to Optimize its Operations",
         "Barilla Data2Value digital transformation program democratized data usage. The Databricks centered platform provides BI for a supply chain extended in 100 countries in the world, across 20 factories, >50 logistic hubs, 1000 suppliers, >10.000 B2B clients and >100.000 transports every year. By using metadata-driven and event-driven approaches, Barilla is able to meet the different needs of its 1000 data consumers from top management to operation management with about 50 use cases, consuming TB of data daily and execute more than 150 data pipelines. During the presentation, Barilla digital transformation strategy will be discussed, the platform architecture, the expected evolution as well as the lessons learned along the way with EY. Will be explained how Barilla has reduced the time to insight using Unity Catalog and MLFlow. Mariama Kamanda, a data scientist from the Barilla Acceleration Team, will provide testimony on the impact of Databricks Lakehouse daily journey. Will be explained how Barilla saved M$ every year by improving the factory losses through advanced analytics and machine learning leveraging Databricks.",
         "Title: Made in Italy: How Barilla Uses Databricks Lakehouse to Optimize its Operations\n                Abstract:  Barilla Data2Value digital transformation program democratized data usage. The Databricks centered platform provides BI for a supply chain extended in 100 countries in the world, across 20 factories, >50 logistic hubs, 1000 suppliers, >10.000 B2B clients and >100.000 transports every year. By using metadata-driven and event-driven approaches, Barilla is able to meet the different needs of its 1000 data consumers from top management to operation management with about 50 use cases, consuming TB of data daily and execute more than 150 data pipelines. During the presentation, Barilla digital transformation strategy will be discussed, the platform architecture, the expected evolution as well as the lessons learned along the way with EY. Will be explained how Barilla has reduced the time to insight using Unity Catalog and MLFlow. Mariama Kamanda, a data scientist from the Barilla Acceleration Team, will provide testimony on the impact of Databricks Lakehouse daily journey. Will be explained how Barilla saved M$ every year by improving the factory losses through advanced analytics and machine learning leveraging Databricks."
        ],
        [
         "Using Lakehouse to Fight Cancer: Ontada’s Journey to Establish a RWD Platform on Databricks Lakehouse",
         "Ontada, a McKesson business, is an oncology real-world data and evidence, clinical education and provider of technology business dedicated to transforming the fight against cancer. Core to Ontada’s mission is using real-world data (RWD) and evidence generation to improve patient health outcomes and to accelerate life science research. \n \n To support its mission, Ontada embarked on a journey to migrate its enterprise data warehouse (EDW) from an on-premise Oracle database to Databricks Lakehouse. This move allows Ontada to now consume data from any source, including structured and unstructured data from its own EHR and genomics lab results, and realize faster time to insight. In addition, using the Lakehouse has helped Ontada eliminate data silos, enabling the organization to realize the full potential of RWD – from running traditional descriptive analytics to extracting biomarkers from unstructured data. \n \n The presentation will cover the following topics: \n - Oracle to Databricks: migration best practices and lessons learned \n - People, process, and tools: expediting innovation while protecting patient information using Unity Catalog \n - Getting the most out of the Databricks Lakehouse: from BI to genomics, running all analytics under one platform \n - Hyperscale biomarker abstraction: reducing the manual effort needed to extract biomarkers from large unstructured data (medical notes, scanned/faxed documents) using spaCY and John Snow Lab NLP libraries \n \n Join this session to hear how Ontada is transforming RWD to deliver safe and effective cancer treatment.",
         "Title: Using Lakehouse to Fight Cancer: Ontada’s Journey to Establish a RWD Platform on Databricks Lakehouse\n                Abstract:  Ontada, a McKesson business, is an oncology real-world data and evidence, clinical education and provider of technology business dedicated to transforming the fight against cancer. Core to Ontada’s mission is using real-world data (RWD) and evidence generation to improve patient health outcomes and to accelerate life science research. \n \n To support its mission, Ontada embarked on a journey to migrate its enterprise data warehouse (EDW) from an on-premise Oracle database to Databricks Lakehouse. This move allows Ontada to now consume data from any source, including structured and unstructured data from its own EHR and genomics lab results, and realize faster time to insight. In addition, using the Lakehouse has helped Ontada eliminate data silos, enabling the organization to realize the full potential of RWD – from running traditional descriptive analytics to extracting biomarkers from unstructured data. \n \n The presentation will cover the following topics: \n - Oracle to Databricks: migration best practices and lessons learned \n - People, process, and tools: expediting innovation while protecting patient information using Unity Catalog \n - Getting the most out of the Databricks Lakehouse: from BI to genomics, running all analytics under one platform \n - Hyperscale biomarker abstraction: reducing the manual effort needed to extract biomarkers from large unstructured data (medical notes, scanned/faxed documents) using spaCY and John Snow Lab NLP libraries \n \n Join this session to hear how Ontada is transforming RWD to deliver safe and effective cancer treatment."
        ],
        [
         "CPG Lakehouse Analytics: Rapidly Implementing Walmart’s Luminate API at the Hershey Company",
         "Accurate, reliable, and timely data is critical for CPG companies to stay ahead in highly competitive retailer relationships, and for a company like the Hershey Company, the commercial relationship with Walmart is one of the most important. The team at Hershey found themselves with a looming deadline for their legacy analytics services and targeted a migration to the brand new Walmart Luminate API.\n  \n Working in partnership with Advancing Analytics, the Hershey Company leveraged a metadata-driven Lakehouse architecture to rapidly onboard the new Luminate API, helping the category management teams to overhaul how they measure, predict, and plan their business operations.\n  \n In this session, we will discuss the impact Luminate has had on Hershey's business covering key areas such as sales, supply chain, and retail field execution, and the technical building blocks that can be used to rapidly provision business users with the data they need, when they need it. We will discuss how key technologies enable this rapid approach, with Databricks Autoloader ingesting and shaping our data, Delta Streaming processing the data through the Lakehouse and Databricks SQL providing a responsive serving layer.\n  \n The session will be jointly run by Hersheys and Advancing Analytics, with Jordan Donmoyer, manager of Hershey's direct data team and Dan Davies, director of Walmart sales analytics providing the commercial commentary, and Simon Whiteley and Zach Stagers from Advancing Analytics covering the technical journey.",
         "Title: CPG Lakehouse Analytics: Rapidly Implementing Walmart’s Luminate API at the Hershey Company\n                Abstract:  Accurate, reliable, and timely data is critical for CPG companies to stay ahead in highly competitive retailer relationships, and for a company like the Hershey Company, the commercial relationship with Walmart is one of the most important. The team at Hershey found themselves with a looming deadline for their legacy analytics services and targeted a migration to the brand new Walmart Luminate API.\n  \n Working in partnership with Advancing Analytics, the Hershey Company leveraged a metadata-driven Lakehouse architecture to rapidly onboard the new Luminate API, helping the category management teams to overhaul how they measure, predict, and plan their business operations.\n  \n In this session, we will discuss the impact Luminate has had on Hershey's business covering key areas such as sales, supply chain, and retail field execution, and the technical building blocks that can be used to rapidly provision business users with the data they need, when they need it. We will discuss how key technologies enable this rapid approach, with Databricks Autoloader ingesting and shaping our data, Delta Streaming processing the data through the Lakehouse and Databricks SQL providing a responsive serving layer.\n  \n The session will be jointly run by Hersheys and Advancing Analytics, with Jordan Donmoyer, manager of Hershey's direct data team and Dan Davies, director of Walmart sales analytics providing the commercial commentary, and Simon Whiteley and Zach Stagers from Advancing Analytics covering the technical journey."
        ],
        [
         "Building a Minimalistic Open Lakehouse Using Open Source Projects Apache Spark™, Project Nessie and Iceberg",
         "A Lakehouse architecture is a combination of various components such as Storage, File format, Table format, and Catalog. What truly makes a lakehouse 'open' is data being stored in open-source table & file formats like Iceberg, Delta & Parquet respectively and the technology being open-sourced for easy & quick adoption by the community. Like any new technology, implementation of a lakehouse may seem daunting at first. However, when we break down the architecture to its open components, this becomes easy to adopt & scale.\n \n Though this session, the idea is to help data engineers getting their leg into the world of data lakehouses, easily learn & implement it. We will go through a Notebook-style presentation to show beginners how to build a minimalistic functional lakehouse using Apache Spark, Project Nessie & Iceberg.\n \n We will talk about:\n - configuring the 3 different components\n - creating tables from raw data files\n - ingesting new data from various sources into the tables, querying it and making updates\n - time travel, compaction, etc capabilities",
         "Title: Building a Minimalistic Open Lakehouse Using Open Source Projects Apache Spark™, Project Nessie and Iceberg\n                Abstract:  A Lakehouse architecture is a combination of various components such as Storage, File format, Table format, and Catalog. What truly makes a lakehouse 'open' is data being stored in open-source table & file formats like Iceberg, Delta & Parquet respectively and the technology being open-sourced for easy & quick adoption by the community. Like any new technology, implementation of a lakehouse may seem daunting at first. However, when we break down the architecture to its open components, this becomes easy to adopt & scale.\n \n Though this session, the idea is to help data engineers getting their leg into the world of data lakehouses, easily learn & implement it. We will go through a Notebook-style presentation to show beginners how to build a minimalistic functional lakehouse using Apache Spark, Project Nessie & Iceberg.\n \n We will talk about:\n - configuring the 3 different components\n - creating tables from raw data files\n - ingesting new data from various sources into the tables, querying it and making updates\n - time travel, compaction, etc capabilities"
        ],
        [
         "Labcorp Data Platform Journey: From Selection to Go-Live in six months",
         "Please join with us to learn about the Labcorp data platform transformation from on-premises Hadoop to AWS Databricks Lakehouse. We will share best practices and lessons learned from cloud-native data platform selection, implementation, and migration from Hadoop (with in 6 months) with Unity Catalog. We will share steps taken to retire several legacy on-premises technologies and leverage Databricks native features like Spark streaming, workflows, job pools, cluster policies and Spark JDBC within Databricks platform. Lessons learned in Implementing Unity Catalog and building a security and governance model that scales across applications. Demos and walkthrough’s of batch frameworks, streaming frameworks, data compare tools used across several applications to improve data quality and speed of delivery. Discover how we have improved operational efficiency, resiliency and reduced TCO, and how we scaled building workspaces and associated cloud infrastructure using Terraform provider.",
         "Title: Labcorp Data Platform Journey: From Selection to Go-Live in six months\n                Abstract:  Please join with us to learn about the Labcorp data platform transformation from on-premises Hadoop to AWS Databricks Lakehouse. We will share best practices and lessons learned from cloud-native data platform selection, implementation, and migration from Hadoop (with in 6 months) with Unity Catalog. We will share steps taken to retire several legacy on-premises technologies and leverage Databricks native features like Spark streaming, workflows, job pools, cluster policies and Spark JDBC within Databricks platform. Lessons learned in Implementing Unity Catalog and building a security and governance model that scales across applications. Demos and walkthrough’s of batch frameworks, streaming frameworks, data compare tools used across several applications to improve data quality and speed of delivery. Discover how we have improved operational efficiency, resiliency and reduced TCO, and how we scaled building workspaces and associated cloud infrastructure using Terraform provider."
        ],
        [
         "Deep Dive Into Grammarly's Data Platform",
         "Grammarly helps 30 million people and 50,000 teams to communicate more effectively. Using the Databricks Lakehouse Platform, we are able to rapidly ingest, transform, aggregate, and query complex data sets from an ecosystem of sources, all governed by Unity Catalog. This presentation overviews Grammarly’s data platform and the decisions that shaped the implementation. We will dive deep into some architectural challenges the Grammarly Data Platform team overcame as we developed a self-service framework for incremental event processing.\n \n Our investment in the Lakehouse and Unity Catalog has dramatically improved the speed of our data value chain: making 5 billion events (ingested, aggregated, de-identified, and governed) available to stakeholders (data scientists, business analysts, sales, marketing) and downstream services (feature store, reporting/dashboards, customer support, operations) available within 15 minutes. As a result, we have improved our query cost performance (110% faster at 10% the cost) compared to our legacy system on AWS EMR.\n \n I will share architecture diagrams, their implications at scale, code samples, and problems solved and to be solved in a technology-focused discussion about Grammarly’s iterative Lakehouse Data Platform.",
         "Title: Deep Dive Into Grammarly's Data Platform\n                Abstract:  Grammarly helps 30 million people and 50,000 teams to communicate more effectively. Using the Databricks Lakehouse Platform, we are able to rapidly ingest, transform, aggregate, and query complex data sets from an ecosystem of sources, all governed by Unity Catalog. This presentation overviews Grammarly’s data platform and the decisions that shaped the implementation. We will dive deep into some architectural challenges the Grammarly Data Platform team overcame as we developed a self-service framework for incremental event processing.\n \n Our investment in the Lakehouse and Unity Catalog has dramatically improved the speed of our data value chain: making 5 billion events (ingested, aggregated, de-identified, and governed) available to stakeholders (data scientists, business analysts, sales, marketing) and downstream services (feature store, reporting/dashboards, customer support, operations) available within 15 minutes. As a result, we have improved our query cost performance (110% faster at 10% the cost) compared to our legacy system on AWS EMR.\n \n I will share architecture diagrams, their implications at scale, code samples, and problems solved and to be solved in a technology-focused discussion about Grammarly’s iterative Lakehouse Data Platform."
        ],
        [
         "Simplifying Migrations to Lakehouse",
         "Following on from Perenti Global's successful presentation at the ANZ (Syd) Data and AI summit in Nov 2022, they were asked by Bede Hackney if they would be interested in supporting Databricks events on a bigger stage, ramping up their reference-ability etc... they are confirmed as able to support us at this event and are excited to do so. \n Link to client presentation noting tweaks to be made upon confirmation of attendance --> https://docs.google.com/presentation/d/1OHMsyFhV-gq6lcVvXqkp-0LY8wS6C6TS/edit#slide=id.p1\n \n \n High level Synopsis:\n - Challenges with Legacy Platforms \n - Perenti Databricks Migration Journey\n - Reimagining Migrations the Databricks Way\n - The Databricks Migration Methodology & Approach",
         "Title: Simplifying Migrations to Lakehouse\n                Abstract:  Following on from Perenti Global's successful presentation at the ANZ (Syd) Data and AI summit in Nov 2022, they were asked by Bede Hackney if they would be interested in supporting Databricks events on a bigger stage, ramping up their reference-ability etc... they are confirmed as able to support us at this event and are excited to do so. \n Link to client presentation noting tweaks to be made upon confirmation of attendance --> https://docs.google.com/presentation/d/1OHMsyFhV-gq6lcVvXqkp-0LY8wS6C6TS/edit#slide=id.p1\n \n \n High level Synopsis:\n - Challenges with Legacy Platforms \n - Perenti Databricks Migration Journey\n - Reimagining Migrations the Databricks Way\n - The Databricks Migration Methodology & Approach"
        ],
        [
         "Having Your Cake and Eating it Too: How Vizio Built a Next-Generation ACR Data Platform While Lowering TCO",
         "As the top manufacturer of smart TVs, Vizio uses TV data to drive its business and provide customers with best digital experiences. Our company's mission is to continually improve the viewing experience for our customers, which is why we developed our award-winning automatic content recognition (ACR) platform. When we first built our data platform almost ten years ago, there was no single platform to run a data as a service business, so we got creative and built our own by stitching together different AWS services and a data warehouse. As our business needs and data volumes have grown exponentially over the years, we made the strategic decision to replatform on Databricks Lakehouse, as it was the only platform that could satisfy all our needs out-of-the-box such as BI analytics, real-time streaming, and AI/ML. Now the Lakehouse is our sole source of truth for all analytics and machine learning projects. The technical value of the Databricks Lakehouse platform, such as traditional data warehousing low-latency query processing with complex joins thanks to Photon to using Apache Spark™  structured streaming; analytics and model serving, will be covered in this session as we talk about our path to the Lakehouse.",
         "Title: Having Your Cake and Eating it Too: How Vizio Built a Next-Generation ACR Data Platform While Lowering TCO\n                Abstract:  As the top manufacturer of smart TVs, Vizio uses TV data to drive its business and provide customers with best digital experiences. Our company's mission is to continually improve the viewing experience for our customers, which is why we developed our award-winning automatic content recognition (ACR) platform. When we first built our data platform almost ten years ago, there was no single platform to run a data as a service business, so we got creative and built our own by stitching together different AWS services and a data warehouse. As our business needs and data volumes have grown exponentially over the years, we made the strategic decision to replatform on Databricks Lakehouse, as it was the only platform that could satisfy all our needs out-of-the-box such as BI analytics, real-time streaming, and AI/ML. Now the Lakehouse is our sole source of truth for all analytics and machine learning projects. The technical value of the Databricks Lakehouse platform, such as traditional data warehousing low-latency query processing with complex joins thanks to Photon to using Apache Spark™  structured streaming; analytics and model serving, will be covered in this session as we talk about our path to the Lakehouse."
        ],
        [
         "From Insights to Recommendations: How SkyWatch Predicts Demand for Satellite Imagery Using Databricks",
         "SkyWatch is on a mission to democratize earth observation data and make it simple for anyone to use.\n \n In this session, you will learn about how SkyWatch aggregates demand signals for EO market and turns them into monetizable recommendations for satellite operators.\n Skywatch’s head of data engineering, Vincent, and data engineer, Aayush, will share how the team built a serverless architecture that synthesizes customer requests for satellite images and identifies geographic locations with high demand, helping satellite operators maximize revenue and satisfying a broad range of EO data hungry consumers.\n \n This session will cover the usage of\n \n - Databricks in-built H3 functions \n - Open Source Mosaic library to process geospatial data\n - Delta Lake to efficiently store data leveraging optimization techniques like Z-Ordering\n - Databricks Serverless SQL endpoint to build REST API with AWS Lambda and step functions.\n - Databricks with AWS serverless architecture (Lambda and step functions)",
         "Title: From Insights to Recommendations: How SkyWatch Predicts Demand for Satellite Imagery Using Databricks\n                Abstract:  SkyWatch is on a mission to democratize earth observation data and make it simple for anyone to use.\n \n In this session, you will learn about how SkyWatch aggregates demand signals for EO market and turns them into monetizable recommendations for satellite operators.\n Skywatch’s head of data engineering, Vincent, and data engineer, Aayush, will share how the team built a serverless architecture that synthesizes customer requests for satellite images and identifies geographic locations with high demand, helping satellite operators maximize revenue and satisfying a broad range of EO data hungry consumers.\n \n This session will cover the usage of\n \n - Databricks in-built H3 functions \n - Open Source Mosaic library to process geospatial data\n - Delta Lake to efficiently store data leveraging optimization techniques like Z-Ordering\n - Databricks Serverless SQL endpoint to build REST API with AWS Lambda and step functions.\n - Databricks with AWS serverless architecture (Lambda and step functions)"
        ],
        [
         "Determining When to Use GPU for Your ETL Pipelines at Scale",
         "Assuming you have hundreds of jobs and/or clusters in your Databricks workspace, what is the best way to determine if those pipelines can take advantage of GPU for speed and/or cost saving? We are presenting NVIDIA GPU Qualification Tool applied at scale to project potential cost saving for your entire workspace.",
         "Title: Determining When to Use GPU for Your ETL Pipelines at Scale\n                Abstract:  Assuming you have hundreds of jobs and/or clusters in your Databricks workspace, what is the best way to determine if those pipelines can take advantage of GPU for speed and/or cost saving? We are presenting NVIDIA GPU Qualification Tool applied at scale to project potential cost saving for your entire workspace."
        ],
        [
         "Massive Data Processing in Adobe Using Delta Lake:  A Year In",
         "At Adobe Experience Platform, we ingest TBs of data everyday and manage PBs of data for our customers as part of the unified profile offering. At the heart of this is a bunch of complex ingestion of a mix of normalized and denormalized data with various linkage scenarios power by a central identity linking graph. This helps power various marketing scenarios that are activated in multiple platforms and channels like email, advertisements etc. We will go over how we built a cost effective and scalable data pipeline using Apache Spark™  and Delta Lake and share our experiences after one year in production.\n \n - What are we storing?\n - Multi-source, multi-channel problem\n - Access pattern to optimize for\n - Custom high performance query engine\n - Data representation and nested schema evolution\n - Performance trade-offs with various formats\n - Go over anti-patterns used \n - String FTW\n - Data manipulation using UDFs \n - Writer worries and how to wipe them away\n  - Locking for contention mechanism\n - Gotchas\n - Concurrency\n - Column size\n - Update frequency\n - Transaction management for a healthy state\n - Staging tables FTW \n - Why we can't live without them\n - Datalake replication lag tracking\n - Instrumentation of the data pipeline gives more confidence to the reade\n - Maintenance jobs\n - Go over essentials of compaction and vacuuming\n - Performance time!\n - What scale are we operating at?\n - Settings like autoCompact and optimizeWrite\n - Timings with and without delta\n - Cost \n  - Lesson learned and burnt",
         "Title: Massive Data Processing in Adobe Using Delta Lake:  A Year In\n                Abstract:  At Adobe Experience Platform, we ingest TBs of data everyday and manage PBs of data for our customers as part of the unified profile offering. At the heart of this is a bunch of complex ingestion of a mix of normalized and denormalized data with various linkage scenarios power by a central identity linking graph. This helps power various marketing scenarios that are activated in multiple platforms and channels like email, advertisements etc. We will go over how we built a cost effective and scalable data pipeline using Apache Spark™  and Delta Lake and share our experiences after one year in production.\n \n - What are we storing?\n - Multi-source, multi-channel problem\n - Access pattern to optimize for\n - Custom high performance query engine\n - Data representation and nested schema evolution\n - Performance trade-offs with various formats\n - Go over anti-patterns used \n - String FTW\n - Data manipulation using UDFs \n - Writer worries and how to wipe them away\n  - Locking for contention mechanism\n - Gotchas\n - Concurrency\n - Column size\n - Update frequency\n - Transaction management for a healthy state\n - Staging tables FTW \n - Why we can't live without them\n - Datalake replication lag tracking\n - Instrumentation of the data pipeline gives more confidence to the reade\n - Maintenance jobs\n - Go over essentials of compaction and vacuuming\n - Performance time!\n - What scale are we operating at?\n - Settings like autoCompact and optimizeWrite\n - Timings with and without delta\n - Cost \n  - Lesson learned and burnt"
        ],
        [
         "DHL eCommerce US – Building a scalable and robust Cloud Data Platform as enabler for Enterprise Analytics",
         "DHL eCommerce Solutions Americas is a high volume B2C domestic and international parcel delivery business and that is on mission to become a data-driven organization to by democratizing the use of data and to maximizing the business value.\n  \n Learn how the DHL eCommerce Solutions Americas data team built a centralized Enterprise grade scalable cloud data platform processing 100s of millions of events daily. This single source of truth powering all analytics use cases helps DHL ecommerce US team to deliver business insights 5x faster.\n We will dive into how we solved challenging key requirements including secure multi-tenant central data platform, scalable data ingestion and processing, cost effective big data repository, fast and regular data refreshes, diverse analytics workload needs.\n \n Finally we will share some example usecases operated on this platform and the business value those usecases are generating along various lines of the business including operations, commercial, customer service, sales, product management, etc.",
         "Title: DHL eCommerce US – Building a scalable and robust Cloud Data Platform as enabler for Enterprise Analytics\n                Abstract:  DHL eCommerce Solutions Americas is a high volume B2C domestic and international parcel delivery business and that is on mission to become a data-driven organization to by democratizing the use of data and to maximizing the business value.\n  \n Learn how the DHL eCommerce Solutions Americas data team built a centralized Enterprise grade scalable cloud data platform processing 100s of millions of events daily. This single source of truth powering all analytics use cases helps DHL ecommerce US team to deliver business insights 5x faster.\n We will dive into how we solved challenging key requirements including secure multi-tenant central data platform, scalable data ingestion and processing, cost effective big data repository, fast and regular data refreshes, diverse analytics workload needs.\n \n Finally we will share some example usecases operated on this platform and the business value those usecases are generating along various lines of the business including operations, commercial, customer service, sales, product management, etc."
        ],
        [
         "Learnings From the Field: Migration From Oracle DW and IBM DataStage to Databricks on AWS",
         "Legacy data warehouses are costly to maintain, unscalable and cannot deliver on data science, ML and real-time analytics use cases. Migrating from your enterprise data warehouse to Databricks lets you scale as your business needs grow and accelerate innovation by running all your data, analytics and AI workloads on a single unified data platform.\n \n In the first part of this talk we will guide you through the well-designed process and tools that will help you from the assessment phase to the actual implementation of an EDW migration project. Also, we will address ways to convert PL/SQL proprietary code to an open standard python code and take advantage of PySpark for ETL workloads and Databricks SQL’s data analytics workload power.\n \n The second part of this talk will be based on an EDW migration project of SNCF (French national railways); one of the major enterprise customers of Databricks in France. Databricks partnered with SNCF to migrate its real estate entity from Oracle DW and IBM DataStage to Databricks on AWS. We will walk you through the customer context, urgency to migration, challenges, target architecture, nitty-gritty details of implementation, best practices, recommendations, and learnings in order to execute a successful migration project in a very accelerated time frame.",
         "Title: Learnings From the Field: Migration From Oracle DW and IBM DataStage to Databricks on AWS\n                Abstract:  Legacy data warehouses are costly to maintain, unscalable and cannot deliver on data science, ML and real-time analytics use cases. Migrating from your enterprise data warehouse to Databricks lets you scale as your business needs grow and accelerate innovation by running all your data, analytics and AI workloads on a single unified data platform.\n \n In the first part of this talk we will guide you through the well-designed process and tools that will help you from the assessment phase to the actual implementation of an EDW migration project. Also, we will address ways to convert PL/SQL proprietary code to an open standard python code and take advantage of PySpark for ETL workloads and Databricks SQL’s data analytics workload power.\n \n The second part of this talk will be based on an EDW migration project of SNCF (French national railways); one of the major enterprise customers of Databricks in France. Databricks partnered with SNCF to migrate its real estate entity from Oracle DW and IBM DataStage to Databricks on AWS. We will walk you through the customer context, urgency to migration, challenges, target architecture, nitty-gritty details of implementation, best practices, recommendations, and learnings in order to execute a successful migration project in a very accelerated time frame."
        ],
        [
         "Why a Major Japanese Financial Institution Chose Databricks to Accelerate its Data and AI-Driven Journey",
         "In this session, we will introduce a case study of migrating the Japanese largest data analysis platform to Databricks.\n \nNTT DATA is one of the largest system integrators in Japan. In the Japanese market, many companies are working on BI, and we are now in the phase of using AI. Our team provides solutions that provide data analytics infrastructure to drive the democratization of data and AI for leading Japanese companies.\n \n The customer in this case study is the largest insurance company in Japan. This project has the following characteristics:\n  - As a financial institution, security requirements are very strict.\n  - Since it is used company-wide, including group companies, it is necessary to support various use cases.\n \n We started operating a data analysis platform on AWS in 2017. Over the next five years, we leveraged AWS-managed services such as Amazon EMR, Amazon Athena, and Amazon Sage Maker to modernize our architecture.\n \n In the near future, in order to promote the use cases of AI as well as BI, we have begun to consider upgrading to a platform that realizes both BI and AI.\n \n This session will cover:\n  - Challenges in developing AI on a DWH-based data analysis platform and why a data lake house is the best choice\n  - Examining the architecture of a platform that supports both AI and BI use cases. In this case study, we will introduce the results of a comparative study of a proposal based on Databricks, a proposal based on Snowflake, and a proposal combining Snowflake and Databricks.\n \n This session is recommended for those who want to accelerate their business by utilizing AI as well as BI.\n \n \n",
         "Title: Why a Major Japanese Financial Institution Chose Databricks to Accelerate its Data and AI-Driven Journey\n                Abstract:  In this session, we will introduce a case study of migrating the Japanese largest data analysis platform to Databricks.\n \nNTT DATA is one of the largest system integrators in Japan. In the Japanese market, many companies are working on BI, and we are now in the phase of using AI. Our team provides solutions that provide data analytics infrastructure to drive the democratization of data and AI for leading Japanese companies.\n \n The customer in this case study is the largest insurance company in Japan. This project has the following characteristics:\n  - As a financial institution, security requirements are very strict.\n  - Since it is used company-wide, including group companies, it is necessary to support various use cases.\n \n We started operating a data analysis platform on AWS in 2017. Over the next five years, we leveraged AWS-managed services such as Amazon EMR, Amazon Athena, and Amazon Sage Maker to modernize our architecture.\n \n In the near future, in order to promote the use cases of AI as well as BI, we have begun to consider upgrading to a platform that realizes both BI and AI.\n \n This session will cover:\n  - Challenges in developing AI on a DWH-based data analysis platform and why a data lake house is the best choice\n  - Examining the architecture of a platform that supports both AI and BI use cases. In this case study, we will introduce the results of a comparative study of a proposal based on Databricks, a proposal based on Snowflake, and a proposal combining Snowflake and Databricks.\n \n This session is recommended for those who want to accelerate their business by utilizing AI as well as BI."
        ],
        [
         "Eliminating Shuffles in Delete Update, and Merge",
         "If you’ve ever had to delete a set of records for regulatory compliance, update a set of records to fix an issue in the ingestion pipeline, or apply changes in a transaction log to a fact table, you know that row-level operations are becoming critical for modern data lake workflows. Even though the industry has seen a tremendous amount of innovation in this area, row-level operations can be still fairly expensive if the underlying data has to be shuffled. This talk will explain how Apache Spark can completely avoid shuffles during row-level operations by leveraging storage-partitioned joins, a key to efficiently modify data at PB scale.",
         "Title: Eliminating Shuffles in Delete Update, and Merge\n                Abstract:  If you’ve ever had to delete a set of records for regulatory compliance, update a set of records to fix an issue in the ingestion pipeline, or apply changes in a transaction log to a fact table, you know that row-level operations are becoming critical for modern data lake workflows. Even though the industry has seen a tremendous amount of innovation in this area, row-level operations can be still fairly expensive if the underlying data has to be shuffled. This talk will explain how Apache Spark can completely avoid shuffles during row-level operations by leveraging storage-partitioned joins, a key to efficiently modify data at PB scale."
        ],
        [
         "When Self-Service Meets Earnings Calls: A Journey Towards FinOps",
         "At Disney Streaming, we extensively use Databricks and the delta lake as an operational and analytical tool. But as we shift focus from growth to profit, our data platform needed to pivot from self-service and ease use and towards automatic tracking, auditing, and continuous efficiency chasing. \n \n In this talk, I'll explain our FinOps journey; how we democratized cost data, increased transparency, tightened policies, and got dozens of analytical and engineering teams to fit cost savings into their roadmap.\n",
         "Title: When Self-Service Meets Earnings Calls: A Journey Towards FinOps\n                Abstract:  At Disney Streaming, we extensively use Databricks and the delta lake as an operational and analytical tool. But as we shift focus from growth to profit, our data platform needed to pivot from self-service and ease use and towards automatic tracking, auditing, and continuous efficiency chasing. \n \n In this talk, I'll explain our FinOps journey; how we democratized cost data, increased transparency, tightened policies, and got dozens of analytical and engineering teams to fit cost savings into their roadmap."
        ],
        [
         "Delta Lake Migration At Scale",
         "Adobe Experience Platform (AEP) serves a large number Adobe digital experience customers with tens of thousands of datasets in non-Delta format. To leverage Delta Lake, we migrate existing tables to Delta format after resolving the following challenges: 1). Short downtime to minimize customer impact; 2). Adding new system metadata; 3). Migration automation for scalability; and 4). Error detection and catastrophic rollback. This talk will cover the solutions to the above challenges and lessons learned from migration. Finally, scalable migration service becomes a core capability of the data platform.",
         "Title: Delta Lake Migration At Scale\n                Abstract:  Adobe Experience Platform (AEP) serves a large number Adobe digital experience customers with tens of thousands of datasets in non-Delta format. To leverage Delta Lake, we migrate existing tables to Delta format after resolving the following challenges: 1). Short downtime to minimize customer impact; 2). Adding new system metadata; 3). Migration automation for scalability; and 4). Error detection and catastrophic rollback. This talk will cover the solutions to the above challenges and lessons learned from migration. Finally, scalable migration service becomes a core capability of the data platform."
        ],
        [
         "Databricks Cost Management",
         "With Databricks you only pay for the compute resources you use. With growing usage it is advisable to have a cost management strategy to avoid surprises at the end of the month. It is also useful to monitor and break down costs for budgeting purposes or to calculate the return on investment (ROI) for specific projects.\n \n In this talk I will:\n - Describe the Databricks pricing model\n - Show how to analyze and break down costs\n - Discuss best practises for cost optimization",
         "Title: Databricks Cost Management\n                Abstract:  With Databricks you only pay for the compute resources you use. With growing usage it is advisable to have a cost management strategy to avoid surprises at the end of the month. It is also useful to monitor and break down costs for budgeting purposes or to calculate the return on investment (ROI) for specific projects.\n \n In this talk I will:\n - Describe the Databricks pricing model\n - Show how to analyze and break down costs\n - Discuss best practises for cost optimization"
        ],
        [
         "Data Asset Bundles: A Standard, Unified Approach to Deploying Data Products on Databricks",
         "In this talk we will introduce data asset bundles, provide a demonstration of how they work for a variety of data products, and how to fit them into an overall CICD strategy for the well-architected Lakehouse.\n \n Data teams produce a variety of assets; datasets, reports and dashboards, ML models, and business applications. These assets depend upon code (notebooks, repos, queries, pipelines), infrastructure (clusters, SQL warehouses, serverless endpoints), and supporting services/resources like Unity Catalog, Databricks Workflows, and DBSQL dashboards. Today, each organization must figure out a deployment strategy for the variety of data products they build on Databricks as there is no consistent way to describe the infrastructure and services associated with project code.\n \n Data asset bundles is a new capability on Databricks that standardizes and unifies the deployment strategy for all data products developed on the platform. It allows developers to describe the infrastructure and resources of their project through a YAML or JSON configuration file, regardless of whether they are producing a report, dashboard, online ML model, or Delta Live Tables pipeline. Behind the scenes, these configuration files use Terraform to manage resources in a Databricks workspace, but knowledge of Terraform is not required to use Data Asset Bundles.",
         "Title: Data Asset Bundles: A Standard, Unified Approach to Deploying Data Products on Databricks\n                Abstract:  In this talk we will introduce data asset bundles, provide a demonstration of how they work for a variety of data products, and how to fit them into an overall CICD strategy for the well-architected Lakehouse.\n \n Data teams produce a variety of assets; datasets, reports and dashboards, ML models, and business applications. These assets depend upon code (notebooks, repos, queries, pipelines), infrastructure (clusters, SQL warehouses, serverless endpoints), and supporting services/resources like Unity Catalog, Databricks Workflows, and DBSQL dashboards. Today, each organization must figure out a deployment strategy for the variety of data products they build on Databricks as there is no consistent way to describe the infrastructure and services associated with project code.\n \n Data asset bundles is a new capability on Databricks that standardizes and unifies the deployment strategy for all data products developed on the platform. It allows developers to describe the infrastructure and resources of their project through a YAML or JSON configuration file, regardless of whether they are producing a report, dashboard, online ML model, or Delta Live Tables pipeline. Behind the scenes, these configuration files use Terraform to manage resources in a Databricks workspace, but knowledge of Terraform is not required to use Data Asset Bundles."
        ],
        [
         "Lakehouses: The Best Start to Your Graph Data and Analytics Journey",
         "Data architects and IT executives are continually looking for the best ways to integrate graph data and analytics into their organizations to improve business outcomes. This presentation outlines how the Data Lakehouse provides the perfect starting point for a successful journey. We will explore how the Data Lakehouse offer the unique combination of scalability, flexibility, and speed to quickly and effectively ingest, pre-process, curate, and analyze graph data to create powerful analytics. Additionally, this talk will discuss the benefits of using the Data Lakehouse over traditional graph databases and how it can help improve time to insight, time to production and overall satisfaction. \n \nAt the end of this presentation, attendees will: \n - Understand the benefits of using a Data Lakehouse for graph data and analytics \n - Learn how to get started with a successful Lakehouse implementation (demo)\n - Discover the advantages of using a Data Lakehouse over graph databases\n - Learn specifically where graph databases integrate and perform better together\n \n Key Takeaways: \n - Data Lakehouses provide the perfect starting point for a successful graph data and analytics journey \n - Data Lakehouses offer scalability, flexibility, and speed to quickly and effectively analyze graph data \n - The Data Lakehouse is a cost-effective alternative to traditional graph database shortening your time to insight and de-risk your project.\n \n Presentation Components:\n The customer journey\n Access patterns\n Reference architecture (logical, physical)\n Customer stories\n Demo (notebook + live graph visualization)",
         "Title: Lakehouses: The Best Start to Your Graph Data and Analytics Journey\n                Abstract:  Data architects and IT executives are continually looking for the best ways to integrate graph data and analytics into their organizations to improve business outcomes. This presentation outlines how the Data Lakehouse provides the perfect starting point for a successful journey. We will explore how the Data Lakehouse offer the unique combination of scalability, flexibility, and speed to quickly and effectively ingest, pre-process, curate, and analyze graph data to create powerful analytics. Additionally, this talk will discuss the benefits of using the Data Lakehouse over traditional graph databases and how it can help improve time to insight, time to production and overall satisfaction. \n \nAt the end of this presentation, attendees will: \n - Understand the benefits of using a Data Lakehouse for graph data and analytics \n - Learn how to get started with a successful Lakehouse implementation (demo)\n - Discover the advantages of using a Data Lakehouse over graph databases\n - Learn specifically where graph databases integrate and perform better together\n \n Key Takeaways: \n - Data Lakehouses provide the perfect starting point for a successful graph data and analytics journey \n - Data Lakehouses offer scalability, flexibility, and speed to quickly and effectively analyze graph data \n - The Data Lakehouse is a cost-effective alternative to traditional graph database shortening your time to insight and de-risk your project.\n \n Presentation Components:\n The customer journey\n Access patterns\n Reference architecture (logical, physical)\n Customer stories\n Demo (notebook + live graph visualization)"
        ],
        [
         "Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse",
         "As you embark on a lakehouse project or evolve your existing data lake, you may want to improve your security posture and take advantage of new security features—there may even be a security team at your company that demands it! Databricks has worked with thousands of customers to securely deploy the Databricks Platform to meet their architecture and security requirements. While many organizations deploy security differently, we have found a common set of guidelines and features among organizations that require a high level of security. In this session, we will detail the security features and architectural choices frequently used by these organizations and walk through a series of threat models for the risks that most concern security teams. While this session is great for people who already know Databricks—don’t worry—that knowledge isn’t required. \nYou will walk away with a full handbook detailing all of the concepts, configurations, check lists, security analysis tool (SAT), and security reference architecture (SRA) automation scripts  from the session so that you can make immediate progress when you get back to the office. \\\nSecurity can be hard, but we’ve collected the hard work already done by some of the best in the industry, and built tools, to make it easier. Come learn how. See how good looks like via a demo.\n",
         "Title: Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse\n                Abstract:  As you embark on a lakehouse project or evolve your existing data lake, you may want to improve your security posture and take advantage of new security features—there may even be a security team at your company that demands it! Databricks has worked with thousands of customers to securely deploy the Databricks Platform to meet their architecture and security requirements. While many organizations deploy security differently, we have found a common set of guidelines and features among organizations that require a high level of security. In this session, we will detail the security features and architectural choices frequently used by these organizations and walk through a series of threat models for the risks that most concern security teams. While this session is great for people who already know Databricks—don’t worry—that knowledge isn’t required. \nYou will walk away with a full handbook detailing all of the concepts, configurations, check lists, security analysis tool (SAT), and security reference architecture (SRA) automation scripts  from the session so that you can make immediate progress when you get back to the office. \\\nSecurity can be hard, but we’ve collected the hard work already done by some of the best in the industry, and built tools, to make it easier. Come learn how. See how good looks like via a demo."
        ],
        [
         "Data Globalization at Conde Nast Using Delta Sharing",
         "Databricks has been an essential part of the Conde Nast architecture for the last few years.\n \n Prior to building our centralized data platform, “evergreen”, we had similar challenges as many other organizations; siloed data, duplicated efforts for engineers, and a lack of collaboration between data teams. These problems led to mistrust in data sets and made it difficult to scale to meet the strategic globalization plan we had for Conde Nast.\n \n Over the last few years we have been extremely successful in building a centralized data platform on Databricks in AWS, fully embracing the lakehouse vision from end-to-end. Now, our analysts and marketers can derive the same insights from one dataset and data scientists can use the same datasets for use cases such as personalization, subscriber propensity models, churn models and on-site recommendations for our iconic brands.\n \n In this talk, we’ll discuss how we plan to incorporate Unity Catalog and Delta Sharing as the next phase of our globalization mission. The evergreen platform has become the global standard for data processing and analytics at Conde. In order to manage the worldwide data and comply with GDPR requirements, we need to make sure data is processed in the appropriate region and PII data is handled appropriately. At the same time, we need to have a global view of the data to allow us to make business decisions at the global level. We’ll talk about how delta sharing allows us a simple, secure way to share de-identified datasets across regions in order to make these strategic business decisions, while complying with security requirements. Additionally, we’ll discuss how Unity Catalog allows us to secure, govern and audit these datasets in an easy and scalable manner.",
         "Title: Data Globalization at Conde Nast Using Delta Sharing\n                Abstract:  Databricks has been an essential part of the Conde Nast architecture for the last few years.\n \n Prior to building our centralized data platform, “evergreen”, we had similar challenges as many other organizations; siloed data, duplicated efforts for engineers, and a lack of collaboration between data teams. These problems led to mistrust in data sets and made it difficult to scale to meet the strategic globalization plan we had for Conde Nast.\n \n Over the last few years we have been extremely successful in building a centralized data platform on Databricks in AWS, fully embracing the lakehouse vision from end-to-end. Now, our analysts and marketers can derive the same insights from one dataset and data scientists can use the same datasets for use cases such as personalization, subscriber propensity models, churn models and on-site recommendations for our iconic brands.\n \n In this talk, we’ll discuss how we plan to incorporate Unity Catalog and Delta Sharing as the next phase of our globalization mission. The evergreen platform has become the global standard for data processing and analytics at Conde. In order to manage the worldwide data and comply with GDPR requirements, we need to make sure data is processed in the appropriate region and PII data is handled appropriately. At the same time, we need to have a global view of the data to allow us to make business decisions at the global level. We’ll talk about how delta sharing allows us a simple, secure way to share de-identified datasets across regions in order to make these strategic business decisions, while complying with security requirements. Additionally, we’ll discuss how Unity Catalog allows us to secure, govern and audit these datasets in an easy and scalable manner."
        ],
        [
         "Unlocking the Value of Data Sharing in Financial Services With Lakehouse",
         "The emergence of secure data sharing is already having a tremendous economic impact, in large part due to the increasing ease and safety of sharing financial data. McKinsey predicts that the impact of open financial data will be 1-4.5% of GDP globally by 2030. This indicates there is a narrowing window on a massive opportunity for financial institutions and it is critical that they prioritize data sharing. This session will first address the ways in which Delta Sharing and Unity Catalog on a Databricks Lakehouse architecture provides a simple and open framework for building a Secure Data Sharing platform in the financial services industry. Next we will use a Databricks environment to walk through different use cases for open banking data and secure data sharing, demonstrating how they will be implemented using Delta Sharing, Unity Catalog, and other parts of the Lakehouse platform. The use cases will include examples of new product features such as Databricks to Databricks sharing, change data feed and streaming on Delta Sharing, table/column lineage, and the Delta Sharing Excel plugin to demonstrate state of the art sharing capabilities.\n Spencer Cook, a Sr. Solutions Architect at Databricks focussed on the Financial Services industry will discuss secure data sharing on Databricks Lakehouse and will demonstrate architecture and code for common sharing use cases in the finance industry.",
         "Title: Unlocking the Value of Data Sharing in Financial Services With Lakehouse\n                Abstract:  The emergence of secure data sharing is already having a tremendous economic impact, in large part due to the increasing ease and safety of sharing financial data. McKinsey predicts that the impact of open financial data will be 1-4.5% of GDP globally by 2030. This indicates there is a narrowing window on a massive opportunity for financial institutions and it is critical that they prioritize data sharing. This session will first address the ways in which Delta Sharing and Unity Catalog on a Databricks Lakehouse architecture provides a simple and open framework for building a Secure Data Sharing platform in the financial services industry. Next we will use a Databricks environment to walk through different use cases for open banking data and secure data sharing, demonstrating how they will be implemented using Delta Sharing, Unity Catalog, and other parts of the Lakehouse platform. The use cases will include examples of new product features such as Databricks to Databricks sharing, change data feed and streaming on Delta Sharing, table/column lineage, and the Delta Sharing Excel plugin to demonstrate state of the art sharing capabilities.\n Spencer Cook, a Sr. Solutions Architect at Databricks focussed on the Financial Services industry will discuss secure data sharing on Databricks Lakehouse and will demonstrate architecture and code for common sharing use cases in the finance industry."
        ],
        [
         "Extending Lakehouse Architecture with Collaborative Identity",
         "Lakehouse architecture has become a valuable solution for unifying data processing for AI, but faces limitations in maximizing data’s full potential. Additional data infrastructure is helpful for strengthening data consolidation and data connectivity with third-party sources, which are necessary for building full data sets for accurate audience modeling.\n\nIn this session, LiveRamp will demonstrate to data and analytics decision-makers how to build on the Lakehouse architecture with extensions for collaborative identity graph construction, including how to simplify and improve data enrichment, data activation and data collaboration. LiveRamp will also introduce a complete data marketplace, which enables easy, pseudonymized data enhancements that widen the attribute set for better behavioral model construction.\n\nWith these techniques and technologies, enterprises across financial services, retail, media, travel and more can safely unlock partner insights and ultimately produce more accurate inputs for personalization engines, and more engaging offers and recommendations for customers.\n",
         "Title: Extending Lakehouse Architecture with Collaborative Identity\n                Abstract:  Lakehouse architecture has become a valuable solution for unifying data processing for AI, but faces limitations in maximizing data’s full potential. Additional data infrastructure is helpful for strengthening data consolidation and data connectivity with third-party sources, which are necessary for building full data sets for accurate audience modeling.\n\nIn this session, LiveRamp will demonstrate to data and analytics decision-makers how to build on the Lakehouse architecture with extensions for collaborative identity graph construction, including how to simplify and improve data enrichment, data activation and data collaboration. LiveRamp will also introduce a complete data marketplace, which enables easy, pseudonymized data enhancements that widen the attribute set for better behavioral model construction.\n\nWith these techniques and technologies, enterprises across financial services, retail, media, travel and more can safely unlock partner insights and ultimately produce more accurate inputs for personalization engines, and more engaging offers and recommendations for customers."
        ],
        [
         "Writing Data-Sharing Apps Using Node.js and Delta Sharing",
         "Javascript remains the top programming language today, with most code repositories written using JavaScript on GitHub. However, JavaScript is evolving beyond just a language for web application development into a language built for tomorrow. Everyday tasks like data wrangling, data analysis, and predictive analytics are possible today directly from a web browser. For example, many popular data analytics libraries, like Tensorflow.js, now support JavaScript SDKs. Another popular library, Danfo.js, makes it possible to wrangle data using familiar Pandas-like operations, shortening the learning curve and arming the typical data engineer or data scientist with another data tool in their toolbox. In this presentation, we’ll explore using the Node.js connector for Delta Sharing to build a data analytics app that summarizes a Twitter dataset.",
         "Title: Writing Data-Sharing Apps Using Node.js and Delta Sharing\n                Abstract:  Javascript remains the top programming language today, with most code repositories written using JavaScript on GitHub. However, JavaScript is evolving beyond just a language for web application development into a language built for tomorrow. Everyday tasks like data wrangling, data analysis, and predictive analytics are possible today directly from a web browser. For example, many popular data analytics libraries, like Tensorflow.js, now support JavaScript SDKs. Another popular library, Danfo.js, makes it possible to wrangle data using familiar Pandas-like operations, shortening the learning curve and arming the typical data engineer or data scientist with another data tool in their toolbox. In this presentation, we’ll explore using the Node.js connector for Delta Sharing to build a data analytics app that summarizes a Twitter dataset."
        ],
        [
         "Creating the First Sports and Entertainment Data Marketplace Powered by Pumpjack Dataworks, Revelate, Immuta and Databricks",
         "Creating a secure and easily actionable marketplace is no simple task. Add to this governance requirements of privacy frameworks and responsibilities of protecting consumer data, and things get harder. With Pumpjack Dataworks partnering with Databricks, Immuta, and Revelate, we bring secure, privacy-focused data products directly to data consumers. With Delta Share, the service solves redundancy of data purchases, time to action, and lowers cost of data acquisition while providing new revenue streams for rights holders. Its a win-win solution for bringing Data Acquisition and Commercial teams together into the new age of data.",
         "Title: Creating the First Sports and Entertainment Data Marketplace Powered by Pumpjack Dataworks, Revelate, Immuta and Databricks\n                Abstract:  Creating a secure and easily actionable marketplace is no simple task. Add to this governance requirements of privacy frameworks and responsibilities of protecting consumer data, and things get harder. With Pumpjack Dataworks partnering with Databricks, Immuta, and Revelate, we bring secure, privacy-focused data products directly to data consumers. With Delta Share, the service solves redundancy of data purchases, time to action, and lowers cost of data acquisition while providing new revenue streams for rights holders. Its a win-win solution for bringing Data Acquisition and Commercial teams together into the new age of data."
        ],
        [
         "Data Interoperability Across Clouds and Regions",
         "L’Oréal has the largest and richest data repository in the beauty industry, with a 110-year heritage solely dedicated to analyzing, measuring and magnifying the beauty needs and desires of consumers around the world. As L’Oréal’s ambition is to become the leader in beauty tech, a profound IT transformation started three years ago to meet their most strategic priorities: \n - Hyper personalization\n - Consumer experience online and offline \n - Preventive care\n - Integrating artificial intelligence into the business\n \n This IT transformation now needs to reach all of their services and teams worldwide with the same level of security, quality and exacting standards. To do so, data must be unlocked and democratized across the world in order to meet their strategic priorities. This is why L’Oréal and Databricks worked together on the data interoperability value proposition in order to :\n - Facilitate multi-cloud interoperability exchanges thanks to open standards technologies\n - Secure and trace through a centralized governance tool to respond to the growing regulatory environment and the application of other data laws (such as in Vietnam)\n - Optimize and rationalize data exchanges to reduce transported volumes and meet L'Oréal's sustainable development objectives\n - Ensure the quality of the data exchanged through quality and validation checks",
         "Title: Data Interoperability Across Clouds and Regions\n                Abstract:  L’Oréal has the largest and richest data repository in the beauty industry, with a 110-year heritage solely dedicated to analyzing, measuring and magnifying the beauty needs and desires of consumers around the world. As L’Oréal’s ambition is to become the leader in beauty tech, a profound IT transformation started three years ago to meet their most strategic priorities: \n - Hyper personalization\n - Consumer experience online and offline \n - Preventive care\n - Integrating artificial intelligence into the business\n \n This IT transformation now needs to reach all of their services and teams worldwide with the same level of security, quality and exacting standards. To do so, data must be unlocked and democratized across the world in order to meet their strategic priorities. This is why L’Oréal and Databricks worked together on the data interoperability value proposition in order to :\n - Facilitate multi-cloud interoperability exchanges thanks to open standards technologies\n - Secure and trace through a centralized governance tool to respond to the growing regulatory environment and the application of other data laws (such as in Vietnam)\n - Optimize and rationalize data exchanges to reduce transported volumes and meet L'Oréal's sustainable development objectives\n - Ensure the quality of the data exchanged through quality and validation checks"
        ],
        [
         "Data Extraction and Sharing Via the Delta Sharing Protocol: Overfetching, Underfetching and Other Lessons and Tips for Development Learned While Building the Delta Sharing Excel Add-in",
         "The Delta Sharing open protocol for secure sharing and distribution of Lakehouse data is designed to reduce friction in getting data to users. Delivering custom data solutions from this protocol further leverages the technical investment committed to your Delta Lake infrastructure.\n \n There are key design and computational concepts unique to Delta Sharing to know when undertaking development. And there are pitfalls and hazards to avoid when delivering modern cloud data to traditional data platforms and users.\n \n In this session we introduce Delta Sharing Protocol development and examine our journey and the lessons learned while creating the Delta Sharing Excel Add-in. We will demonstrate scenarios of overfetching, underfetching, and interpretation of types. We will suggest methods to overcome these development challenges.\n \n The session will combine live demonstrations that exercise the Delta Sharing REST protocol with detailed analysis of the responses. The demonstrations will elaborate on optional capabilities of the protocol’s query mechanism, and how they are used and interpreted in real-life scenarios.\n \n As a reference baseline for data professionals, the Delta Sharing exercises will be framed relative to SQL counterparts. Specific attention will be paid to how they differ, and how Delta Sharing’s Change Data Feed (CDF) can power next-generation data architectures.\n \n The session will conclude with a survey of available integration solutions for getting the most out of your Delta Sharing environment, including frameworks, connectors, and managed services.\n \n Attendees are encouraged to be familiar with REST, JSON, and modern programming concepts. A working knowledge of Delta Lake, the Parquet file format, and the Delta Sharing Protocol are advised.",
         "Title: Data Extraction and Sharing Via the Delta Sharing Protocol: Overfetching, Underfetching and Other Lessons and Tips for Development Learned While Building the Delta Sharing Excel Add-in\n                Abstract:  The Delta Sharing open protocol for secure sharing and distribution of Lakehouse data is designed to reduce friction in getting data to users. Delivering custom data solutions from this protocol further leverages the technical investment committed to your Delta Lake infrastructure.\n \n There are key design and computational concepts unique to Delta Sharing to know when undertaking development. And there are pitfalls and hazards to avoid when delivering modern cloud data to traditional data platforms and users.\n \n In this session we introduce Delta Sharing Protocol development and examine our journey and the lessons learned while creating the Delta Sharing Excel Add-in. We will demonstrate scenarios of overfetching, underfetching, and interpretation of types. We will suggest methods to overcome these development challenges.\n \n The session will combine live demonstrations that exercise the Delta Sharing REST protocol with detailed analysis of the responses. The demonstrations will elaborate on optional capabilities of the protocol’s query mechanism, and how they are used and interpreted in real-life scenarios.\n \n As a reference baseline for data professionals, the Delta Sharing exercises will be framed relative to SQL counterparts. Specific attention will be paid to how they differ, and how Delta Sharing’s Change Data Feed (CDF) can power next-generation data architectures.\n \n The session will conclude with a survey of available integration solutions for getting the most out of your Delta Sharing environment, including frameworks, connectors, and managed services.\n \n Attendees are encouraged to be familiar with REST, JSON, and modern programming concepts. A working knowledge of Delta Lake, the Parquet file format, and the Delta Sharing Protocol are advised."
        ],
        [
         "Data sharing & beyond with Delta Sharing",
         "Stepping into this brave new digital world we are certain that data will be a central product for many organizations. The way to convey their knowledge and their assets will be through data and analytics. Delta Sharing was the world's first open protocol for secure and scalable real-time data sharing. Through our customer conversations, there is a lot of anticipation of how Delta Sharing can be extended to non-tabular assets, such as machine learning experiments and models.\n \n In this talk, we will cover how we extended the Delta Sharing protocol to other sharing workflows, enabling sharing of ML models, arbitrary files and more. The development resulted in Arcuate, a Databricks Labs project with a data sharing flavour. The talk will start with the high-level approach and how it can be extended to cover other similar use cases. It will then move to our implementation and how it integrates seamlessly with Databricks-managed Delta Sharing server and notebooks. We finally conclude with lessons learned, and our visions for a future of data sharing & beyond",
         "Title: Data sharing & beyond with Delta Sharing\n                Abstract:  Stepping into this brave new digital world we are certain that data will be a central product for many organizations. The way to convey their knowledge and their assets will be through data and analytics. Delta Sharing was the world's first open protocol for secure and scalable real-time data sharing. Through our customer conversations, there is a lot of anticipation of how Delta Sharing can be extended to non-tabular assets, such as machine learning experiments and models.\n \n In this talk, we will cover how we extended the Delta Sharing protocol to other sharing workflows, enabling sharing of ML models, arbitrary files and more. The development resulted in Arcuate, a Databricks Labs project with a data sharing flavour. The talk will start with the high-level approach and how it can be extended to cover other similar use cases. It will then move to our implementation and how it integrates seamlessly with Databricks-managed Delta Sharing server and notebooks. We finally conclude with lessons learned, and our visions for a future of data sharing & beyond"
        ],
        [
         "Ad Measurement: From Impressions to Attribution",
         "\"Effectv, the advertising sales division of Comcast, delivers linear and digital advertising to help advertisers reach potential customers. In this talk, Joe Walsh, Director of Attribution & Measurement and Derek Sugden, Ad Measurement Lead, will discuss their team's journey of measuring the impact of advertising campaigns on customer behavior by combining Comcast's household-level exposure data with various types of third-party conversion data sourced externally. They will highlight the challenges of sharing and receiving data securely while preserving customer privacy, and share how they have implemented an internal \"\"clean room\"\" concept to restrict roles and usage of a cluster",
         "Title: Ad Measurement: From Impressions to Attribution\n                Abstract:  \"Effectv, the advertising sales division of Comcast, delivers linear and digital advertising to help advertisers reach potential customers. In this talk, Joe Walsh, Director of Attribution & Measurement and Derek Sugden, Ad Measurement Lead, will discuss their team's journey of measuring the impact of advertising campaigns on customer behavior by combining Comcast's household-level exposure data with various types of third-party conversion data sourced externally. They will highlight the challenges of sharing and receiving data securely while preserving customer privacy, and share how they have implemented an internal \"\"clean room\"\" concept to restrict roles and usage of a cluster"
        ],
        [
         "Clean Room Primer: Using Clean Rooms on Databricks to Utilize More and Better Data in your BI, ML, and Beyond",
         "In this session, Habu’s Chief Product Officer, Matt Karasick and Senior Architect, Anil Puliyeril, will discuss the foundational changes in the ecosystem, the implications of data insights on marketing, analytics, and measurement, and how companies are coming together to collaborate through data clean rooms in new and exciting ways to power mutually beneficial value for their businesses while preserving privacy and governance.\n \n We will delve into the concept and key features of clean room technology and how they can be used to access more and better data for business intelligence (BI), machine learning (ML), and other data-driven initiatives. By examining real-world use cases of clean rooms in action, attendees will gain a clear understanding of the benefits they can bring to industries like CPG, retail, and media & entertainment. \n \n In addition, we will unpack the advantages of using Databricks as a clean room platform, specifically showcasing how interoperable clean rooms can be leveraged to enhance BI, ML and other compute scenarios. By the end of this talk, you will be equipped with the knowledge and inspiration to explore how clean rooms can unlock new collaboration opportunities that drive better outcomes for your business and improved experiences for consumers.",
         "Title: Clean Room Primer: Using Clean Rooms on Databricks to Utilize More and Better Data in your BI, ML, and Beyond\n                Abstract:  In this session, Habu’s Chief Product Officer, Matt Karasick and Senior Architect, Anil Puliyeril, will discuss the foundational changes in the ecosystem, the implications of data insights on marketing, analytics, and measurement, and how companies are coming together to collaborate through data clean rooms in new and exciting ways to power mutually beneficial value for their businesses while preserving privacy and governance.\n \n We will delve into the concept and key features of clean room technology and how they can be used to access more and better data for business intelligence (BI), machine learning (ML), and other data-driven initiatives. By examining real-world use cases of clean rooms in action, attendees will gain a clear understanding of the benefits they can bring to industries like CPG, retail, and media & entertainment. \n \n In addition, we will unpack the advantages of using Databricks as a clean room platform, specifically showcasing how interoperable clean rooms can be leveraged to enhance BI, ML and other compute scenarios. By the end of this talk, you will be equipped with the knowledge and inspiration to explore how clean rooms can unlock new collaboration opportunities that drive better outcomes for your business and improved experiences for consumers."
        ],
        [
         "Embrace First-Party Data Collaboration to Lower Acquisition Costs with Lookalike Audiences in Media Cleanrooms",
         "Third-party tracking is dead! Yet customer acquisition costs continue to rise to meet the demands of customer attention. Lookalike audience modeling leveraging first-party customer data has offered organizations a cost-effective means to identifying and engaging new customers at scale. However, as we move into a privacy-centric era, collaboration with first-party data across organizations becomes increasingly challenging. So how can we work together to meet both needs? The answer is data clean rooms for the lakehouse, fueled by first-party customer data to build lookalike audience profiles and drive highly personalized ads and experiences in media and commerce.\n \n Using Snowplow's generation of granular first-party customer behavioral data with governance metadata and consent tracking, managed through data clean rooms in the lakehouse, organizations can collaborate securely across their first-party datasets. For example, brands and advertisers can join their first-party onsite data with third-party datasets shared by their agency and advertising partners to boost CTR and ROAS with lookalike audiences.\n \nThis session you focus on:\n - Best practices for creating lookalike audiences with first-party customer data to lower acquisition costs\n - See a live demonstration of how audiences derived through the Databricks Lakehouse can be securely exchanged with customers and partners to foster data-driven innovations creating a customer behavioral profile using Snowplow \n - Data engineering and preparation of Snowplow Behavioral Data with DBT and Databricks\n - Development and training of an ML model using PySpark on Databricks to create a model to drive contextual advertising\n - Implementation of the model for scoring\n - Engage in Q&A",
         "Title: Embrace First-Party Data Collaboration to Lower Acquisition Costs with Lookalike Audiences in Media Cleanrooms\n                Abstract:  Third-party tracking is dead! Yet customer acquisition costs continue to rise to meet the demands of customer attention. Lookalike audience modeling leveraging first-party customer data has offered organizations a cost-effective means to identifying and engaging new customers at scale. However, as we move into a privacy-centric era, collaboration with first-party data across organizations becomes increasingly challenging. So how can we work together to meet both needs? The answer is data clean rooms for the lakehouse, fueled by first-party customer data to build lookalike audience profiles and drive highly personalized ads and experiences in media and commerce.\n \n Using Snowplow's generation of granular first-party customer behavioral data with governance metadata and consent tracking, managed through data clean rooms in the lakehouse, organizations can collaborate securely across their first-party datasets. For example, brands and advertisers can join their first-party onsite data with third-party datasets shared by their agency and advertising partners to boost CTR and ROAS with lookalike audiences.\n \nThis session you focus on:\n - Best practices for creating lookalike audiences with first-party customer data to lower acquisition costs\n - See a live demonstration of how audiences derived through the Databricks Lakehouse can be securely exchanged with customers and partners to foster data-driven innovations creating a customer behavioral profile using Snowplow \n - Data engineering and preparation of Snowplow Behavioral Data with DBT and Databricks\n - Development and training of an ML model using PySpark on Databricks to create a model to drive contextual advertising\n - Implementation of the model for scoring\n - Engage in Q&A"
        ],
        [
         null,
         "S&P merged with IHS Markit unlocking massive market opportunities, but it also created organizational and technology challenges that made it difficult to centralize all their data and make it readily available for analytics and AI. This impacted cross-team collaboration and limited their ability to provide innovative solutions for their clients looking to make smarter, sustainable investment choices. There were infrastructure complexities and they significantly increased data volumes that limited their ability to democratize data and insights. Their legacy data warehouse also struggled with unifying alternative data sources with other data sets. This impacted various use cases across the business including credit risk, ESG, and Marketplace Workbench. The Sustainable1 team has spearheaded getting all their datasets readily available for sharing across all of their internal divisions via Delta Share, and also to share S&P data externally with their customers. ",
         "Title: None\n                Abstract:  S&P merged with IHS Markit unlocking massive market opportunities, but it also created organizational and technology challenges that made it difficult to centralize all their data and make it readily available for analytics and AI. This impacted cross-team collaboration and limited their ability to provide innovative solutions for their clients looking to make smarter, sustainable investment choices. There were infrastructure complexities and they significantly increased data volumes that limited their ability to democratize data and insights. Their legacy data warehouse also struggled with unifying alternative data sources with other data sets. This impacted various use cases across the business including credit risk, ESG, and Marketplace Workbench. The Sustainable1 team has spearheaded getting all their datasets readily available for sharing across all of their internal divisions via Delta Share, and also to share S&P data externally with their customers."
        ],
        [
         "How to Create and Manage a High-Performance Analytics Team",
         "Data Science and analytics teams are unique. Large and small corporations want to build and manage analytics teams to convert their data and analytic assets into revenue and competitive advantage, but many are failing before they make their first hire. In this presentation, the audience will learn how to structure, hire, manage and grow an analytics team. Organizational structure, project and program portfolios, neurodiversity, developing talent, and more will be discussed. Questions and discussion will be encouraged and engaged in. The audience will leave with a deeper understanding of how to succeed in turning data and analytics into tangible results.",
         "Title: How to Create and Manage a High-Performance Analytics Team\n                Abstract:  Data Science and analytics teams are unique. Large and small corporations want to build and manage analytics teams to convert their data and analytic assets into revenue and competitive advantage, but many are failing before they make their first hire. In this presentation, the audience will learn how to structure, hire, manage and grow an analytics team. Organizational structure, project and program portfolios, neurodiversity, developing talent, and more will be discussed. Questions and discussion will be encouraged and engaged in. The audience will leave with a deeper understanding of how to succeed in turning data and analytics into tangible results."
        ],
        [
         "What it Takes to Democratize AI and ML in a Large Company: The Importance of User Enablement and Technical Training",
         "The biggest critical factor to success in a Cloud transformation is people. As such, having a change management process in place to manage the impact of the transformation and user enablement is foundational to any large program. In this session we will dive into how TD bank democratizes data, mobilizes our 2000+ analytics user community and the tactics we used to successfully enable new use cases on Cloud. The session will focus on\n \n To democratize data:\n • Centralize a data platform that is accessible to all employees and allow for easy data sharing \n • Implement privacy and security to protect data and use data ethically\n • Compliance and governance for using data in responsible and compliant way\n • Simplification of processes and procedures to reduce redundancy and faster adoption\n\n To mobilize end users:\n • Increase data literacy: provide training and resources for employees to increase their abilities and skills\n • Foster a culture of collaborationand openness: cross-functional teams to collaborate and share ideas\n • Encourage exploration of innovative ideas that impact the organization's values and customers\n \nTechnical enablement and adoption tactics we've used at TD Bank:\n • Hands-on training for over 1300+ analytics users with emphasis on learn by doing, to relate to real-life situations\n • Online tutorials and documentations to be used as self-paced study \n • Workshops and office hours on specific topics to empower business users\n • Coaching to work with teams on a specific use case/complex issue and provide recommendations for a faster, cost effective solutions\n • Offer certification and encourage continuous education for employees to keep up to date with latest \n • Feedback loop: get user feedback on training and user experience to improve future trainings",
         "Title: What it Takes to Democratize AI and ML in a Large Company: The Importance of User Enablement and Technical Training\n                Abstract:  The biggest critical factor to success in a Cloud transformation is people. As such, having a change management process in place to manage the impact of the transformation and user enablement is foundational to any large program. In this session we will dive into how TD bank democratizes data, mobilizes our 2000+ analytics user community and the tactics we used to successfully enable new use cases on Cloud. The session will focus on\n \n To democratize data:\n • Centralize a data platform that is accessible to all employees and allow for easy data sharing \n • Implement privacy and security to protect data and use data ethically\n • Compliance and governance for using data in responsible and compliant way\n • Simplification of processes and procedures to reduce redundancy and faster adoption\n\n To mobilize end users:\n • Increase data literacy: provide training and resources for employees to increase their abilities and skills\n • Foster a culture of collaborationand openness: cross-functional teams to collaborate and share ideas\n • Encourage exploration of innovative ideas that impact the organization's values and customers\n \nTechnical enablement and adoption tactics we've used at TD Bank:\n • Hands-on training for over 1300+ analytics users with emphasis on learn by doing, to relate to real-life situations\n • Online tutorials and documentations to be used as self-paced study \n • Workshops and office hours on specific topics to empower business users\n • Coaching to work with teams on a specific use case/complex issue and provide recommendations for a faster, cost effective solutions\n • Offer certification and encourage continuous education for employees to keep up to date with latest \n • Feedback loop: get user feedback on training and user experience to improve future trainings"
        ],
        [
         "Weaving the Data Mesh in the Department of Defense",
         "The Chief Digital and AI Office (CDAO) was created to lead the strategy and policy on data, analytics, and AI adoption across the Department of Defense. To enable that vision, the Department must achieve new ways to scale and standardize delivery under a global strategy while enabling decentralized workflows that capture the wealth of data and domain expertise. \n \n CDAO’s strategy and goals are aligned with data mesh principles. This alignment starts with providing enterprise-level infrastructure and services to advance the adoption of data, analytics, and AI, creating the self-service data infrastructure as a platform. And it continues through implementing policy for federated computational governance centered around decentralizing data ownership to become domain-oriented but enforcing the quality and trustworthiness of data. CDAO seeks to expand and make enterprise data more accessible through providing data as a product and leveraging a federated data catalog to designate authoritative data and common data models. This results in domain-oriented, decentralized data ownership to empower the business domains across the Department to increase mission and business impact that result in significant cost savings, saving lives, and data serving as a “public good.”\n \n Please join us in our presentation as we discuss how the CDAO leverages modern, innovative implementations that accelerate the delivery of data and AI throughout one of the largest distributed organizations in the world; the Department of Defence.  We will walk through how this enables delivery in various Department of Defence use cases.",
         "Title: Weaving the Data Mesh in the Department of Defense\n                Abstract:  The Chief Digital and AI Office (CDAO) was created to lead the strategy and policy on data, analytics, and AI adoption across the Department of Defense. To enable that vision, the Department must achieve new ways to scale and standardize delivery under a global strategy while enabling decentralized workflows that capture the wealth of data and domain expertise. \n \n CDAO’s strategy and goals are aligned with data mesh principles. This alignment starts with providing enterprise-level infrastructure and services to advance the adoption of data, analytics, and AI, creating the self-service data infrastructure as a platform. And it continues through implementing policy for federated computational governance centered around decentralizing data ownership to become domain-oriented but enforcing the quality and trustworthiness of data. CDAO seeks to expand and make enterprise data more accessible through providing data as a product and leveraging a federated data catalog to designate authoritative data and common data models. This results in domain-oriented, decentralized data ownership to empower the business domains across the Department to increase mission and business impact that result in significant cost savings, saving lives, and data serving as a “public good.”\n \n Please join us in our presentation as we discuss how the CDAO leverages modern, innovative implementations that accelerate the delivery of data and AI throughout one of the largest distributed organizations in the world; the Department of Defence.  We will walk through how this enables delivery in various Department of Defence use cases."
        ],
        [
         "The Modern Composable CX Stack: Deconstructing the Data Lifecycle From Infrastructure to Orchestration",
         " In today’s privacy-conscious world, it’s imperative for IT to have ownership and full control of sensitive customer data. But more than ever, business stakeholders are in dogged pursuit of customer data that can drive better customer understanding and more effective campaigns. For IT, maintaining the integrity of data and architecture while driving customer 360 and other customer experience initiatives can become costly, time consuming and a source of friction in the organization.\n \n Hear from ActionIQ’s SVP of Product Justin DeBrabant and Northwestern Mutual’s Chief Data Officer Don Vu as they discuss how Northwestern Mutual is leveraging Databricks and ActionIQ’s CX Hub to bring together business and technical teams around existing data and architecture investments while enabling powerful CX initiatives.\n \n You’ll learn:\n - How to maintain IT choice, agility and control across the stack with composable technology \n - Strategies to optimize and showcase existing data investments of IT while elevating the stack \n - Tactics to drive business impact from data management, governance and modernization initiatives",
         "Title: The Modern Composable CX Stack: Deconstructing the Data Lifecycle From Infrastructure to Orchestration\n                Abstract:   In today’s privacy-conscious world, it’s imperative for IT to have ownership and full control of sensitive customer data. But more than ever, business stakeholders are in dogged pursuit of customer data that can drive better customer understanding and more effective campaigns. For IT, maintaining the integrity of data and architecture while driving customer 360 and other customer experience initiatives can become costly, time consuming and a source of friction in the organization.\n \n Hear from ActionIQ’s SVP of Product Justin DeBrabant and Northwestern Mutual’s Chief Data Officer Don Vu as they discuss how Northwestern Mutual is leveraging Databricks and ActionIQ’s CX Hub to bring together business and technical teams around existing data and architecture investments while enabling powerful CX initiatives.\n \n You’ll learn:\n - How to maintain IT choice, agility and control across the stack with composable technology \n - Strategies to optimize and showcase existing data investments of IT while elevating the stack \n - Tactics to drive business impact from data management, governance and modernization initiatives"
        ],
        [
         "Leveraging Product Thinking to Scale a Data Platform to a 400-person (Or More!) Data & Analytics Team",
         "There exists a large and complex set of big data and analytics tools. How do you decide which tools to use? When to use them? In what way? And how to ensure availability and productivity? In this session we'll talk about how we organized the data platform team so that we could provide the best data environment for the 400+ engineers and data scientists at Anheuser-Busch InBev. \n \n Data platforms are generally a set of tools that enable data analysis activities. They are built on many different technologies and processes. How to guarantee reliability and constant evolutions for our users was the problem we started to face during the first months after the launch of our Data Platform. To solve this issue, we used an agile mindset and product management framework, already consolidated in software management teams. As a result, we divided the data platform team into squads, each one having its own OKRs and backlog aimed at improving user productivity. \n \n Having well-defined responsibilities for each squad according to the stages of the data journey within the platform made it possible to establish clear priorities for evolution in terms of benefits generated for users and the company. This also allowed us to have a highly specialized operations team to each part of the architecture, as each squad is responsible for keeping its product up and running. \n \n In this session, we will show the evolution of the team's structure over three years, as well as the challenges encountered. In addition, we will show how the daily work of the squads is organized.",
         "Title: Leveraging Product Thinking to Scale a Data Platform to a 400-person (Or More!) Data & Analytics Team\n                Abstract:  There exists a large and complex set of big data and analytics tools. How do you decide which tools to use? When to use them? In what way? And how to ensure availability and productivity? In this session we'll talk about how we organized the data platform team so that we could provide the best data environment for the 400+ engineers and data scientists at Anheuser-Busch InBev. \n \n Data platforms are generally a set of tools that enable data analysis activities. They are built on many different technologies and processes. How to guarantee reliability and constant evolutions for our users was the problem we started to face during the first months after the launch of our Data Platform. To solve this issue, we used an agile mindset and product management framework, already consolidated in software management teams. As a result, we divided the data platform team into squads, each one having its own OKRs and backlog aimed at improving user productivity. \n \n Having well-defined responsibilities for each squad according to the stages of the data journey within the platform made it possible to establish clear priorities for evolution in terms of benefits generated for users and the company. This also allowed us to have a highly specialized operations team to each part of the architecture, as each squad is responsible for keeping its product up and running. \n \n In this session, we will show the evolution of the team's structure over three years, as well as the challenges encountered. In addition, we will show how the daily work of the squads is organized."
        ],
        [
         "Experience the New Era of Data & AI: Taking Bold Steps",
         "Being an organization operating in >65 countries, PETRONAS has a complex and  growing data landscape across its value chain. It is undoubted that digital transformation is a necessity; however, its success is underpinned by data being a strategic enabler. At PETRONAS, we took bold steps in pioneering new frontiers through the undertaking of several strategic initiatives since 2020 to accelerate the digital transformation in PETRONAS:\n \n 1. Liberalization of data & analytics\n Data & analytics are made available and accessible across PETRONAS through a paradigm shift in ways of working with PETRONAS having 'default access rights' to data.\n  \n 2. Insight velocity via single source of truth data platform for analytics\n Enterprise data hub as a dual cloud-based data platform with integrated capabilities and robust technology stacks, whereby data (internal and external) and analytic products that brings the biggest impact and value is curated for insights.\n \n 3. Analytics for everyone\n Everyone can now step-up in using data and advanced analytics without the need for programming skillsets through advanced analytics in EDH that is low-code and augmented by AI\n \n 4. Line of sight across multiple data platforms and applications\n Data Governance Control Tower to provide the line of sight on areas such as metadata management, data access, security and data quality across all data platforms and applications\n \n 5. Modernized data governance adaptive data governance; a modern approach, with centralized governance and decentralised assurance to achieve a centralized, inclusive and resilient data governance, while removing bottlenecks and breaking down siloes through federated execution across the business\n \n PETRONAS has achieved major milestones with the above. As such, we would like to share our knowledge & expertise.",
         "Title: Experience the New Era of Data & AI: Taking Bold Steps\n                Abstract:  Being an organization operating in >65 countries, PETRONAS has a complex and  growing data landscape across its value chain. It is undoubted that digital transformation is a necessity; however, its success is underpinned by data being a strategic enabler. At PETRONAS, we took bold steps in pioneering new frontiers through the undertaking of several strategic initiatives since 2020 to accelerate the digital transformation in PETRONAS:\n \n 1. Liberalization of data & analytics\n Data & analytics are made available and accessible across PETRONAS through a paradigm shift in ways of working with PETRONAS having 'default access rights' to data.\n  \n 2. Insight velocity via single source of truth data platform for analytics\n Enterprise data hub as a dual cloud-based data platform with integrated capabilities and robust technology stacks, whereby data (internal and external) and analytic products that brings the biggest impact and value is curated for insights.\n \n 3. Analytics for everyone\n Everyone can now step-up in using data and advanced analytics without the need for programming skillsets through advanced analytics in EDH that is low-code and augmented by AI\n \n 4. Line of sight across multiple data platforms and applications\n Data Governance Control Tower to provide the line of sight on areas such as metadata management, data access, security and data quality across all data platforms and applications\n \n 5. Modernized data governance adaptive data governance; a modern approach, with centralized governance and decentralised assurance to achieve a centralized, inclusive and resilient data governance, while removing bottlenecks and breaking down siloes through federated execution across the business\n \n PETRONAS has achieved major milestones with the above. As such, we would like to share our knowledge & expertise."
        ],
        [
         "Advancing Customer Centricity: Mission Data's Data & Analytics Transformation",
         "Overview - \n Mission Data was a three-year transformation focused on leveraging data to better understand our members and engage with them in a personalized way to help them better manage their finances. We’re accomplishing this by transforming Navy Federal into a member-centric, data-driven and AI-powered organization. Leveraging member data in new and innovative ways allows us to create a superior member experience by delivering value back to our members to show them that we know them, and this has always been at the core of our Member Centric strategy. Due to the evolving nature of the broader technological marketplace, consumers have come to expect increasing degrees of personalization across their experiences. Navy Federal members expect the same from their financial institutions and Mission Data is enabling the financial service and guidance our members have come to expect through personalized insights. A personalized insight takes the guess work out of managing finances for members. We show them we know them AND we do it for them…From providing target savings guidance, tracking financial goals, or letting members know they’ve reached a milestone achievement. Delivering these types of insights tailored to our member’s individual needs ultimately deepens their relationship and loyalty with Navy Federal as their trusted financial partner. This is an effort we’re accelerating as we democratize the Mission Data Platform—making member data accessible and actionable across the enterprise. In 2019, we developed a strategy and roadmap for Navy Federal to build the data and analytics capabilities needed to deliver these types of experiences. Key Enablers in the Journey: People, Mindset, Change, Adoption.",
         "Title: Advancing Customer Centricity: Mission Data's Data & Analytics Transformation\n                Abstract:  Overview - \n Mission Data was a three-year transformation focused on leveraging data to better understand our members and engage with them in a personalized way to help them better manage their finances. We’re accomplishing this by transforming Navy Federal into a member-centric, data-driven and AI-powered organization. Leveraging member data in new and innovative ways allows us to create a superior member experience by delivering value back to our members to show them that we know them, and this has always been at the core of our Member Centric strategy. Due to the evolving nature of the broader technological marketplace, consumers have come to expect increasing degrees of personalization across their experiences. Navy Federal members expect the same from their financial institutions and Mission Data is enabling the financial service and guidance our members have come to expect through personalized insights. A personalized insight takes the guess work out of managing finances for members. We show them we know them AND we do it for them…From providing target savings guidance, tracking financial goals, or letting members know they’ve reached a milestone achievement. Delivering these types of insights tailored to our member’s individual needs ultimately deepens their relationship and loyalty with Navy Federal as their trusted financial partner. This is an effort we’re accelerating as we democratize the Mission Data Platform—making member data accessible and actionable across the enterprise. In 2019, we developed a strategy and roadmap for Navy Federal to build the data and analytics capabilities needed to deliver these types of experiences. Key Enablers in the Journey: People, Mindset, Change, Adoption."
        ],
        [
         "The C-Level Guide to Data Strategy Success With the Lakehouse",
         "Join us for a practical session on implementing a data strategy leveraging people, process, and technology to meet the growing demands of your business stakeholders for faster innovation at lower cost. Dael Williamson and Robin Sutara, EMEA Field CTOs at Databricks, will share real-world examples on best practices and things to avoid as you drive your strategy from the board to the business units in your organization",
         "Title: The C-Level Guide to Data Strategy Success With the Lakehouse\n                Abstract:  Join us for a practical session on implementing a data strategy leveraging people, process, and technology to meet the growing demands of your business stakeholders for faster innovation at lower cost. Dael Williamson and Robin Sutara, EMEA Field CTOs at Databricks, will share real-world examples on best practices and things to avoid as you drive your strategy from the board to the business units in your organization"
        ],
        [
         "Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse",
         "Abstract\n How customers implemented Data Mesh on Databricks and how standardizing on delta format enabled Delta-to-Delta Share to non-Databricks consumers.\n \n Presentation Agenda:\n \n 1. Current State of the IT Landscape\n a. Data Silos (problems with organizations not having connected data in the ecosystem)\n b. A look back on why we moved away from Data warehouses and choose cloud in the first place\n c. What caused the data chaos in the cloud (instrumentation and too much stitching together) ~ periodic table list of services of the cloud\n \n 2. How to strike the balance between Autonomy and Centralization\n \n 3. Why Databricks Unity Catalog puts you in the right path to implementing Data Mesh strategy\n \n 4. What are the process and features that enable and end-to-end Implementation of a Data Strategy\n \n 5. How customers were able to successfully implement the Data mesh on out of the box Unity Catalog and Delta Sharing without overwhelming their IT tool Stack\n \n 6. Use-Cases\n a. Delta-to-Delta Data Sharing \n b. Delta-to-Others Data Sharing\n \n 7. How do you navigate when data today is available across regions, across clouds,on-prem and external systems\n a. Change Data Feed to share only “data that has changed”\n \n 8. Data stewardship\n a. Why ABAC is important\n b. How file based access policies and governance play an important role\n \n 9. Future State and its pitfalls\n a. Egress Costs\n b. Data compliances",
         "Title: Unity Catalog, Delta Sharing and Data Mesh on Databricks Lakehouse\n                Abstract:  Abstract\n How customers implemented Data Mesh on Databricks and how standardizing on delta format enabled Delta-to-Delta Share to non-Databricks consumers.\n \n Presentation Agenda:\n \n 1. Current State of the IT Landscape\n a. Data Silos (problems with organizations not having connected data in the ecosystem)\n b. A look back on why we moved away from Data warehouses and choose cloud in the first place\n c. What caused the data chaos in the cloud (instrumentation and too much stitching together) ~ periodic table list of services of the cloud\n \n 2. How to strike the balance between Autonomy and Centralization\n \n 3. Why Databricks Unity Catalog puts you in the right path to implementing Data Mesh strategy\n \n 4. What are the process and features that enable and end-to-end Implementation of a Data Strategy\n \n 5. How customers were able to successfully implement the Data mesh on out of the box Unity Catalog and Delta Sharing without overwhelming their IT tool Stack\n \n 6. Use-Cases\n a. Delta-to-Delta Data Sharing \n b. Delta-to-Others Data Sharing\n \n 7. How do you navigate when data today is available across regions, across clouds,on-prem and external systems\n a. Change Data Feed to share only “data that has changed”\n \n 8. Data stewardship\n a. Why ABAC is important\n b. How file based access policies and governance play an important role\n \n 9. Future State and its pitfalls\n a. Egress Costs\n b. Data compliances"
        ],
        [
         "Event Driven Real-Time Supply Chain Ecosystem Powered by Lakehouse",
         "As the backbone of Australia’s supply chain, the Australia Rail Track Corporation (ARTC) plays a vital role in the management and monitoring of goods transportation across 8,500km of its rail network throughout Australia. ARTC provides weighbridges along their track which read train weights as they pass at speeds of up to 60 kilometers an hour. This information is highly valuable and is required both by ARTC and their customers to provide accurate haulage weight details, analyze technical equipment, and help ensure wagons have been loaded correctly. \n \n 750 trains run across a network of 8500 km in a day and generate real-time data at approximately 50 sensor platforms. \n \n With the help of structured streaming and Delta Lake, ARTC was able to analyze and store: \n \n 1. Precise train location\n 2. Weight of the train in real-time\n 3. Train crossing time to the second level \n 4. Train speed, temperature, sound frequency, and friction \n 5. Train schedule lookups \n \n Once all the IoT data has been pulled together from an IoT event hub, it is processed in real-time using structured streaming and stored in Delta Lake. To understand the train GPS location API calls are then made per minute per train from the Lakehouse. API calls are made in real-time to another scheduling system to lookup customer info. Once the processed, enriched, data is stored in Delta Lake, an API layer was also created on top of it to expose this data to all consumers. \n \n Outcome:\n \n Increased transparency on weight data as it is now made available to customers. \nA digital data ecosystem that now ARTC’s customers use to meet their KPIs/ planning. \nAbility to determine temporary speed restrictions across the network to improve train scheduling accuracy and also schedule network maintenance based on train schedules/ speed.",
         "Title: Event Driven Real-Time Supply Chain Ecosystem Powered by Lakehouse\n                Abstract:  As the backbone of Australia’s supply chain, the Australia Rail Track Corporation (ARTC) plays a vital role in the management and monitoring of goods transportation across 8,500km of its rail network throughout Australia. ARTC provides weighbridges along their track which read train weights as they pass at speeds of up to 60 kilometers an hour. This information is highly valuable and is required both by ARTC and their customers to provide accurate haulage weight details, analyze technical equipment, and help ensure wagons have been loaded correctly. \n \n 750 trains run across a network of 8500 km in a day and generate real-time data at approximately 50 sensor platforms. \n \n With the help of structured streaming and Delta Lake, ARTC was able to analyze and store: \n \n 1. Precise train location\n 2. Weight of the train in real-time\n 3. Train crossing time to the second level \n 4. Train speed, temperature, sound frequency, and friction \n 5. Train schedule lookups \n \n Once all the IoT data has been pulled together from an IoT event hub, it is processed in real-time using structured streaming and stored in Delta Lake. To understand the train GPS location API calls are then made per minute per train from the Lakehouse. API calls are made in real-time to another scheduling system to lookup customer info. Once the processed, enriched, data is stored in Delta Lake, an API layer was also created on top of it to expose this data to all consumers. \n \n Outcome:\n \n Increased transparency on weight data as it is now made available to customers. \nA digital data ecosystem that now ARTC’s customers use to meet their KPIs/ planning. \nAbility to determine temporary speed restrictions across the network to improve train scheduling accuracy and also schedule network maintenance based on train schedules/ speed."
        ],
        [
         "Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark™",
         "Apache Spark™’s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important.\n \n Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa’s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data.",
         "Title: Streaming Tens of Millions of Events Made Simple at Visa Using Apache Spark™\n                Abstract:  Apache Spark™’s continuous streaming approach works really well with the data lakehouse architecture. However, building streaming pipelines at VISA scale has two types of challenges, the first of building a single pipeline - performance, debuggability and the guarantees, and the second of scaling it to multiple teams and users including the business teams where productivity, standardization, reusability are very important.\n \n Sr. Director Durga Kala and Principal Data Engineer Varun Sharma, wanted to enable their Visa data engineers to quickly prototype, share their code base and be productive on ever increasing workloads. They introduced a new approach to building a low-code framework on the Apache Spark ecosystem, that allows users to create new pipelines quickly, while handling the scale of tens of millions of transactions an hour, fault tolerance, and satisfying rigid security requirements out of the box. The result? Visa’s development cycles shrank from weeks to days and various business teams can now leverage low-latency streaming data."
        ],
        [
         "Taking Control of Streaming Healthcare Data",
         "Chesapeake Regional Information System for our Patients (CRISP), a nonprofit healthcare information exchange (HIE), initially partnered with Slalom to build a Databricks data lakehouse architecture in response to the analytics demands of the COVID-19 pandemic, since then they have expanded the platform to additional use cases. Recently they have worked together to engineer streaming data pipelines to process healthcare messages, such as HL7, to help CRISP become vendor independent.\n \n Our talk will focus on the improvements CRISP has made to their data lakehouse platform to support streaming use cases and the impact these changes have had for the organization. We will touch on using Databricks Auto Loader to efficiently ingest incoming files, ensuring data quality with Delta Live Tables, and sharing data internally with a SQL warehouse, as well as some of the work CRISP has done to parse and standardize HL7 messages from hundreds of sources. These efforts have allowed CRISP to stream over 4 million messages daily in near real-time with the scalability it needs to continue to onboard new healthcare providers so it can continue to facilitate care and improve health outcomes.",
         "Title: Taking Control of Streaming Healthcare Data\n                Abstract:  Chesapeake Regional Information System for our Patients (CRISP), a nonprofit healthcare information exchange (HIE), initially partnered with Slalom to build a Databricks data lakehouse architecture in response to the analytics demands of the COVID-19 pandemic, since then they have expanded the platform to additional use cases. Recently they have worked together to engineer streaming data pipelines to process healthcare messages, such as HL7, to help CRISP become vendor independent.\n \n Our talk will focus on the improvements CRISP has made to their data lakehouse platform to support streaming use cases and the impact these changes have had for the organization. We will touch on using Databricks Auto Loader to efficiently ingest incoming files, ensuring data quality with Delta Live Tables, and sharing data internally with a SQL warehouse, as well as some of the work CRISP has done to parse and standardize HL7 messages from hundreds of sources. These efforts have allowed CRISP to stream over 4 million messages daily in near real-time with the scalability it needs to continue to onboard new healthcare providers so it can continue to facilitate care and improve health outcomes."
        ],
        [
         "Deploying the Lakehouse to Improve the Viewer Experience on Discovery+",
         "In this session we will discuss how real-time data streaming can be used to gain insights into user behavior and preferences, and how this data is being used to provide personalized content and recommendations on discovery+. We will examine techniques that enables faster decision making and insights on accurate real time data including data masking and data validation. To enable a wide set of data consumers from data engineers to data scientists to data analysts, we will discuss how Unity Catalog is leveraged for secure data access and sharing while still allowing teams flexibility. Operating at this scale requires examining the value being created by the data being processed and optimizing along the way and we will share some of our success in this area.",
         "Title: Deploying the Lakehouse to Improve the Viewer Experience on Discovery+\n                Abstract:  In this session we will discuss how real-time data streaming can be used to gain insights into user behavior and preferences, and how this data is being used to provide personalized content and recommendations on discovery+. We will examine techniques that enables faster decision making and insights on accurate real time data including data masking and data validation. To enable a wide set of data consumers from data engineers to data scientists to data analysts, we will discuss how Unity Catalog is leveraged for secure data access and sharing while still allowing teams flexibility. Operating at this scale requires examining the value being created by the data being processed and optimizing along the way and we will share some of our success in this area."
        ],
        [
         "Apache Spark™ Streaming and Delta Live Tables Accelerates KPMG Clients For Real-Time IoT Insights",
         "Unplanned downtime in manufacturing costs firms up to a trillion dollars annually. Time that materials spend sitting on a production line is lost revenue. Even just 15 hours of downtime a week adds up to over 800 hours of downtime yearly. The use of internet of things or IoT devices can cut this time down by providing details of machine metrics. However, IoT predictive maintenance is challenged by the lack of effective, scalable infrastructure and machine learning solutions. IoT data can be the size of multiple terabytes per day and can come in a variety of formats. Furthermore, without any insights and analysis, this data becomes just another table. \n \n The KPMG Databricks IoT Accelerator is an end-to-end solution enabling manufacturing plant operators to have a bird’s eye view of their machines’ health and empowers proactive machine maintenance across their portfolio of IoT devices. The Databricks Accelerator ingests IoT streaming data at scale and implements the Databricks medallion architecture while leveraging delta live tables to clean and process data. Real time machine learning models are developed from IoT machine measurements and are managed in MLflow. The AI predictions and IoT device readings are compiled in the gold table powering downstream dashboards like Tableau. Dashboards inform machine operators of not only machines’ ailments, but action they can take to mitigate issues before they arise. Operators can see fault history to aid in understanding failure trends, and can filter dashboards by fault type, machine, or specific sensor reading.",
         "Title: Apache Spark™ Streaming and Delta Live Tables Accelerates KPMG Clients For Real-Time IoT Insights\n                Abstract:  Unplanned downtime in manufacturing costs firms up to a trillion dollars annually. Time that materials spend sitting on a production line is lost revenue. Even just 15 hours of downtime a week adds up to over 800 hours of downtime yearly. The use of internet of things or IoT devices can cut this time down by providing details of machine metrics. However, IoT predictive maintenance is challenged by the lack of effective, scalable infrastructure and machine learning solutions. IoT data can be the size of multiple terabytes per day and can come in a variety of formats. Furthermore, without any insights and analysis, this data becomes just another table. \n \n The KPMG Databricks IoT Accelerator is an end-to-end solution enabling manufacturing plant operators to have a bird’s eye view of their machines’ health and empowers proactive machine maintenance across their portfolio of IoT devices. The Databricks Accelerator ingests IoT streaming data at scale and implements the Databricks medallion architecture while leveraging delta live tables to clean and process data. Real time machine learning models are developed from IoT machine measurements and are managed in MLflow. The AI predictions and IoT device readings are compiled in the gold table powering downstream dashboards like Tableau. Dashboards inform machine operators of not only machines’ ailments, but action they can take to mitigate issues before they arise. Operators can see fault history to aid in understanding failure trends, and can filter dashboards by fault type, machine, or specific sensor reading."
        ],
        [
         "Optimizing batch and streaming aggregations",
         "I've got a client recently who asked me to optimize their batch and streaming workloads. It happened to be aggregations using DataFrame.groupBy operation with a custom Scala UDAF over a data stream from Kafka. Just a single simple-looking request that turned itself up into a a-few-month-long hunt to find a more performant query execution planning than ObjectHashAggregateExec that kept falling back to a sort-based aggregation (i.e. the worst possible aggregation runtime performance). It quickly taught us that an aggregation using a custom Scala UDAF cannot be planned other than ObjectHashAggregateExec but at least tasks don't always have to fall back. And that's just batch workloads. When you throw in streaming semantics and think of the different output modes, windowing and streaming watermark optimizing aggregation can take a long time to do right.\n \n During the talk you'll learn the different execution strategies of aggregation (groupBy) in Apache Spark and what issues I faced while hunting down the tiniest performance improvements. You will learn why an aggregation with a Scala UDAF cannot be planned using HashAggregateExec and what problems you may come across while using streaming aggregation over the built-in Kafka data source. Lots of optimization tips and tricks with quite a few looks at the internals of aggregation query execution.",
         "Title: Optimizing batch and streaming aggregations\n                Abstract:  I've got a client recently who asked me to optimize their batch and streaming workloads. It happened to be aggregations using DataFrame.groupBy operation with a custom Scala UDAF over a data stream from Kafka. Just a single simple-looking request that turned itself up into a a-few-month-long hunt to find a more performant query execution planning than ObjectHashAggregateExec that kept falling back to a sort-based aggregation (i.e. the worst possible aggregation runtime performance). It quickly taught us that an aggregation using a custom Scala UDAF cannot be planned other than ObjectHashAggregateExec but at least tasks don't always have to fall back. And that's just batch workloads. When you throw in streaming semantics and think of the different output modes, windowing and streaming watermark optimizing aggregation can take a long time to do right.\n \n During the talk you'll learn the different execution strategies of aggregation (groupBy) in Apache Spark and what issues I faced while hunting down the tiniest performance improvements. You will learn why an aggregation with a Scala UDAF cannot be planned using HashAggregateExec and what problems you may come across while using streaming aggregation over the built-in Kafka data source. Lots of optimization tips and tricks with quite a few looks at the internals of aggregation query execution."
        ],
        [
         "Structured Streaming: Demystifying Arbitrary Stateful Operations",
         "Let’s face it -- data is messy. And your company’s business requirements? Even messier! You’re staring at your screen, knowing there is a tool that will let you give your business partners the information they need as quickly as they need it. There’s even a Python version of it now. But…it looks kinda scary. You’ve never used it before, and you don’t know where to start. Yes, we’re talking about the dreaded flatMapGroupsWithState! But fear not - we’ve got you covered. In this session we’ll take a real-word use case and use it to show you how to break down flatMapGroupsWithState into its basic building blocks. We’ll explain each piece in both Scala and the newly-released Python, and at the end we’ll illustrate how it all comes together to enable the implementation of arbitrary stateful operations with Spark Structured Streaming.",
         "Title: Structured Streaming: Demystifying Arbitrary Stateful Operations\n                Abstract:  Let’s face it -- data is messy. And your company’s business requirements? Even messier! You’re staring at your screen, knowing there is a tool that will let you give your business partners the information they need as quickly as they need it. There’s even a Python version of it now. But…it looks kinda scary. You’ve never used it before, and you don’t know where to start. Yes, we’re talking about the dreaded flatMapGroupsWithState! But fear not - we’ve got you covered. In this session we’ll take a real-word use case and use it to show you how to break down flatMapGroupsWithState into its basic building blocks. We’ll explain each piece in both Scala and the newly-released Python, and at the end we’ll illustrate how it all comes together to enable the implementation of arbitrary stateful operations with Spark Structured Streaming."
        ],
        [
         "How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience",
         "Disney+ uses Amazon Kinesis and Databricks Apache Spark™ streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers.",
         "Title: How Disney+ uses Amazon Kinesis and Databricks to Deliver Personalized Customer Experience\n                Abstract:  Disney+ uses Amazon Kinesis and Databricks Apache Spark™ streaming to drive real-time actions like providing title recommendations for customers, identifying customer trends, and building data warehouse for operational analytics to improve the customer experience. In this session, you will learn how Disney+ built real-time, data-driven capabilities on a unified streaming and analytics platform. This platform ingests billions of events per hour in Amazon Kinesis Data Streams, processes and analyzes that data in Databricks Spark Streaming to generate insights. Hear how Amazon Kinesis and Databricks have helped Disney+ scale its video streaming platform to hundreds of millions of customers."
        ],
        [
         "The Future is Open: Data Streaming in an Omni-Cloud Reality",
         "An assortment of low-code connectors boasts the ability to make data available for analytics in real time. However, many such turn-key solutions face challenges: cost-inefficient EDW targets; inability to evolve schema; forbiddingly expensive data exports due to cloud and vendor lock-in. The alternative: an open data lake that unifies batch and streaming workloads. Increasingly, we must adapt to a multi-cloud reality. Delta Lake decouples data storage from proprietary formats, dramatically reducing data extraction costs. Exporting data from delta tables eliminates the compute and storage API costs required by EDW. Bronze landing zones in Delta format build the foundation for a medallion architecture. Apache Spark™ Structured Streaming provides a unified ingestion strategy. Streaming triggers allow us to switch back and forth between batch and stream with one-line code changes. Streaming aggregation enables us to incrementally compute on data that arrives near each other.\n It is a misconception that streaming requires always-on clusters and thus is prohibitively expensive. Streaming means the ability to automatically handle new data and avoid reprocessing of historical data; real time is optional depending on use cases. Autoloader is a technique to discover newly arrived data and ensure exactly once, incremental processing. DLT further simplifies streaming jobs, accelerating the development cycle.\n Workflows bring it all together and the open source dbx project applies SWE best practices. Workflows can orchestrate tasks such as DLT pipelines, spark jobs, SQL scripts, as well as dbt transformations. Developers can define compute resources, scheduling, and dependencies in descriptive yaml format. Both task definitions and job code are versioned with popular source control systems.",
         "Title: The Future is Open: Data Streaming in an Omni-Cloud Reality\n                Abstract:  An assortment of low-code connectors boasts the ability to make data available for analytics in real time. However, many such turn-key solutions face challenges: cost-inefficient EDW targets; inability to evolve schema; forbiddingly expensive data exports due to cloud and vendor lock-in. The alternative: an open data lake that unifies batch and streaming workloads. Increasingly, we must adapt to a multi-cloud reality. Delta Lake decouples data storage from proprietary formats, dramatically reducing data extraction costs. Exporting data from delta tables eliminates the compute and storage API costs required by EDW. Bronze landing zones in Delta format build the foundation for a medallion architecture. Apache Spark™ Structured Streaming provides a unified ingestion strategy. Streaming triggers allow us to switch back and forth between batch and stream with one-line code changes. Streaming aggregation enables us to incrementally compute on data that arrives near each other.\n It is a misconception that streaming requires always-on clusters and thus is prohibitively expensive. Streaming means the ability to automatically handle new data and avoid reprocessing of historical data; real time is optional depending on use cases. Autoloader is a technique to discover newly arrived data and ensure exactly once, incremental processing. DLT further simplifies streaming jobs, accelerating the development cycle.\n Workflows bring it all together and the open source dbx project applies SWE best practices. Workflows can orchestrate tasks such as DLT pipelines, spark jobs, SQL scripts, as well as dbt transformations. Developers can define compute resources, scheduling, and dependencies in descriptive yaml format. Both task definitions and job code are versioned with popular source control systems."
        ],
        [
         "Getting Insight From Anything: Gathering Data With IoT Devices and Delta Live",
         "The drive to make data-driven decisions shapes the strategy of many companies. However, this dream has a problem; just because you need to make a decision does not mean you have the data to make it! Often these decisions require data from sources typically unconnected to the internet. This could be an architect determined to understand how the construction of her building is progressing, or a manufacturer wanting to live their best Industry 4.0 life. It's not uncommon for companies to find this journey to become “smart” is a long one; with difficult ethical ground to cover, or prohibitive costs associated with procuring the necessary equipment. However, the development of cheap and accessible IoT devices allows for the gathering of data from anything to be more within reach than ever.\n \n This talk will look at efforts to build proof-of-concept IoT systems. We will discuss our (continuing) journey of using and managing hardware and the problems we faced along the way. The main focus however will be the software stack. Azure IoT Hub is used for device management, with Power BI supporting the front end. However, the centre-piece is Delta Live Tables - allowing us a method of near real-time analysis of our data. We will discuss the advantages of using Delta Live for this, and how we wish to extend our IoT systems in the future.",
         "Title: Getting Insight From Anything: Gathering Data With IoT Devices and Delta Live\n                Abstract:  The drive to make data-driven decisions shapes the strategy of many companies. However, this dream has a problem; just because you need to make a decision does not mean you have the data to make it! Often these decisions require data from sources typically unconnected to the internet. This could be an architect determined to understand how the construction of her building is progressing, or a manufacturer wanting to live their best Industry 4.0 life. It's not uncommon for companies to find this journey to become “smart” is a long one; with difficult ethical ground to cover, or prohibitive costs associated with procuring the necessary equipment. However, the development of cheap and accessible IoT devices allows for the gathering of data from anything to be more within reach than ever.\n \n This talk will look at efforts to build proof-of-concept IoT systems. We will discuss our (continuing) journey of using and managing hardware and the problems we faced along the way. The main focus however will be the software stack. Azure IoT Hub is used for device management, with Power BI supporting the front end. However, the centre-piece is Delta Live Tables - allowing us a method of near real-time analysis of our data. We will discuss the advantages of using Delta Live for this, and how we wish to extend our IoT systems in the future."
        ],
        [
         "Practical Pipelines: A Houseplant Alerting System With ksqlDB",
         "Houseplants can be hard; in many cases, over-watering and under-watering can have the same symptoms. Take away the guesswork involved in caring for your houseplants while also gaining valuable experience in building a practical, event-driven pipeline in your own home! This session explores the process of building a houseplant monitoring and alerting system using a Raspberry Pi and Apache Kafka. \n \n Moisture and temperature readings are captured from sensors in the soil and streamed into Kafka. From here, we’ll use stream processing to transform the data, create a summary view of the current state, and drive real-time push alerts through Telegram. In this session, I’ll talk about how I ingest the data followed by the tools – including ksqlDB and Kafka Connect – that help transform the raw data into useful information, and finally, I’ll show how to use Kafka Producers and Consumers to make the entire application more interactive.\n \n By the end of this session you’ll have everything you need to start building practical streaming pipelines in your own home. Roll up your sleeves – let’s get our hands dirty!",
         "Title: Practical Pipelines: A Houseplant Alerting System With ksqlDB\n                Abstract:  Houseplants can be hard; in many cases, over-watering and under-watering can have the same symptoms. Take away the guesswork involved in caring for your houseplants while also gaining valuable experience in building a practical, event-driven pipeline in your own home! This session explores the process of building a houseplant monitoring and alerting system using a Raspberry Pi and Apache Kafka. \n \n Moisture and temperature readings are captured from sensors in the soil and streamed into Kafka. From here, we’ll use stream processing to transform the data, create a summary view of the current state, and drive real-time push alerts through Telegram. In this session, I’ll talk about how I ingest the data followed by the tools – including ksqlDB and Kafka Connect – that help transform the raw data into useful information, and finally, I’ll show how to use Kafka Producers and Consumers to make the entire application more interactive.\n \n By the end of this session you’ll have everything you need to start building practical streaming pipelines in your own home. Roll up your sleeves – let’s get our hands dirty!"
        ],
        [
         "Disaster Recovery Strategies for Structured Streams",
         "In recent years, many businesses have adopted real-time streaming applications to enable faster decision making, quicker predictions and improved customer experiences. Few of these applications are driving critical business use cases like financial fraud detection, loan application processing, personalized offers, etc. These business critical applications need robust disaster recovery strategies to recover from the catastrophic events to reduce the lost uptime. However, most organizations find it hard to set up disaster recovery for streaming applications as it involves continuous data flow. Streaming state and temporal behavior of data brings additional complexities to the DR strategy.\n \n A reliable disaster recovery strategy includes backup, failover and failback approaches for the streaming application. Unlike the batch applications, these steps include many moving elements and need a very sophisticated approach to ensure that the services are failing over the DR region and meet the set RTO and RPO requirements. In this session, we will cover following topics with a FINSERV use case demo:\n 1. Backup strategy: backup of delta tables, message bus services and checkpoint including offsets\n 2. Failover strategy: failover strategy to disable services in the primary region and start the services in the secondary region with minimum data loss \n 3. Failback strategy: failback strategy to restart the services in the primary region once all the services are restored \n 4. Common challenges and best practices for backup",
         "Title: Disaster Recovery Strategies for Structured Streams\n                Abstract:  In recent years, many businesses have adopted real-time streaming applications to enable faster decision making, quicker predictions and improved customer experiences. Few of these applications are driving critical business use cases like financial fraud detection, loan application processing, personalized offers, etc. These business critical applications need robust disaster recovery strategies to recover from the catastrophic events to reduce the lost uptime. However, most organizations find it hard to set up disaster recovery for streaming applications as it involves continuous data flow. Streaming state and temporal behavior of data brings additional complexities to the DR strategy.\n \n A reliable disaster recovery strategy includes backup, failover and failback approaches for the streaming application. Unlike the batch applications, these steps include many moving elements and need a very sophisticated approach to ensure that the services are failing over the DR region and meet the set RTO and RPO requirements. In this session, we will cover following topics with a FINSERV use case demo:\n 1. Backup strategy: backup of delta tables, message bus services and checkpoint including offsets\n 2. Failover strategy: failover strategy to disable services in the primary region and start the services in the secondary region with minimum data loss \n 3. Failback strategy: failback strategy to restart the services in the primary region once all the services are restored \n 4. Common challenges and best practices for backup"
        ],
        [
         "Near Real-Time Data Linkage and Entity Resolution for High Volumes of Continuous and Unsynchronized Data Streams",
         "Citrix Analytics is a cloud-based service that provides analytical insights from data across Citrix product portfolio. It collates data from various sources and proactively detects security threats.\n \n The data consists of information from various entities like users, devices, applications, network, files and shares, along with their correlation over a period. This data is analyzed by various ML algorithms to detect any suspicious activity. The ML models need correlated data in real-time so vulnerabilities can be detected instantly and remediation actions can be taken.\n \n The data pipeline comprises of multiple event sources, each one of them producing high volumes of continuous data. This data generated from different sources need to be correlated using common identifiers to produce final stream of data with low latency and high data quality.\n \n The proposed solution is to build scalable, extensible, fault-tolerant system for stateful joining of two or more high volume event streams that are not fully synchronised, event ordering is not guaranteed, and certain events arrive a bit late. The system also ensures to combine the events/link so the data is in near real-time with low latency so that there is no impact on the downstream applications like ML models in determining the suspicious behavior. Apart from combining events the system also ensures to propagate the needed entities to other product streams to help in entity resolution. If any of the needed data is yet to arrive, we can configure few parameters based on use cases, so as to achieve the required eventual or attribute consistency.\n \n The architecture is an agnostic streaming framework, but our implementation leverages Spark Streaming and Kafka. The solution is implemented and is processing billions of events per day.",
         "Title: Near Real-Time Data Linkage and Entity Resolution for High Volumes of Continuous and Unsynchronized Data Streams\n                Abstract:  Citrix Analytics is a cloud-based service that provides analytical insights from data across Citrix product portfolio. It collates data from various sources and proactively detects security threats.\n \n The data consists of information from various entities like users, devices, applications, network, files and shares, along with their correlation over a period. This data is analyzed by various ML algorithms to detect any suspicious activity. The ML models need correlated data in real-time so vulnerabilities can be detected instantly and remediation actions can be taken.\n \n The data pipeline comprises of multiple event sources, each one of them producing high volumes of continuous data. This data generated from different sources need to be correlated using common identifiers to produce final stream of data with low latency and high data quality.\n \n The proposed solution is to build scalable, extensible, fault-tolerant system for stateful joining of two or more high volume event streams that are not fully synchronised, event ordering is not guaranteed, and certain events arrive a bit late. The system also ensures to combine the events/link so the data is in near real-time with low latency so that there is no impact on the downstream applications like ML models in determining the suspicious behavior. Apart from combining events the system also ensures to propagate the needed entities to other product streams to help in entity resolution. If any of the needed data is yet to arrive, we can configure few parameters based on use cases, so as to achieve the required eventual or attribute consistency.\n \n The architecture is an agnostic streaming framework, but our implementation leverages Spark Streaming and Kafka. The solution is implemented and is processing billions of events per day."
        ],
        [
         "Realtime ML in Marketplace @ Lyft",
         "Lyft is a ride-sharing company which is a two-sided marketplace; balancing supply and demand using various levers (passenger pricing, driver incentive etc.) to maintain an efficient system. Lyft has built a real-time optimization platform that helps to build the product faster. This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. This infrastructure consumes a massive number of events from different sources to make real-time product decisions.\n Rakesh discusses how Lyft organically evolved and scaled the streaming platform that provides a consistent view of the marketplace to aid an individual team independently run their optimization. The platform offers online and offline feature access that helps teams to back test their model in the future. It provides various other powerful capabilities such as replaying the production ML feature in PyNotebook, feature validation, near-realtime model training, executing multi-layer of models in a DAG, etc. \n He is also going to elaborate things that helped him scale the systems to process millions of events per minute and power T0 products with tighter latency SLA.",
         "Title: Realtime ML in Marketplace @ Lyft\n                Abstract:  Lyft is a ride-sharing company which is a two-sided marketplace; balancing supply and demand using various levers (passenger pricing, driver incentive etc.) to maintain an efficient system. Lyft has built a real-time optimization platform that helps to build the product faster. This complex system makes real-time decisions using various data sources; machine learning models; and a streaming infrastructure for low latency, reliability and scalability. This infrastructure consumes a massive number of events from different sources to make real-time product decisions.\n Rakesh discusses how Lyft organically evolved and scaled the streaming platform that provides a consistent view of the marketplace to aid an individual team independently run their optimization. The platform offers online and offline feature access that helps teams to back test their model in the future. It provides various other powerful capabilities such as replaying the production ML feature in PyNotebook, feature validation, near-realtime model training, executing multi-layer of models in a DAG, etc. \n He is also going to elaborate things that helped him scale the systems to process millions of events per minute and power T0 products with tighter latency SLA."
        ],
        [
         "Streaming Data Analytics With Power BI and Databricks",
         "In this session we will present a series of end-to-end technical demos illustrating the synergy between Databricks and Power BI for streaming use cases. \n  - Scenario 1: DLT + Power BI Direct Query and Auto Refresh\n  - Scenario 2: Structured Streaming + Power BI streaming datasets \n  - Scenario 3: DLT + Power BI composite datasets\n  - Considerations when to choose which scenario",
         "Title: Streaming Data Analytics With Power BI and Databricks\n                Abstract:  In this session we will present a series of end-to-end technical demos illustrating the synergy between Databricks and Power BI for streaming use cases. \n  - Scenario 1: DLT + Power BI Direct Query and Auto Refresh\n  - Scenario 2: Structured Streaming + Power BI streaming datasets \n  - Scenario 3: DLT + Power BI composite datasets\n  - Considerations when to choose which scenario"
        ],
        [
         "How Coinbase Built and Optimized SOON, a Streaming Ingestion Framework",
         "Data with low latency is important for real-time incident analysis and metrics. Though we have up-to-date data in OLTP databases, they cannot support those scenarios. Data need to be replicated to a data warehouse to serve queries doing group-bys, and joins across tables from different systems. At Coinbase, we designed SOON (Spark cOntinuOus iNgestion) based on Kafka, Kafka Connect, and Spark as an incremental table replication solution to replicate tables of any size from any database to Delta Lake in a timely manner. It also supports Kafka events ingestion naturally.\n \n SOON incrementally ingests Kafka events as appends, updates, and deletes to an existing table on Delta Lake. The events are grouped into two categories: CDC (change data capture) events generated by Kafka Connect source connectors, and non-CDC events by the frontend or backend services. Both types can be appended or merged into the Delta Lake. Non-CDC events can be in any format, but CDC events must be in the standard SOON CDC schema. We implemented Kafka Connect SMTs to transform raw CDC events into this standardized format. SOON unifies all streaming ingestion scenarios such that users only need to learn one onboarding experience and the team only needs to maintain one framework.\n \n We care about the ingestion performance. The biggest append-only table onboarded has ingress traffic at hundreds of thousands events per second; the biggest CDC-merge table onboarded has a snapshot size of a few TBs and CDC update traffic at hundreds of events per second. A lot of innovative ideas are incorporated in SOON to improve its performance, such as min-max range merge optimization, KMeans merge optimization, no-update merge for deduplication, generated columns as partitions, etc.",
         "Title: How Coinbase Built and Optimized SOON, a Streaming Ingestion Framework\n                Abstract:  Data with low latency is important for real-time incident analysis and metrics. Though we have up-to-date data in OLTP databases, they cannot support those scenarios. Data need to be replicated to a data warehouse to serve queries doing group-bys, and joins across tables from different systems. At Coinbase, we designed SOON (Spark cOntinuOus iNgestion) based on Kafka, Kafka Connect, and Spark as an incremental table replication solution to replicate tables of any size from any database to Delta Lake in a timely manner. It also supports Kafka events ingestion naturally.\n \n SOON incrementally ingests Kafka events as appends, updates, and deletes to an existing table on Delta Lake. The events are grouped into two categories: CDC (change data capture) events generated by Kafka Connect source connectors, and non-CDC events by the frontend or backend services. Both types can be appended or merged into the Delta Lake. Non-CDC events can be in any format, but CDC events must be in the standard SOON CDC schema. We implemented Kafka Connect SMTs to transform raw CDC events into this standardized format. SOON unifies all streaming ingestion scenarios such that users only need to learn one onboarding experience and the team only needs to maintain one framework.\n \n We care about the ingestion performance. The biggest append-only table onboarded has ingress traffic at hundreds of thousands events per second; the biggest CDC-merge table onboarded has a snapshot size of a few TBs and CDC update traffic at hundreds of events per second. A lot of innovative ideas are incorporated in SOON to improve its performance, such as min-max range merge optimization, KMeans merge optimization, no-update merge for deduplication, generated columns as partitions, etc."
        ],
        [
         "High Volume Intelligent Streaming with Sub-Minute SLA for Near Real-Time Data Replication",
         "Come and learn an innovative solution built around Databricks structured streaming and Delta Live Tables (DLT) to replicate thousands of tables from on-premisis to cloud-based relational databases. A highly desirable pattern for many enterprises across the industries to replicate on-premises data to cloud-based data lakes and data stores in near real time for consumption. This powerful architecture can offload legacy platform workloads and accelerate cloud journey. The intelligent cost-efficient solution leverages thread-pools, multi-task jobs, Kafka, spark structured streaming and DLT. This session will go into detail about problems, solutions, lessons-learned and best practices.",
         "Title: High Volume Intelligent Streaming with Sub-Minute SLA for Near Real-Time Data Replication\n                Abstract:  Come and learn an innovative solution built around Databricks structured streaming and Delta Live Tables (DLT) to replicate thousands of tables from on-premisis to cloud-based relational databases. A highly desirable pattern for many enterprises across the industries to replicate on-premises data to cloud-based data lakes and data stores in near real time for consumption. This powerful architecture can offload legacy platform workloads and accelerate cloud journey. The intelligent cost-efficient solution leverages thread-pools, multi-task jobs, Kafka, spark structured streaming and DLT. This session will go into detail about problems, solutions, lessons-learned and best practices."
        ],
        [
         "Using Cisco Spaces Firehose API as a Stream of Data for Real-Time Occupancy Modelling",
         "Honeywell manages the control of equipment for hundreds of thousands of buildings worldwide. Many of our outcomes relating to energy and comfort rely on knowing where people are in the building at any one time. This is so we can target health and comfort conditions more suitably to areas where are more densely populated. Many of these buildings have Cisco IT infrastructure in them. Using their WIFI points and the RSSI signal strength from people’s laptops and phones, Cisco can calculate the number of people in each area of the building. Cisco Spaces offer this data up as a real-time streaming source. Honeywell HBT has utilised this stream of data by writing delta live table pipelines to consume this data source.\n Honeywell buildings can now receive this firehose data from hundred of concurrent customers and provide this occupancy data as a service to our vertical offerings in commercial, health, real estate and education. We will discuss the benefits of using DLT to handle this sort of incoming stream data, and illustrate the pain points we had and the resolutions we undertook in successfully receiving the stream of Cisco data. We will illustrate how our DLT pipeline was designed, and how it scaled to deal with huge quantities of real time streaming data.",
         "Title: Using Cisco Spaces Firehose API as a Stream of Data for Real-Time Occupancy Modelling\n                Abstract:  Honeywell manages the control of equipment for hundreds of thousands of buildings worldwide. Many of our outcomes relating to energy and comfort rely on knowing where people are in the building at any one time. This is so we can target health and comfort conditions more suitably to areas where are more densely populated. Many of these buildings have Cisco IT infrastructure in them. Using their WIFI points and the RSSI signal strength from people’s laptops and phones, Cisco can calculate the number of people in each area of the building. Cisco Spaces offer this data up as a real-time streaming source. Honeywell HBT has utilised this stream of data by writing delta live table pipelines to consume this data source.\n Honeywell buildings can now receive this firehose data from hundred of concurrent customers and provide this occupancy data as a service to our vertical offerings in commercial, health, real estate and education. We will discuss the benefits of using DLT to handle this sort of incoming stream data, and illustrate the pain points we had and the resolutions we undertook in successfully receiving the stream of Cisco data. We will illustrate how our DLT pipeline was designed, and how it scaled to deal with huge quantities of real time streaming data."
        ],
        [
         "Leveraging IoT Data at Scale to Mitigate Global Water Risks Using Apache Spark Streaming and Delta",
         "Every year, billions of dollars are lost due to water risks from storms, floods, and droughts. Water data scarcity and excess are issues that risk models cannot overcome, creating a world of uncertainty. Divirod is building a platform of water data by normalizing diverse data sources of varying velocity into one unified data asset. In addition to publicly available third-party datasets, we are rapidly deploying our own IoT sensors. \n \n These sensors ingest signals at a rate of about 100,000 messages per hour into preprocessing, signal-processing, analytics, and postprocessing workloads in one spark-streaming pipeline to enable critical real-time decision-making processes. By leveraging streaming architecture, we were able to reduce end-to-end latency from 10s of minutes to just a few seconds. \n \n We are leveraging Delta to provide a single query interface across multiple tables of this continuously changing data. This enables data science and analytics workloads to always use the most current and comprehensive information available. In addition to the obvious schema transformations, we implement data-quality metrics and datum conversions to provide a trustworthy unified dataset.",
         "Title: Leveraging IoT Data at Scale to Mitigate Global Water Risks Using Apache Spark Streaming and Delta\n                Abstract:  Every year, billions of dollars are lost due to water risks from storms, floods, and droughts. Water data scarcity and excess are issues that risk models cannot overcome, creating a world of uncertainty. Divirod is building a platform of water data by normalizing diverse data sources of varying velocity into one unified data asset. In addition to publicly available third-party datasets, we are rapidly deploying our own IoT sensors. \n \n These sensors ingest signals at a rate of about 100,000 messages per hour into preprocessing, signal-processing, analytics, and postprocessing workloads in one spark-streaming pipeline to enable critical real-time decision-making processes. By leveraging streaming architecture, we were able to reduce end-to-end latency from 10s of minutes to just a few seconds. \n \n We are leveraging Delta to provide a single query interface across multiple tables of this continuously changing data. This enables data science and analytics workloads to always use the most current and comprehensive information available. In addition to the obvious schema transformations, we implement data-quality metrics and datum conversions to provide a trustworthy unified dataset."
        ],
        [
         "Unlocking Near Real-Time Data Replication with CDC, Apache Spark Streaming, and Delta Lake",
         "Tune into DoorDash's journey to migrate from a flaky ETL system with 24-hour data delays, to standardizing a CDC streaming pattern across 150+ databases to produce near real-time data in a scalable, configurable, and reliable manner. During this journey, understand how we use Delta Lake to build a self-serve, read-optimized data lake with data latencies of 15 minutes, whilst reducing operational overhead. Furthermore, understand how certain tradeoffs like conceding to a non-real-time system allow for multiple optimizations but still permit for OLTP query use-cases, and the benefits it provides.",
         "Title: Unlocking Near Real-Time Data Replication with CDC, Apache Spark Streaming, and Delta Lake\n                Abstract:  Tune into DoorDash's journey to migrate from a flaky ETL system with 24-hour data delays, to standardizing a CDC streaming pattern across 150+ databases to produce near real-time data in a scalable, configurable, and reliable manner. During this journey, understand how we use Delta Lake to build a self-serve, read-optimized data lake with data latencies of 15 minutes, whilst reducing operational overhead. Furthermore, understand how certain tradeoffs like conceding to a non-real-time system allow for multiple optimizations but still permit for OLTP query use-cases, and the benefits it provides."
        ],
        [
         "Streaming Schema Drift Discovery and Controlled Mitigation",
         "\"When creating streaming work loads with Databricks, it can sometimes be difficult to capture and understand the current structure of your source data. For example, what happens if you are ingesting JSON events from a vendor, and the keys are very sparsely populated, or contain dynamic content? Ideally, data engineers want to \"\"lock in\"\" a target schema in order to minimize complexity and maximize performance for known access patterns. What do you do when your data sources just don't cooperate with that vision?",
         "Title: Streaming Schema Drift Discovery and Controlled Mitigation\n                Abstract:  \"When creating streaming work loads with Databricks, it can sometimes be difficult to capture and understand the current structure of your source data. For example, what happens if you are ingesting JSON events from a vendor, and the keys are very sparsely populated, or contain dynamic content? Ideally, data engineers want to \"\"lock in\"\" a target schema in order to minimize complexity and maximize performance for known access patterns. What do you do when your data sources just don't cooperate with that vision?"
        ],
        [
         " \n The first step is to quantify how far your current source data is drifting from your established Delta table. But how? In this talk I will demonstrate a way to capture and visual drift across all of your streaming tables. The next question is",
         " \"\"Now that I see all of the data I'm missing",
         "Title:  \n The first step is to quantify how far your current source data is drifting from your established Delta table. But how? In this talk I will demonstrate a way to capture and visual drift across all of your streaming tables. The next question is\n                Abstract:   \"\"Now that I see all of the data I'm missing"
        ],
        [
         "Real-Time Streaming Solution for a Call Center Analytics: Business Challenges and Technical Enablement",
         "A large international client with a business footprint in North America, Europe and Africa, reached out to us with an interest in having a real-time streaming solution designed and implemented for its call center handling incoming and outgoing client calls. The client had a previous bad experience with another vendor, who overpromised and underdelivered on the latency of the streaming solution. The previous vendor delivered an over-complex streaming data pipeline resulting in the data taking over 5 minutes to reach a visualization layer. The client felt that architecture was too complex and involved many services integrated together. Our immediate challenges involved gaining the client's trust and proving that our design and implementation quality would supercede a previous experience. To resolve an immediate challenge of the overly complicated pipeline design, we deployed a Databricks Lakehouse architecture with Azure Databricks at the center of the solution. Our reference archiitecture integrated Genesys Cloud -> App Services -> Event Hub -> Databricks <-> Data Lake -> Power BI. The streaming solution proved to be low latency (seconds) during the POV stage, which led to subsequent productionalization of the pipeline with deployment of jobs, DLTs pipeline, including multi-notebook workflow and business and performance metrics dashboarding relied on by the call center staff for a day-to-day performance monitoring and improvements.",
         "Title: Real-Time Streaming Solution for a Call Center Analytics: Business Challenges and Technical Enablement\n                Abstract:  A large international client with a business footprint in North America, Europe and Africa, reached out to us with an interest in having a real-time streaming solution designed and implemented for its call center handling incoming and outgoing client calls. The client had a previous bad experience with another vendor, who overpromised and underdelivered on the latency of the streaming solution. The previous vendor delivered an over-complex streaming data pipeline resulting in the data taking over 5 minutes to reach a visualization layer. The client felt that architecture was too complex and involved many services integrated together. Our immediate challenges involved gaining the client's trust and proving that our design and implementation quality would supercede a previous experience. To resolve an immediate challenge of the overly complicated pipeline design, we deployed a Databricks Lakehouse architecture with Azure Databricks at the center of the solution. Our reference archiitecture integrated Genesys Cloud -> App Services -> Event Hub -> Databricks <-> Data Lake -> Power BI. The streaming solution proved to be low latency (seconds) during the POV stage, which led to subsequent productionalization of the pipeline with deployment of jobs, DLTs pipeline, including multi-notebook workflow and business and performance metrics dashboarding relied on by the call center staff for a day-to-day performance monitoring and improvements."
        ],
        [
         "Top Mistakes to Avoid in Streaming Applications",
         "Nowadays, streaming use cases become very important in almost all companies. When you think of streaming, Apache Spark™ Structured Streaming is at the top of the list in the technology. As we are dealing with many customer streaming use cases, we learned many do’s and don’t of the streaming application. I will be discussing the top mistakes to avoid when working with streaming applications with regard to different sources and sinks like DLT, Kafka, Delta, and so on. If you are avoiding these mistakes while architecting/running/restarting the application, then you will avoid the costly mistakes at a later point in time.",
         "Title: Top Mistakes to Avoid in Streaming Applications\n                Abstract:  Nowadays, streaming use cases become very important in almost all companies. When you think of streaming, Apache Spark™ Structured Streaming is at the top of the list in the technology. As we are dealing with many customer streaming use cases, we learned many do’s and don’t of the streaming application. I will be discussing the top mistakes to avoid when working with streaming applications with regard to different sources and sinks like DLT, Kafka, Delta, and so on. If you are avoiding these mistakes while architecting/running/restarting the application, then you will avoid the costly mistakes at a later point in time."
        ],
        [
         "Embracing the Future of Data Engineering: The Serverless, Real-Time Lakehouse in Action",
         "As we venture into the future of data engineering, streaming and serverless technologies take center stage. In this fun, hands-on, in-depth and interactive session you can learn about the essence of future data engineering today. In this session we will tackle the challenge of processing streaming events created by hundreds of sensors in the conference room from a serverless web app (bring your phone and be a part of it!). The focus is on the system architecture and the involved products. Which Databricks product, capability and settings will be most useful for our scenario? What does streaming really mean and why does it make our life easier? What are the exact benefits of serverless and how much serverless is a particular solution?\nLeveraging the power of the Databricks Lakehouse Platform, I will demonstrate how to create a streaming data pipeline with delta live tables ingesting data from AWS Kinesis. Further, I’ll utilize advanced Databricks workflows triggers for efficient orchestration and real-time alerts feeding into a real-time dashboard. And since I don’t want you to leave with empty hands - I will use Delta Sharing to share the results of the demo we built with every participant in the room.\nJoin me in this hands-on exploration of cutting-edge data engineering techniques and witness the future in action!",
         "Title: Embracing the Future of Data Engineering: The Serverless, Real-Time Lakehouse in Action\n                Abstract:  As we venture into the future of data engineering, streaming and serverless technologies take center stage. In this fun, hands-on, in-depth and interactive session you can learn about the essence of future data engineering today. In this session we will tackle the challenge of processing streaming events created by hundreds of sensors in the conference room from a serverless web app (bring your phone and be a part of it!). The focus is on the system architecture and the involved products. Which Databricks product, capability and settings will be most useful for our scenario? What does streaming really mean and why does it make our life easier? What are the exact benefits of serverless and how much serverless is a particular solution?\nLeveraging the power of the Databricks Lakehouse Platform, I will demonstrate how to create a streaming data pipeline with delta live tables ingesting data from AWS Kinesis. Further, I’ll utilize advanced Databricks workflows triggers for efficient orchestration and real-time alerts feeding into a real-time dashboard. And since I don’t want you to leave with empty hands - I will use Delta Sharing to share the results of the demo we built with every participant in the room.\nJoin me in this hands-on exploration of cutting-edge data engineering techniques and witness the future in action!"
        ],
        [
         "Building and Managing Data Platform for 13+ PB Delta Lake and 1000’s of Users:  AT&T'S Story",
         "Data runs AT&T’s business, just like it runs most businesses these days. Data can lead to a greater understanding of a business and when translated correctly into information can provide human and business systems valuable insights to make better decisions. Unique to AT&T is the volume of data we support, how much of our work that is driven by AI and the scale at which data and AI drive value for our customers and stakeholders. Our Cloud migration journey includes making data and AI more accessible to employees throughout AT&T so they can use their deep business expertise to leverage data more easily and rapidly. We always had to balance this data democratization and desire for speed with keeping our data private and secure. We loved the open ecosystem model of Lakehouse that enables data , BI and ML tools to be seamlessly integrated on a single pane arena, it just simplifies the architecture and reduces dependencies between technologies in the cloud. Being clear in our architecture guidelines and patterns was very important to us for our success. We are seeing more interest from our business unit partners and continuing to build the AI as a service capabilities to support more citizen data scientists. To scale up our Lakehouse journey , we built a Databricks center of excellence (CoE) function in AT&T which today has ~1400+ active members , further concentrating existing expertise and resources in ML/AI discipline to collaborate on all things Databricks like technical support, trainings , FAQ’s and best practices to attain and sustain world-class performance and drive business value for AT&T. Join us to learn more about how we process and manage 10+ Petabytes of our network Lakehouse with Delta Lake and Databricks.",
         "Title: Building and Managing Data Platform for 13+ PB Delta Lake and 1000’s of Users:  AT&T'S Story\n                Abstract:  Data runs AT&T’s business, just like it runs most businesses these days. Data can lead to a greater understanding of a business and when translated correctly into information can provide human and business systems valuable insights to make better decisions. Unique to AT&T is the volume of data we support, how much of our work that is driven by AI and the scale at which data and AI drive value for our customers and stakeholders. Our Cloud migration journey includes making data and AI more accessible to employees throughout AT&T so they can use their deep business expertise to leverage data more easily and rapidly. We always had to balance this data democratization and desire for speed with keeping our data private and secure. We loved the open ecosystem model of Lakehouse that enables data , BI and ML tools to be seamlessly integrated on a single pane arena, it just simplifies the architecture and reduces dependencies between technologies in the cloud. Being clear in our architecture guidelines and patterns was very important to us for our success. We are seeing more interest from our business unit partners and continuing to build the AI as a service capabilities to support more citizen data scientists. To scale up our Lakehouse journey , we built a Databricks center of excellence (CoE) function in AT&T which today has ~1400+ active members , further concentrating existing expertise and resources in ML/AI discipline to collaborate on all things Databricks like technical support, trainings , FAQ’s and best practices to attain and sustain world-class performance and drive business value for AT&T. Join us to learn more about how we process and manage 10+ Petabytes of our network Lakehouse with Delta Lake and Databricks."
        ],
        [
         "Data Democratization with Lakehouse: An Open Banking Application Case",
         "\"Banco Bradesco represents one of the largest companies in the financial sector in Latin America. They have more than 99 million customers, 79 years of history, and a legacy of data distributed in hundreds of on-premises systems. With the spread of data-driven approaches and the growth of cloud computing adoption, we needed to innovate and adapt to new trends and enable an analytical environment with democratized data.  We will show how more than 8 business departments have already engaged in using the Lakehouse exploratory environment, with more than 190 use cases mapped and a multi-bank financial manager. Unlike with on-premises, the cost of each process can be isolated and managed in near real-time, allowing quick responses to cost and budget deviations, while increasing the deployment speed of new features 36 times compared to on-premises. The data is now used and shared safely and easily between different areas and companies of the group. Also, the view of dashboards within Databricks allows panels to be efficiently \"\"prototyped\"\" with real data",
         "Title: Data Democratization with Lakehouse: An Open Banking Application Case\n                Abstract:  \"Banco Bradesco represents one of the largest companies in the financial sector in Latin America. They have more than 99 million customers, 79 years of history, and a legacy of data distributed in hundreds of on-premises systems. With the spread of data-driven approaches and the growth of cloud computing adoption, we needed to innovate and adapt to new trends and enable an analytical environment with democratized data.  We will show how more than 8 business departments have already engaged in using the Lakehouse exploratory environment, with more than 190 use cases mapped and a multi-bank financial manager. Unlike with on-premises, the cost of each process can be isolated and managed in near real-time, allowing quick responses to cost and budget deviations, while increasing the deployment speed of new features 36 times compared to on-premises. The data is now used and shared safely and easily between different areas and companies of the group. Also, the view of dashboards within Databricks allows panels to be efficiently \"\"prototyped\"\" with real data"
        ],
        [
         "Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying",
         "Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks’ dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia’s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies,  including success stories that resulted in significant cost savings and efficiency gain.",
         "Title: Jet Streaming Data and Predictive Analytics: How the Lakehouse and Apache Spark Enable Collins Aerospace to Keep Aircraft Flying\n                Abstract:  Most have experienced the frustration and disappointment of a flight delay or cancelation due to aircraft issues. The Collins Aerospace business unit at Raytheon Technologies is committed to redefining aerospace by using data to deliver a more reliable, sustainable, efficient, and enjoyable aviation industry. Ascentia is a product example of this with focus on helping airlines make smarter and more sustainable decisions by anticipating aircraft maintenance issues in advance, leading to more reliable flight schedules and fewer delays. Over the past five years a variety of products from the Databricks technology suite were employed to achieve this. Leveraging cloud infrastructure and harnessing the Databricks Lakehouse, SPARK development, and Databricks’ dynamic platform, Collins has been able to accelerate development and deployment of predictive health monitoring (PHM) analytics to generate Ascentia’s aircraft maintenance recommendations. This presentation provides a walkthrough of these technologies,  including success stories that resulted in significant cost savings and efficiency gain."
        ],
        [
         "How RecRoom Processes Billions of Events Per Day With Databricks and RudderStack",
         "Learn how Rec Room, a fast-growing augmented and virtual reality software startup, is saving 50% of their engineering team's time by using Databricks and RudderStack to power real-time analytics and insights for their 85 million gaming customers. In this session, you will walk through a step-by-step explanation of how Rec Room set up efficient processes for ingestion into their data lakehouse, transformation, reverse-ETL and product analytics. You will also see how Rec Room is using incremental materialization of tables to save costs and establish an uptime of close to 100%.",
         "Title: How RecRoom Processes Billions of Events Per Day With Databricks and RudderStack\n                Abstract:  Learn how Rec Room, a fast-growing augmented and virtual reality software startup, is saving 50% of their engineering team's time by using Databricks and RudderStack to power real-time analytics and insights for their 85 million gaming customers. In this session, you will walk through a step-by-step explanation of how Rec Room set up efficient processes for ingestion into their data lakehouse, transformation, reverse-ETL and product analytics. You will also see how Rec Room is using incremental materialization of tables to save costs and establish an uptime of close to 100%."
        ],
        [
         "Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?",
         "Rust is a unique language whose traits make it very appealing for data engineering. In this session, we'll walk through the different aspects of the language that make it such a good fit for big data processing including: how it improves performance and how it provides greater safety guarantees and compatibility with a wide range of existing tools that make it well positioned to become a major building block for the future of analytics. \n \n We will also take a hands-on look through real code examples at a few emerging technologies built on top of Rust that utilize these capabilities, and learn how to apply them to our modern lakehouse architecture.",
         "Title: Delta-rs, Apache Arrow, Polars, WASM: Is Rust the Future of Analytics?\n                Abstract:  Rust is a unique language whose traits make it very appealing for data engineering. In this session, we'll walk through the different aspects of the language that make it such a good fit for big data processing including: how it improves performance and how it provides greater safety guarantees and compatibility with a wide range of existing tools that make it well positioned to become a major building block for the future of analytics. \n \n We will also take a hands-on look through real code examples at a few emerging technologies built on top of Rust that utilize these capabilities, and learn how to apply them to our modern lakehouse architecture."
        ],
        [
         "Making the Shift to Application-Driven Intelligence",
         "In the digital economy, application-driven intelligence delivered against live, real-time data will become a core capability of successful enterprises. It has the potential to improve the experience that you provide to your customers and deepen their engagement. But to make application-driven intelligence a reality, you can no longer rely only on copying live application data out of operational systems into analytics stores.\n Rather, it takes the unique real-time application-serving layer of a MongoDB database combined with the scale and real-time capabilities of a Databricks Lakehouse to automate and operationalize complex and AI-enhanced applications at scale. In this session we will show how it can be seamless for developers and data scientists to automate decisioning and actions on fresh application data and we'll deliver a practical demonstration on how operational data can be integrated in real time to run complex Machine Learning pipelines.",
         "Title: Making the Shift to Application-Driven Intelligence\n                Abstract:  In the digital economy, application-driven intelligence delivered against live, real-time data will become a core capability of successful enterprises. It has the potential to improve the experience that you provide to your customers and deepen their engagement. But to make application-driven intelligence a reality, you can no longer rely only on copying live application data out of operational systems into analytics stores.\n Rather, it takes the unique real-time application-serving layer of a MongoDB database combined with the scale and real-time capabilities of a Databricks Lakehouse to automate and operationalize complex and AI-enhanced applications at scale. In this session we will show how it can be seamless for developers and data scientists to automate decisioning and actions on fresh application data and we'll deliver a practical demonstration on how operational data can be integrated in real time to run complex Machine Learning pipelines."
        ],
        [
         "Making Travel More Accessible for Customers Bringing Mobility Devices",
         "American Airlines takes great pride in caring for customers travel, and recognize the importance of supporting the dignity and independence of everyone who travels with us. As we work to improve the customer experience, we're committed to making our airline more accessible to everyone. Our work to ensure that travel that is accessible to all is well underway. We have been particularly focused on making the journey smoother for customers who rely on wheelchairs or other mobility devices. We have implemented the use of a bag tag specifically for wheelchairs and scooters that gives team members more information, like the mobility device’s weight and battery type, or whether it needs to be returned to a customer before a connecting flight.\n \n As a data engineering and analytics team, we at American Airlines are building a passenger service request data product that will provide timely insights on expected mobility device traffic at each airport so that the front-line team members can provide seamless travel experience to the passengers.",
         "Title: Making Travel More Accessible for Customers Bringing Mobility Devices\n                Abstract:  American Airlines takes great pride in caring for customers travel, and recognize the importance of supporting the dignity and independence of everyone who travels with us. As we work to improve the customer experience, we're committed to making our airline more accessible to everyone. Our work to ensure that travel that is accessible to all is well underway. We have been particularly focused on making the journey smoother for customers who rely on wheelchairs or other mobility devices. We have implemented the use of a bag tag specifically for wheelchairs and scooters that gives team members more information, like the mobility device’s weight and battery type, or whether it needs to be returned to a customer before a connecting flight.\n \n As a data engineering and analytics team, we at American Airlines are building a passenger service request data product that will provide timely insights on expected mobility device traffic at each airport so that the front-line team members can provide seamless travel experience to the passengers."
        ],
        [
         "Improve Apache Spark DS V2 Query Planning Using Column Stats",
         "When doing the TPC-DS benchmark using external V2 data source, we have observed that for several of the queries, DS V1 has better join plans than Spark. The main reason is that DS V1 uses column stats, especially number of distinct values (NDV) for query optimization. Currently, Spark DS V2 only has interfaces for data sources to report table statistics such as size in bytes and number of rows. In order to use column stats in DS V2, we have added new interfaces to allow external data sources to report column stats to Spark. For a data source with huge data, it’s always challenging to get the column stats, especially the NDV. We plan to calculate NDV using Apache DataSketches Theta sketch and save the serialized compact sketch in the statistics file. The NDV and other column stats will be reported to Spark for query plan optimization.",
         "Title: Improve Apache Spark DS V2 Query Planning Using Column Stats\n                Abstract:  When doing the TPC-DS benchmark using external V2 data source, we have observed that for several of the queries, DS V1 has better join plans than Spark. The main reason is that DS V1 uses column stats, especially number of distinct values (NDV) for query optimization. Currently, Spark DS V2 only has interfaces for data sources to report table statistics such as size in bytes and number of rows. In order to use column stats in DS V2, we have added new interfaces to allow external data sources to report column stats to Spark. For a data source with huge data, it’s always challenging to get the column stats, especially the NDV. We plan to calculate NDV using Apache DataSketches Theta sketch and save the serialized compact sketch in the statistics file. The NDV and other column stats will be reported to Spark for query plan optimization."
        ],
        [
         "Databricks Powered Enterprise Data Platform for Faster Insights with Optimal Cost",
         "A leading Australian Bank engaged Deloitte to review architecture of current enterprise data platform enabled using Databricks. Bank’s vision is to limit proliferation of diverse tools/technology while enabling “best of breed” capabilities in the bank’s next-gen data platform. To realize this vision, the enterprise data platform will use various Databricks capabilities: DLT (Delta Live tables), Unity Catalog, Delta Sharing, Databricks Dashboards and Databricks Alerts. Using of these Databricks high performance and scalable capabilities allows the bank to avoid proliferation of disparate tools and reduce a need for custom development or integration. \n \n During this session, Deloitte, Databricks and Australian Bank will present how Databricks can be leveraged to develop a robust, resilient and scalable data platform which allows an organization to achieve scaled data sharing, reducing lead time to generate insights via reports, dashboards, AI/ML and Data Sharing.",
         "Title: Databricks Powered Enterprise Data Platform for Faster Insights with Optimal Cost\n                Abstract:  A leading Australian Bank engaged Deloitte to review architecture of current enterprise data platform enabled using Databricks. Bank’s vision is to limit proliferation of diverse tools/technology while enabling “best of breed” capabilities in the bank’s next-gen data platform. To realize this vision, the enterprise data platform will use various Databricks capabilities: DLT (Delta Live tables), Unity Catalog, Delta Sharing, Databricks Dashboards and Databricks Alerts. Using of these Databricks high performance and scalable capabilities allows the bank to avoid proliferation of disparate tools and reduce a need for custom development or integration. \n \n During this session, Deloitte, Databricks and Australian Bank will present how Databricks can be leveraged to develop a robust, resilient and scalable data platform which allows an organization to achieve scaled data sharing, reducing lead time to generate insights via reports, dashboards, AI/ML and Data Sharing."
        ],
        [
         "Using Databricks to Power Insights and Visualizations on the S&P Global Marketplace",
         "In this session, we will explain the visualizations that serve to shorten the time to insight for our prospects and encourage potential buyers to take the next step and request more information from our commercial team. The S&P Global Marketplace is a discovery and exploration platform that enables prospective buyers and clients to easily search fundamental and alternative datasets from across S&P Global and curated third-party providers. It serves as a digital storefront that provides transparency into data coverage and use cases, reducing the time and effort for clients to find data for their needs. A key feature of Marketplace is our interactive data visualizations that provide insight into the coverage of a dataset and demonstrate how the dataset can be used to make more informed decisions.   \n \n  \n \n The S&P Global Marketplace’s interactive visualizations are displayed in Tableau and are powered by Databricks. The Databricks platform allows for easy integration of S&P Global data and provides a collaborative environment where our team of product managers and data engineers can develop the code to generate each visualization. The team utilizes the web interface to develop the queries that perform the heavy lifting of data transformation instead of performing these tasks in Tableau. The final notebook output is saved into a custom datamart (“golden table”) which is the source for Tableau. We also developed an automated process that refreshes the whole process to ensure Marketplace has up to date visualizations.",
         "Title: Using Databricks to Power Insights and Visualizations on the S&P Global Marketplace\n                Abstract:  In this session, we will explain the visualizations that serve to shorten the time to insight for our prospects and encourage potential buyers to take the next step and request more information from our commercial team. The S&P Global Marketplace is a discovery and exploration platform that enables prospective buyers and clients to easily search fundamental and alternative datasets from across S&P Global and curated third-party providers. It serves as a digital storefront that provides transparency into data coverage and use cases, reducing the time and effort for clients to find data for their needs. A key feature of Marketplace is our interactive data visualizations that provide insight into the coverage of a dataset and demonstrate how the dataset can be used to make more informed decisions.   \n \n  \n \n The S&P Global Marketplace’s interactive visualizations are displayed in Tableau and are powered by Databricks. The Databricks platform allows for easy integration of S&P Global data and provides a collaborative environment where our team of product managers and data engineers can develop the code to generate each visualization. The team utilizes the web interface to develop the queries that perform the heavy lifting of data transformation instead of performing these tasks in Tableau. The final notebook output is saved into a custom datamart (“golden table”) which is the source for Tableau. We also developed an automated process that refreshes the whole process to ensure Marketplace has up to date visualizations."
        ],
        [
         "Self-Service Geospatial Analysis Leveraging Databricks, Apache Sedona, and R",
         "Geospatial data analysis is critical to understanding the impact of agricultural operations on environmental sustainability with respect to water quality, soil health, greenhouse gasses, and more. Outside of a few specialized software products, however, support for spatial data types is often limited or missing from analytics and visualization platforms. In this session, we show how Truterra is using Databricks, Apache Sedona, and R to analyze spatial data at scale. Additionally, learn how Truterra uses spatial insights to educate and promote practices that optimize profitability, sustainability, and stewardship outcomes at the farm. In this session, you will see how Databricks and Apache Sedona are used to process large spatial datasets including field, watershed, and hydrologic boundaries. You will see dynamic widgets, SQL and R used in tandem to generate map visuals, display them, and enable download all from a Databricks notebook.",
         "Title: Self-Service Geospatial Analysis Leveraging Databricks, Apache Sedona, and R\n                Abstract:  Geospatial data analysis is critical to understanding the impact of agricultural operations on environmental sustainability with respect to water quality, soil health, greenhouse gasses, and more. Outside of a few specialized software products, however, support for spatial data types is often limited or missing from analytics and visualization platforms. In this session, we show how Truterra is using Databricks, Apache Sedona, and R to analyze spatial data at scale. Additionally, learn how Truterra uses spatial insights to educate and promote practices that optimize profitability, sustainability, and stewardship outcomes at the farm. In this session, you will see how Databricks and Apache Sedona are used to process large spatial datasets including field, watershed, and hydrologic boundaries. You will see dynamic widgets, SQL and R used in tandem to generate map visuals, display them, and enable download all from a Databricks notebook."
        ],
        [
         "Processing Delta Lake Tables on AWS Using AWS Glue, Amazon Athena, and Amazon Redshift",
         "Delta Lake is an open-source project that helps implement modern data lake architectures commonly built on cloud storages. With Delta Lake, you can achieve ACID transactions, time travel queries, CDC, and other common use cases on the cloud. There are a lot of use cases of Delta tables on AWS. AWS has invested a lot in this technology, and now Delta Lake is available with multiple AWS services, such as AWS Glue Spark jobs, Amazon EMR, Amazon Athena, and Amazon Redshift Spectrum. AWS Glue is a serverless, scalable data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources. With AWS Glue, you can easily ingest data from multiple data sources such as on-prem databases, Amazon RDS, DynamoDB, MongoDB into Delta Lake on Amazon S3 even without expertise in coding.\n \n This session will demonstrate how to get started with processing Delta Lake tables on Amazon S3 using AWS Glue, and querying from Amazon Athena, and Amazon Redshift. The session also covers recent AWS service updates related to Delta Lake.",
         "Title: Processing Delta Lake Tables on AWS Using AWS Glue, Amazon Athena, and Amazon Redshift\n                Abstract:  Delta Lake is an open-source project that helps implement modern data lake architectures commonly built on cloud storages. With Delta Lake, you can achieve ACID transactions, time travel queries, CDC, and other common use cases on the cloud. There are a lot of use cases of Delta tables on AWS. AWS has invested a lot in this technology, and now Delta Lake is available with multiple AWS services, such as AWS Glue Spark jobs, Amazon EMR, Amazon Athena, and Amazon Redshift Spectrum. AWS Glue is a serverless, scalable data integration service that makes it easier to discover, prepare, move, and integrate data from multiple sources. With AWS Glue, you can easily ingest data from multiple data sources such as on-prem databases, Amazon RDS, DynamoDB, MongoDB into Delta Lake on Amazon S3 even without expertise in coding.\n \n This session will demonstrate how to get started with processing Delta Lake tables on Amazon S3 using AWS Glue, and querying from Amazon Athena, and Amazon Redshift. The session also covers recent AWS service updates related to Delta Lake."
        ],
        [
         "A Simple SQL Execution API for the Databricks Lakehouse Architecture",
         "Given the widespread support of REST architectures, data managed by the Databricks Lakehouse Platform can now be accessed from anywhere, making it easy to build data applications tailored to your needs. You can use the SQL Execution API to build web services-based applications, integrate with various applications and computing devices, create generic integration layers for enterprise services, or create custom client libraries for your programming language of choice.\n \n In this talk we are going to deep dive into the design and implementation of the SQL Execution API with a focus on:\n 1. The key API features which can be used to develop data applications.\n 2. The underlying Databricks architecture such as Cloud Fetch that we developed to enable this API.",
         "Title: A Simple SQL Execution API for the Databricks Lakehouse Architecture\n                Abstract:  Given the widespread support of REST architectures, data managed by the Databricks Lakehouse Platform can now be accessed from anywhere, making it easy to build data applications tailored to your needs. You can use the SQL Execution API to build web services-based applications, integrate with various applications and computing devices, create generic integration layers for enterprise services, or create custom client libraries for your programming language of choice.\n \n In this talk we are going to deep dive into the design and implementation of the SQL Execution API with a focus on:\n 1. The key API features which can be used to develop data applications.\n 2. The underlying Databricks architecture such as Cloud Fetch that we developed to enable this API."
        ],
        [
         "Best Exploration of Columnar Shuffle Design",
         "To significantly improve the performance of Spark SQL, there is a trend to offload Spark SQL execution to highly optimized native libraries or accelerators in past several years, like Photon from Databricks, Nvidia's Rapids plug-in, and Intel & Kyligence's initiated open source Gluten project. By the multi-fold performance improvement from these solutions, more and more Spark users have started to adopt the new technology. One characteristics of native libraries is that they all use columnar data format as the basic data format. It's because the columnar data format has the intrinsic affinity to vectorized data processing using SIMD instructions. While vanilla Spark's shuffle is based on spark's internal row data format. The high overhead of the columnar to row and row to columnar conversion during the shuffle makes reusing current shuffle not possible. Due to the importance of shuffle service in Spark, we have to implement an efficient columnar shuffle, which brings couple of new challenges, like the split of columnar data, or the dictionary support during shuffle.\n \n In this session, we will share 1) the exploration process of the columnar shuffle design during our Gazelle and Gluten development, and best practices for implementing the columnar shuffle service. We will also share how we learned from the development of vanilla Spark's shuffle, for example, how to address the small files issue then we will propose the new shuffle solution. We will show the performance comparison between Columnar shuffle and vanilla Spark's row-based shuffle. Finally, we will share how the new built-in accelerators like QAT and IAA in the latest Intel processor are used in our columnar shuffle service and boost the performance.",
         "Title: Best Exploration of Columnar Shuffle Design\n                Abstract:  To significantly improve the performance of Spark SQL, there is a trend to offload Spark SQL execution to highly optimized native libraries or accelerators in past several years, like Photon from Databricks, Nvidia's Rapids plug-in, and Intel & Kyligence's initiated open source Gluten project. By the multi-fold performance improvement from these solutions, more and more Spark users have started to adopt the new technology. One characteristics of native libraries is that they all use columnar data format as the basic data format. It's because the columnar data format has the intrinsic affinity to vectorized data processing using SIMD instructions. While vanilla Spark's shuffle is based on spark's internal row data format. The high overhead of the columnar to row and row to columnar conversion during the shuffle makes reusing current shuffle not possible. Due to the importance of shuffle service in Spark, we have to implement an efficient columnar shuffle, which brings couple of new challenges, like the split of columnar data, or the dictionary support during shuffle.\n \n In this session, we will share 1) the exploration process of the columnar shuffle design during our Gazelle and Gluten development, and best practices for implementing the columnar shuffle service. We will also share how we learned from the development of vanilla Spark's shuffle, for example, how to address the small files issue then we will propose the new shuffle solution. We will show the performance comparison between Columnar shuffle and vanilla Spark's row-based shuffle. Finally, we will share how the new built-in accelerators like QAT and IAA in the latest Intel processor are used in our columnar shuffle service and boost the performance."
        ],
        [
         "What Happens When Curious and Knowledgeable Business People are Provided Access to the Right Data and Equipped With Simple Tools and Supported by Guidance From In-House Technical Experts",
         "Too often business decisions in large organizations are based on time consuming and labor-intensive data extracts, fragile excel or access sheets that require significant manual intervention. The teams that prepare these manual reports have invaluable heuristic knowledge that, when combined with meaningful data and tools, can make smart business decisions. \n \n \n Imagine a world where these business teams are empowered with tools that help them build meaningful reports despite their limited technical expertise.\n \n \n In this talk we will discuss : \n \n The value derived from investing in developing citizen data personas within a business organization\n \n How we successfully built a citizen data analytics culture within Michelin \n \n Real examples of the impact of this initiative on - the business and on the people themselves.\n \n The audience will walk away with some convincing arguments for building a citizen data culture in their organization and a how-to cookbook that they can use to cultivate citizen data personas.\n \n Finally, they will also have the opportunity to interactively uncover key success factors in the case of Michelin that can help drive a similar initiative in their respective companies.",
         "Title: What Happens When Curious and Knowledgeable Business People are Provided Access to the Right Data and Equipped With Simple Tools and Supported by Guidance From In-House Technical Experts\n                Abstract:  Too often business decisions in large organizations are based on time consuming and labor-intensive data extracts, fragile excel or access sheets that require significant manual intervention. The teams that prepare these manual reports have invaluable heuristic knowledge that, when combined with meaningful data and tools, can make smart business decisions. \n \n \n Imagine a world where these business teams are empowered with tools that help them build meaningful reports despite their limited technical expertise.\n \n \n In this talk we will discuss : \n \n The value derived from investing in developing citizen data personas within a business organization\n \n How we successfully built a citizen data analytics culture within Michelin \n \n Real examples of the impact of this initiative on - the business and on the people themselves.\n \n The audience will walk away with some convincing arguments for building a citizen data culture in their organization and a how-to cookbook that they can use to cultivate citizen data personas.\n \n Finally, they will also have the opportunity to interactively uncover key success factors in the case of Michelin that can help drive a similar initiative in their respective companies."
        ],
        [
         "Modularized Approach to Scale Suite of Advanced Omnichannel Analytics Products",
         "GSK started the journey of Omnichannel vision a few years ago using static business rules and customer targeting. We kicked off 2022 with conceptualization of a suite of monitoring, measurement and planning products that would help GSK establish a comprehensive omnichannel measurement strategy and plan across its global markets. One of the key guiding principles during conceptualization was deeply and seamlessly embedding these products within GSK’s data transformation and storage architecture built on Azure Databricks. We built the platform powered extensively by Databricks, GitHub and open-source technologies like Python. In this session, we will cover the following aspects of our journey along with some best practices that we have learned over time.",
         "Title: Modularized Approach to Scale Suite of Advanced Omnichannel Analytics Products\n                Abstract:  GSK started the journey of Omnichannel vision a few years ago using static business rules and customer targeting. We kicked off 2022 with conceptualization of a suite of monitoring, measurement and planning products that would help GSK establish a comprehensive omnichannel measurement strategy and plan across its global markets. One of the key guiding principles during conceptualization was deeply and seamlessly embedding these products within GSK’s data transformation and storage architecture built on Azure Databricks. We built the platform powered extensively by Databricks, GitHub and open-source technologies like Python. In this session, we will cover the following aspects of our journey along with some best practices that we have learned over time."
        ],
        [
         "AI to FI with Databricks",
         "Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\n 1. Challenges of common pipeline design to support AI-FI \n 2. Challenges of leveraging data acquired over many acquisitions\n 3. How is the Lakehouse paradigm lending itself to solving these challenges?\n 4. Some cool results we can show the world.\n \n To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \n \n To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying.",
         "Title: AI to FI with Databricks\n                Abstract:  Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\n 1. Challenges of common pipeline design to support AI-FI \n 2. Challenges of leveraging data acquired over many acquisitions\n 3. How is the Lakehouse paradigm lending itself to solving these challenges?\n 4. Some cool results we can show the world.\n \n To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \n \n To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying."
        ],
        [
         "How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse",
         "Don't miss this session where we demonstrate how the Texas Rangers baseball team organized their predictive models by using MLFlow and the MLRegistry inside Databricks. They started using Databricks as a simple solution to centralizing our development on the cloud. This helped lessen the issue of siloed development in our team, and allowed us to leverage the benefits of distributed cloud computing. But we quickly found that Databricks was a perfect solution to another problem that we faced in our data engineering stack. Specifically, cost, complexity, and scalability issues hampered our data architecture development for years, and we decided we needed to modernize our stack by migrating to a lakehouse. With our lakehouse, ad-hoc-analytics, ETL operations, and MLOps all living within Databricks, development at scale has never been easier for our team. Going forward, we hope to fully eliminate the silos of development, and remove the disconnect between our analytics and data engineering teams. From computer vision, pose analytics, and player tracking, to pitch design, base stealing likelihood, and more, come see how the Texas Rangers are using innovative cloud technologies to create action-driven reports from the current sea of Big Data. ",
         "Title: How the Texas Rangers Revolutionized Baseball Analytics with a Modern Data Lakehouse\n                Abstract:  Don't miss this session where we demonstrate how the Texas Rangers baseball team organized their predictive models by using MLFlow and the MLRegistry inside Databricks. They started using Databricks as a simple solution to centralizing our development on the cloud. This helped lessen the issue of siloed development in our team, and allowed us to leverage the benefits of distributed cloud computing. But we quickly found that Databricks was a perfect solution to another problem that we faced in our data engineering stack. Specifically, cost, complexity, and scalability issues hampered our data architecture development for years, and we decided we needed to modernize our stack by migrating to a lakehouse. With our lakehouse, ad-hoc-analytics, ETL operations, and MLOps all living within Databricks, development at scale has never been easier for our team. Going forward, we hope to fully eliminate the silos of development, and remove the disconnect between our analytics and data engineering teams. From computer vision, pose analytics, and player tracking, to pitch design, base stealing likelihood, and more, come see how the Texas Rangers are using innovative cloud technologies to create action-driven reports from the current sea of Big Data."
        ],
        [
         "Improving Hospital Operations With Streaming Data and Real Time AI/ML",
         "Over the past two years, Providence has developed a robust streaming data platform (SDP) leveraging Databricks in Azure. The SDP enables us to ingest and process real-time data reflecting clinical operations across our 52 hospitals and roughly 1000 ambulatory clinics. HL7 messages generated by Epic are parsed using Databricks in our secure cloud environment and used to generate an up-to-the minute picture of exactly what is happening at the point of care. We are already leveraging this information to minimize hospital overcrowding and have been actively integrating AI/ML to accurately forecast future conditions (e.g., arrivals, length of stay, acuity, and discharge requirements). This allows us to both improve resource utilization (e.g., nurse staffing levels) and to optimize patient throughput. The result is both improved patient care and operational efficiency. In this session we will share how these outcomes are only possible with the power and elegance afforded by our investments in Azure, Databricks, and increasingly Lakehouse. We will demonstrate Providence's blueprint for enabling real-time analytics which can be generalized to other healthcare providers.",
         "Title: Improving Hospital Operations With Streaming Data and Real Time AI/ML\n                Abstract:  Over the past two years, Providence has developed a robust streaming data platform (SDP) leveraging Databricks in Azure. The SDP enables us to ingest and process real-time data reflecting clinical operations across our 52 hospitals and roughly 1000 ambulatory clinics. HL7 messages generated by Epic are parsed using Databricks in our secure cloud environment and used to generate an up-to-the minute picture of exactly what is happening at the point of care. We are already leveraging this information to minimize hospital overcrowding and have been actively integrating AI/ML to accurately forecast future conditions (e.g., arrivals, length of stay, acuity, and discharge requirements). This allows us to both improve resource utilization (e.g., nurse staffing levels) and to optimize patient throughput. The result is both improved patient care and operational efficiency. In this session we will share how these outcomes are only possible with the power and elegance afforded by our investments in Azure, Databricks, and increasingly Lakehouse. We will demonstrate Providence's blueprint for enabling real-time analytics which can be generalized to other healthcare providers."
        ],
        [
         "AI Regulation is Coming: The EU AI Act and How Databricks Can Help with Compliance",
         "With the heightened attention on LLMs and what they can do, and the widening impact of AI on day-to-day life, the push by regulators across the globe to regulate AI is intensifying. As with GDPR in the privacy realm, the EU is leading the way with the EU Artificial Intelligence Act (AIA). Regulators everywhere will be looking to the AIA as precedent, and understanding the requirements imposed by the AIA is important for all players in the AI channel. Although not finalized, the basic framework regarding how the AIA will work is becoming clearer. The impact on developers and deployers of AI (‘providers’ and ‘users’ under the AIA) will be substantial. Although the AIA will probably not go into effect until early 2025, AI applications developed today will likely be affected, and design and development decisions made now should take the future regulations into account.\n\nIn this session, Matteo Quattrocchi, Brussels-based Director, Policy – EMEA, for BSA (the Software Alliance – the leading advocacy organization representing the enterprise software sector), will present an overview of the current proposed requirements under the AIA and give an update on the ongoing deliberations and likely timing for enactment. We will also highlight some of the ways the Lakehouse platform, including Managed MLflow, can help providers and users of ML-based applications meet the requirements of the AIA and other upcoming AI regulations.\n",
         "Title: AI Regulation is Coming: The EU AI Act and How Databricks Can Help with Compliance\n                Abstract:  With the heightened attention on LLMs and what they can do, and the widening impact of AI on day-to-day life, the push by regulators across the globe to regulate AI is intensifying. As with GDPR in the privacy realm, the EU is leading the way with the EU Artificial Intelligence Act (AIA). Regulators everywhere will be looking to the AIA as precedent, and understanding the requirements imposed by the AIA is important for all players in the AI channel. Although not finalized, the basic framework regarding how the AIA will work is becoming clearer. The impact on developers and deployers of AI (‘providers’ and ‘users’ under the AIA) will be substantial. Although the AIA will probably not go into effect until early 2025, AI applications developed today will likely be affected, and design and development decisions made now should take the future regulations into account.\n\nIn this session, Matteo Quattrocchi, Brussels-based Director, Policy – EMEA, for BSA (the Software Alliance – the leading advocacy organization representing the enterprise software sector), will present an overview of the current proposed requirements under the AIA and give an update on the ongoing deliberations and likely timing for enactment. We will also highlight some of the ways the Lakehouse platform, including Managed MLflow, can help providers and users of ML-based applications meet the requirements of the AIA and other upcoming AI regulations."
        ],
        [
         "Using Databricks to Develop Statistical and Mathematical Models to Forecast the Monkeypox Outbreak in Washington State",
         "In the spring and summer of 2022, monkeypox was detected in the United States and quickly spread throughout the country. To contain and mitigate the spread of the disease in Washington State, the Washington Department of Health data science team used the Databricks platform to develop a modeling pipeline that employed statistical and mathematical techniques to forecast the course of the monkeypox outbreak throughout the state. These models provided actionable information that helped inform decision making and guide the public health response to the outbreak. \n \n We used contact-tracing data, standard line-lists, and published parameters to train a variety of time-series forecasting models, including an ARIMA model, a Poisson regression, and an SEIR compartmental model. We also calculated the daily R-effective rate as an additional output. The compartmental model best fit the reported cases when tested out of sample, but the statistical models were quicker and easier to deploy and helped inform initial decision-making. The R-effective rate was particularly useful throughout the effort. \n \n Overall, these efforts highlighted the importance of rapidly deployable and scalable infectious disease modeling pipelines. Public health data science is still a nascent field, however, so common best practices in other industries are often-times novel approaches in public health. The need for stable, generalizable pipelines is crucial. Using the Databricks platform has allowed us to more quickly scale and iteratively improve our modeling pipelines to include other infectious diseases, such as influenza and RSV. Further development of scalable and standardized approaches to disease forecasting at the state and local level is vital to better informing future public health response efforts.",
         "Title: Using Databricks to Develop Statistical and Mathematical Models to Forecast the Monkeypox Outbreak in Washington State\n                Abstract:  In the spring and summer of 2022, monkeypox was detected in the United States and quickly spread throughout the country. To contain and mitigate the spread of the disease in Washington State, the Washington Department of Health data science team used the Databricks platform to develop a modeling pipeline that employed statistical and mathematical techniques to forecast the course of the monkeypox outbreak throughout the state. These models provided actionable information that helped inform decision making and guide the public health response to the outbreak. \n \n We used contact-tracing data, standard line-lists, and published parameters to train a variety of time-series forecasting models, including an ARIMA model, a Poisson regression, and an SEIR compartmental model. We also calculated the daily R-effective rate as an additional output. The compartmental model best fit the reported cases when tested out of sample, but the statistical models were quicker and easier to deploy and helped inform initial decision-making. The R-effective rate was particularly useful throughout the effort. \n \n Overall, these efforts highlighted the importance of rapidly deployable and scalable infectious disease modeling pipelines. Public health data science is still a nascent field, however, so common best practices in other industries are often-times novel approaches in public health. The need for stable, generalizable pipelines is crucial. Using the Databricks platform has allowed us to more quickly scale and iteratively improve our modeling pipelines to include other infectious diseases, such as influenza and RSV. Further development of scalable and standardized approaches to disease forecasting at the state and local level is vital to better informing future public health response efforts."
        ],
        [
         "Accelerating the Development of Viewership Personas With a Unified Feature Store",
         "With the proliferation of video content and flourishing consumer demand, there is an enormous opportunity for customer-centric video entertainment companies to use data & analytics to understand what their viewers want and deliver more of the content that that meets their needs.\n \n At DIRECTV, our Data Science Center of Excellence is constantly looking to push the boundary of innovation in how we can better and more quickly understand the needs of our customers and leverage those actionable insights to deliver business impact. One way in which we do so is through the development of Viewership Personas with cluster analysis at scale to group our customers by the types of content they enjoy watching. This process is significantly accelerated by a unified feature store which contain a wide array of features that captures key information on viewing preferences.\n \n This presentation will focus on how the DIRECTV Data Science team utilizes Databricks to help: (1) develop a unified feature store and (2) how we leverage the feature store to accelerate the process of running machine learning algorithms to find meaningful viewership clusters.",
         "Title: Accelerating the Development of Viewership Personas With a Unified Feature Store\n                Abstract:  With the proliferation of video content and flourishing consumer demand, there is an enormous opportunity for customer-centric video entertainment companies to use data & analytics to understand what their viewers want and deliver more of the content that that meets their needs.\n \n At DIRECTV, our Data Science Center of Excellence is constantly looking to push the boundary of innovation in how we can better and more quickly understand the needs of our customers and leverage those actionable insights to deliver business impact. One way in which we do so is through the development of Viewership Personas with cluster analysis at scale to group our customers by the types of content they enjoy watching. This process is significantly accelerated by a unified feature store which contain a wide array of features that captures key information on viewing preferences.\n \n This presentation will focus on how the DIRECTV Data Science team utilizes Databricks to help: (1) develop a unified feature store and (2) how we leverage the feature store to accelerate the process of running machine learning algorithms to find meaningful viewership clusters."
        ],
        [
         "How You Can Audit A Language Model",
         "Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\n \n Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\n \n * Studying past incidents using the AI Incident Database and using this information to guide debugging.\n \n * Finding & fixing common data quality issues.\n \n * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\n \n * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\n \n * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\n \n * Testing performance, sentiment & toxicity across different & less common languages.\n \n * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\n \n * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\n \n * Engaging stakeholders to help find problems system designers & developers cannot see.\n \n It's now time to figure out how to live with AI, and that means audits, risk management & regulation.",
         "Title: How You Can Audit A Language Model\n                Abstract:  Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\n \n Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\n \n * Studying past incidents using the AI Incident Database and using this information to guide debugging.\n \n * Finding & fixing common data quality issues.\n \n * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\n \n * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\n \n * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\n \n * Testing performance, sentiment & toxicity across different & less common languages.\n \n * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\n \n * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\n \n * Engaging stakeholders to help find problems system designers & developers cannot see.\n \n It's now time to figure out how to live with AI, and that means audits, risk management & regulation."
        ],
        [
         "JetBlue’s real-time AI & ML digital twin journey using Databricks",
         "JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \n \n The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code.",
         "Title: JetBlue’s real-time AI & ML digital twin journey using Databricks\n                Abstract:  JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \n \n The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code."
        ],
        [
         "Intermittent Service Part Demand Forecasting at John Deere",
         "John Deere customers demand reliability and uptime for their equipment and John Deere dealers must fulfill that need by stocking the right mix of service parts in the right quantities to keep them up and running. John Deere dealers operate on six continents with geographically dispersed customers, products, and equipment. This presents a significant operations and logistics challenge to maintain best-in-industry service levels. Improved demand forecasting enables better retail stocking decisions and inventory planning, however, the variety of equipment serviced, diversity of customer segments, and total number of SKUs at dealer retail locations presents a complex demand forecasting challenge.\n \n The John Deere Aftermarket Analytics team took on this opportunity and developed forecasts and stocking recommendations for difficult low volume and intermittent demand service parts that comprise a large percentage of dealers' part sales. Building and iterating the solution required development of data pipelines, exploratory analysis and investigation, model training with multiple R and Python packages, experiment tracking in MLFlow, and model deployment. All of this was enabled with John Deere's Enterprise Data Lake paired with Databricks.\n \n In this session, learn about:\n \n • Practical approaches for intermittent demand forecasting \n • Leveraging Workspaces and Repos for efficient team collaboration\n • Developing scalable data pipelines and features for ML models with Delta Lake\n • Rapid model experimentation with AutoML and MLFlow\n  • Job orchestration using Workflows",
         "Title: Intermittent Service Part Demand Forecasting at John Deere\n                Abstract:  John Deere customers demand reliability and uptime for their equipment and John Deere dealers must fulfill that need by stocking the right mix of service parts in the right quantities to keep them up and running. John Deere dealers operate on six continents with geographically dispersed customers, products, and equipment. This presents a significant operations and logistics challenge to maintain best-in-industry service levels. Improved demand forecasting enables better retail stocking decisions and inventory planning, however, the variety of equipment serviced, diversity of customer segments, and total number of SKUs at dealer retail locations presents a complex demand forecasting challenge.\n \n The John Deere Aftermarket Analytics team took on this opportunity and developed forecasts and stocking recommendations for difficult low volume and intermittent demand service parts that comprise a large percentage of dealers' part sales. Building and iterating the solution required development of data pipelines, exploratory analysis and investigation, model training with multiple R and Python packages, experiment tracking in MLFlow, and model deployment. All of this was enabled with John Deere's Enterprise Data Lake paired with Databricks.\n \n In this session, learn about:\n \n • Practical approaches for intermittent demand forecasting \n • Leveraging Workspaces and Repos for efficient team collaboration\n • Developing scalable data pipelines and features for ML models with Delta Lake\n • Rapid model experimentation with AutoML and MLFlow\n  • Job orchestration using Workflows"
        ],
        [
         "Scaling AI Applications With Databricks, HuggingFace and Pinecone",
         "The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\n In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution. ",
         "Title: Scaling AI Applications With Databricks, HuggingFace and Pinecone\n                Abstract:  The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\n In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution."
        ],
        [
         "Hyperparameter Tuning Via Apache Spark and Ray",
         "Model selection through hyperparameter tuning (HPT) is a computationally intensive task. Development of specialized hardware, distributed computation, and modern model selection algorithms has allowed a wide range of potential solutions to be evaluated by the data science community. In this session, you will learn more about how Marks and Spencer tackled this task. In examining the tools, you see that Spark UDF and Ray have received considerable attention from the data science community. Spark UDF allows parallelized execution of custom workloads on distributed datasets in a fault tolerant and scalable manner. Ray is an open-source framework for distributed model selection. It features a well-designed, narrow waist API that seamlessly integrates a wide range of hyperparameter search algorithms. Ray creates trials and controls their execution with the help of a scheduler equipped with well-known HPT algorithms allowing Bayesian optimization and Population-based training. ",
         "Title: Hyperparameter Tuning Via Apache Spark and Ray\n                Abstract:  Model selection through hyperparameter tuning (HPT) is a computationally intensive task. Development of specialized hardware, distributed computation, and modern model selection algorithms has allowed a wide range of potential solutions to be evaluated by the data science community. In this session, you will learn more about how Marks and Spencer tackled this task. In examining the tools, you see that Spark UDF and Ray have received considerable attention from the data science community. Spark UDF allows parallelized execution of custom workloads on distributed datasets in a fault tolerant and scalable manner. Ray is an open-source framework for distributed model selection. It features a well-designed, narrow waist API that seamlessly integrates a wide range of hyperparameter search algorithms. Ray creates trials and controls their execution with the help of a scheduler equipped with well-known HPT algorithms allowing Bayesian optimization and Population-based training."
        ],
        [
         "Large Scale Multi-Task Learning Recommender Service at Verizon",
         "Millions of customers visit the Verizon company website and stores monthly to shop, from plans to products and accessories. Numerous task-specific recommender models have been built at Verizon to enhance customer experience. Motivated by the latest development in deep learning, multi-task learning architectures can better understand customer behavior therefore greatly improve model performance, and also save time to learn new tasks. However, it’s challenging to build such a model that digests heterogeneous data from purchases, online clicks, store visits to customer services and many more, while also  accomplishing various tasks where some of them might contradict with each other. Motivated by the Pathways vision, we built a novel multi-task learning recommender service called Pathways Recommender Service (PaRS), which directly handles multiple abstract forms of data and is able to generalize across multiple recommendation tasks at Verizon. You will see the that we explored built-in bias mitigation in PaRS to promote diverse, relevant and fair recommendations.",
         "Title: Large Scale Multi-Task Learning Recommender Service at Verizon\n                Abstract:  Millions of customers visit the Verizon company website and stores monthly to shop, from plans to products and accessories. Numerous task-specific recommender models have been built at Verizon to enhance customer experience. Motivated by the latest development in deep learning, multi-task learning architectures can better understand customer behavior therefore greatly improve model performance, and also save time to learn new tasks. However, it’s challenging to build such a model that digests heterogeneous data from purchases, online clicks, store visits to customer services and many more, while also  accomplishing various tasks where some of them might contradict with each other. Motivated by the Pathways vision, we built a novel multi-task learning recommender service called Pathways Recommender Service (PaRS), which directly handles multiple abstract forms of data and is able to generalize across multiple recommendation tasks at Verizon. You will see the that we explored built-in bias mitigation in PaRS to promote diverse, relevant and fair recommendations."
        ],
        [
         "Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases",
         "Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases.",
         "Title: Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases\n                Abstract:  Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases."
        ],
        [
         "International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale",
         "International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \nIn this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.",
         "Title: International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale\n                Abstract:  International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \nIn this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps."
        ],
        [
         "How MLOps on Databricks helped adidas to gain speed in productionising ML projects",
         "MLOps enables Data science teams to bring ML models into production environments in an efficient manner while making sure that the model pipeline is maintained and model is monitored continuously. When drift is detected, models are retrained promptly and teams are notified with the availability of new version of model. Our team at adidas has created a ML template that uses Databricks, Jenkins and MLflow, which facilitates in the faster deployment process for our ML models in production. We employ a hybrid deployment strategy to minimize compute costs for large models. This presentation will demonstrate how the template was developed, how our teams are utilizing it for model deployment and how it helped us to gain speed to bring or use cases live.",
         "Title: How MLOps on Databricks helped adidas to gain speed in productionising ML projects\n                Abstract:  MLOps enables Data science teams to bring ML models into production environments in an efficient manner while making sure that the model pipeline is maintained and model is monitored continuously. When drift is detected, models are retrained promptly and teams are notified with the availability of new version of model. Our team at adidas has created a ML template that uses Databricks, Jenkins and MLflow, which facilitates in the faster deployment process for our ML models in production. We employ a hybrid deployment strategy to minimize compute costs for large models. This presentation will demonstrate how the template was developed, how our teams are utilizing it for model deployment and how it helped us to gain speed to bring or use cases live."
        ],
        [
         "How We Built a Unified Talent Solution Using Databricks Machine Learning",
         "Using Databricks, we built a “Unified Talent Solution”, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta’s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data",
         "Title: How We Built a Unified Talent Solution Using Databricks Machine Learning\n                Abstract:  Using Databricks, we built a “Unified Talent Solution”, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta’s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data"
        ],
        [
         "Meet LOLA: The Innovation Engine Brewing Models at Scale for AB-InBev",
         "Today the world's largest brewer, AB-InBev, is disrupting the industry with BEES and LOLA. LOLA is our machine learning platform that enables us to brew models at scale. The foundation of LOLA rests on Databricks Lakehouse's platform and is now being extended globally with Unity Catalog. However, three years ago, things were different. Back then, LOLA was just a simple recommender with less than 60% account coverage in the US. Now it is evolving into an ML platform that supports more models, like our new recommendation engine DEAR, with 95%+ account coverage and integration with BEES in the US.\nAs LOLA evolves, the foundation laid by Lakehouse paved the way for the cornerstone of our platform, our platinum data layer, the Feature Store. The Feature Store is the hub for all feature engineering in LOLA, using Databricks Workflows to support over 250+ production features. But, more importantly, it has become the abstraction layer that helps build out our growing number of ML pipelines.\n  \n ",
         "Title: Meet LOLA: The Innovation Engine Brewing Models at Scale for AB-InBev\n                Abstract:  Today the world's largest brewer, AB-InBev, is disrupting the industry with BEES and LOLA. LOLA is our machine learning platform that enables us to brew models at scale. The foundation of LOLA rests on Databricks Lakehouse's platform and is now being extended globally with Unity Catalog. However, three years ago, things were different. Back then, LOLA was just a simple recommender with less than 60% account coverage in the US. Now it is evolving into an ML platform that supports more models, like our new recommendation engine DEAR, with 95%+ account coverage and integration with BEES in the US.\nAs LOLA evolves, the foundation laid by Lakehouse paved the way for the cornerstone of our platform, our platinum data layer, the Feature Store. The Feature Store is the hub for all feature engineering in LOLA, using Databricks Workflows to support over 250+ production features. But, more importantly, it has become the abstraction layer that helps build out our growing number of ML pipelines."
        ],
        [
         "Scaling MLOps for a demand forecasting across multiple markets for a large CPG",
         "In this session, we look at how one of the world’s largest CPG company setup a scalable MLOps pipeline for a Demand Forecasting use case that predicted demand at 100k+ DFUs (demand forecasting units) on a weekly basis across 20+ markets. This implementation resulted in significant cost savings in terms of improved productivity, reduced cloud usage and faster time to value amongst other benefits.\n The attendees of this session will leave this session with a clearer picture on the following:\n 1. Best practices in scaling mlops with Databricks and Azure for a demand forecasting use case with a multi-market and multi-region roll-out.\n 2. Best practices related to model re-factoring and setting up standard CI-CD pipelines for MLOps\n 3. What are some of the pitfalls to avoid in such scenarios?",
         "Title: Scaling MLOps for a demand forecasting across multiple markets for a large CPG\n                Abstract:  In this session, we look at how one of the world’s largest CPG company setup a scalable MLOps pipeline for a Demand Forecasting use case that predicted demand at 100k+ DFUs (demand forecasting units) on a weekly basis across 20+ markets. This implementation resulted in significant cost savings in terms of improved productivity, reduced cloud usage and faster time to value amongst other benefits.\n The attendees of this session will leave this session with a clearer picture on the following:\n 1. Best practices in scaling mlops with Databricks and Azure for a demand forecasting use case with a multi-market and multi-region roll-out.\n 2. Best practices related to model re-factoring and setting up standard CI-CD pipelines for MLOps\n 3. What are some of the pitfalls to avoid in such scenarios?"
        ],
        [
         "MLOps at Gucci: From Zero to Hero",
         "In recent years, similar principles to those of DevOps in software development have been applied to Machine Learning projects with the goal of productionizing automated solutions. However, Machine Learning Operations (MLOps) practices have proven to be of difficult implementation, as they often require putting together several different tools in a complex architecture. In this session, we introduce MLOps concepts and describe how we implement MLOps principles in our projects at Gucci by leveraging Databricks functionalities. A use case of deploying a data science tool for supporting media budget allocation decisions is presented. After the development of a POC to experiment and effectively address the business problem, the code was versioned and refactored. Code reviews were executed to ensure quality and readability. Within Databricks, we rapidly moved the project to the production stage by achieving an automated solution including unit tests, environment configuration, registration and versioning of models, monitoring of model performances as well as model serving and a dashboard to visualize results. This template is being successfully replicated for other projects and is allowing our new Data Science team to quickly bring value to different business areas of the company. These best practices and insights can be used as an example of how this can be beneficial to you or other practitioners.",
         "Title: MLOps at Gucci: From Zero to Hero\n                Abstract:  In recent years, similar principles to those of DevOps in software development have been applied to Machine Learning projects with the goal of productionizing automated solutions. However, Machine Learning Operations (MLOps) practices have proven to be of difficult implementation, as they often require putting together several different tools in a complex architecture. In this session, we introduce MLOps concepts and describe how we implement MLOps principles in our projects at Gucci by leveraging Databricks functionalities. A use case of deploying a data science tool for supporting media budget allocation decisions is presented. After the development of a POC to experiment and effectively address the business problem, the code was versioned and refactored. Code reviews were executed to ensure quality and readability. Within Databricks, we rapidly moved the project to the production stage by achieving an automated solution including unit tests, environment configuration, registration and versioning of models, monitoring of model performances as well as model serving and a dashboard to visualize results. This template is being successfully replicated for other projects and is allowing our new Data Science team to quickly bring value to different business areas of the company. These best practices and insights can be used as an example of how this can be beneficial to you or other practitioners."
        ],
        [
         "Streamlining API Deployment for ML Models Across Multiple Brands: Ahold Delhaize's Experience on Serverless",
         "At Ahold Delhaize, we have 19 local brands. Most of our brands have common goals, such as providing personalized offers to their customers, a better search engine on e-commerce websites, and forecasting models to reduce food waste and ensure availability. As a central team, our goal is to standardize the way of working across all of these brands, including the deployment of machine learning models. To this end, we have adopted Databricks as our standard platform for our batch inference models. \n\nHowever, API deployment for real time inference models remained challenging due to the varying capabilities of our brands. Our attempts to standardize API deployments with different tools failed due to complexity of our organization. Fortunately, Databricks has recently introduced a new feature: serverless API deployment. Since all our brands already use Databricks, this feature was easy to adopt. It allows us to easily reuse API deployment across all of our brands, significantly reducing time to market (from 6-12 months to 1 month), increasing efficiency, and reducing the costs. In this session, you will see the solution architecture, sample use case specifically used to cross-sell model deployed to 4 different brands, and API deployment using Databricks Serverless API with custom model.",
         "Title: Streamlining API Deployment for ML Models Across Multiple Brands: Ahold Delhaize's Experience on Serverless\n                Abstract:  At Ahold Delhaize, we have 19 local brands. Most of our brands have common goals, such as providing personalized offers to their customers, a better search engine on e-commerce websites, and forecasting models to reduce food waste and ensure availability. As a central team, our goal is to standardize the way of working across all of these brands, including the deployment of machine learning models. To this end, we have adopted Databricks as our standard platform for our batch inference models. \n\nHowever, API deployment for real time inference models remained challenging due to the varying capabilities of our brands. Our attempts to standardize API deployments with different tools failed due to complexity of our organization. Fortunately, Databricks has recently introduced a new feature: serverless API deployment. Since all our brands already use Databricks, this feature was easy to adopt. It allows us to easily reuse API deployment across all of our brands, significantly reducing time to market (from 6-12 months to 1 month), increasing efficiency, and reducing the costs. In this session, you will see the solution architecture, sample use case specifically used to cross-sell model deployed to 4 different brands, and API deployment using Databricks Serverless API with custom model."
        ],
        [
         "Monetizing Data Assets: Sharing Data, Models and Features",
         "Exec Summary:\n \n - Data is an asset and selling/sharing data has (largely) been solved\n - Hosted models exist (example: ChatGPT) but moving sensitive data across the public internet or across clouds is problematic\n - Sharing features (the result of feature engineering) can be monetized for new potential revenue streams\n - Sharing models can also be monetized while avoiding the transfer of sensitive data\n - This presentation will walk through a few examples of how to share models and features to generate new revenue streams using Delta Sharing, MLflow, and Databricks",
         "Title: Monetizing Data Assets: Sharing Data, Models and Features\n                Abstract:  Exec Summary:\n \n - Data is an asset and selling/sharing data has (largely) been solved\n - Hosted models exist (example: ChatGPT) but moving sensitive data across the public internet or across clouds is problematic\n - Sharing features (the result of feature engineering) can be monetized for new potential revenue streams\n - Sharing models can also be monetized while avoiding the transfer of sensitive data\n - This presentation will walk through a few examples of how to share models and features to generate new revenue streams using Delta Sharing, MLflow, and Databricks"
        ],
        [
         "An API for DL Inferencing on Spark",
         "Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization.\n \n In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models.",
         "Title: An API for DL Inferencing on Spark\n                Abstract:  Apache Spark is a popular distributed framework for big data processing. It is commonly used for ETL (Extract, Transform and Load) across large datasets. Today, the Transform stage can often include the application of Deep Learning models on the data. For example, common models can be used for classification of images, sentiment analysis of text, language translation, anomaly detection, and many other use cases. Applying these models within Spark can be done today with the combination of PySpark, PandasUDF, and a lot of glue code. Often, that glue code can be difficult to get right, because it requires expertise across multiple domains - DL frameworks, PySpark APIs, PandasUDF internal behavior, and performance optimization.\n \n In this talk, we introduce a new, simplified API for DL inferencing on Spark, introduced in SPARK-40264 as a collaboration between NVIDIA and Databricks, which seeks to standardize and open-source this glue code to make DL inference integrations easier for everyone. We discuss its design and demonstrate its usage across multiple DL frameworks and models."
        ],
        [
         "DEIMOS - Optimizing Time-Series Modeling at Scale With Applications in CPG",
         "At the beginning of a forecasting project, data scientists must make several decisions, each of which impacts the success of a project. Should the model be univariate or multivariate? What features warrant the best model inputs? What model type should you choose? Are the best hyperparameters selected? All of these parts of the modeling process can be quite costly in terms of both time and computational resources. In this session, you will see solution we've created called DEIMOS (Deep Experimental Integrated Multivariate Optimized Series). DEIMOS is built to handle many of the critical problems in time-series modeling automatically and at scale. This package is designed to be simple and straightforward to use, with easily interpretable results to both technical and non-technical audiences. Moreover, we’ve designed it to be flexible so that it can be easily applied to many use cases.",
         "Title: DEIMOS - Optimizing Time-Series Modeling at Scale With Applications in CPG\n                Abstract:  At the beginning of a forecasting project, data scientists must make several decisions, each of which impacts the success of a project. Should the model be univariate or multivariate? What features warrant the best model inputs? What model type should you choose? Are the best hyperparameters selected? All of these parts of the modeling process can be quite costly in terms of both time and computational resources. In this session, you will see solution we've created called DEIMOS (Deep Experimental Integrated Multivariate Optimized Series). DEIMOS is built to handle many of the critical problems in time-series modeling automatically and at scale. This package is designed to be simple and straightforward to use, with easily interpretable results to both technical and non-technical audiences. Moreover, we’ve designed it to be flexible so that it can be easily applied to many use cases."
        ],
        [
         "Comparing Databricks and Snowflake for Machine Learning",
         "Snowflake and Databricks both aim to provide data science toolkits for machine learning workflows, albeit with different approaches and resources. While developing ML models is technically possible using either platform, the Hitachi Solutions Empower team tested which solution will be easier, faster, and cheaper to work with in terms of both user experience and business outcomes for our customers. To do this, we designed and conducted a series of experiments with use cases from the TPCx-AI benchmark standard. We developed both single-node and multi-node versions of these experiments, which sometimes required us to set up separate compute infrastructure outside of the platform, in the case of Snowflake. We also built datasets of various sizes (1GB, 10GB, and 100GB), to assess how each platform/node setup handles scale. Based on our findings. On the average, Databricks is faster, cheaper, and easier to use for developing machine learning models, and we use it exclusively for data science on the Empower platform. Snowflake’s reliance on third party resources for distributed training is a major drawback, and the need to use multiple compute environments to scale up training is complex and, in our view, an unnecessary complication to achieve best results.",
         "Title: Comparing Databricks and Snowflake for Machine Learning\n                Abstract:  Snowflake and Databricks both aim to provide data science toolkits for machine learning workflows, albeit with different approaches and resources. While developing ML models is technically possible using either platform, the Hitachi Solutions Empower team tested which solution will be easier, faster, and cheaper to work with in terms of both user experience and business outcomes for our customers. To do this, we designed and conducted a series of experiments with use cases from the TPCx-AI benchmark standard. We developed both single-node and multi-node versions of these experiments, which sometimes required us to set up separate compute infrastructure outside of the platform, in the case of Snowflake. We also built datasets of various sizes (1GB, 10GB, and 100GB), to assess how each platform/node setup handles scale. Based on our findings. On the average, Databricks is faster, cheaper, and easier to use for developing machine learning models, and we use it exclusively for data science on the Empower platform. Snowflake’s reliance on third party resources for distributed training is a major drawback, and the need to use multiple compute environments to scale up training is complex and, in our view, an unnecessary complication to achieve best results."
        ],
        [
         "Testing Generative AI Models - What You Need to Know",
         "Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\n \n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\n \n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.",
         "Title: Testing Generative AI Models - What You Need to Know\n                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\n \n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\n \n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk."
        ],
        [
         "Simplifying Real-Time Machine Learning: A Look at Feature Platforms and Modern Real-time ML Architectures Using MLflow & Tecton",
         "Are you struggling to keep up with the demands of real-time machine learning? Like most organizations building real-time ML, you’re probably looking for a better way to: Manage the lifecycle of ML models and features, Implement batch, streaming, and real-time data pipelines, Generate accurate training datasets and Serve models and data online with strict SLAs, supporting millisecond latencies and high query volumes. Look no further! In this session, we will unveil a modern technical architecture that simplifies the process of managing real-time ML models and features. Using MLFlow and Tecton, we’ll show you how to build a robust MLOps platform on Databricks that can easily handle the unique challenges of real-time data processing. Join us to discover how to streamline the lifecycle of ML models and features, implement data pipelines with ease, and generate accurate training datasets with minimal effort. See how to serve models and data online with mission-critical speed and reliability, supporting millisecond latencies and high query volumes. Take a firsthand look at how FanDuel uses this solution to power their real-time ML applications, from responsible gaming to content recommendations and marketing optimization. See for yourself how this system can be used to define features, train models, process streaming data, and serve both models and features online for real-time inference with a live demo. Join us to learn how to build a modern MLOps platform for your real-time ML use cases.",
         "Title: Simplifying Real-Time Machine Learning: A Look at Feature Platforms and Modern Real-time ML Architectures Using MLflow & Tecton\n                Abstract:  Are you struggling to keep up with the demands of real-time machine learning? Like most organizations building real-time ML, you’re probably looking for a better way to: Manage the lifecycle of ML models and features, Implement batch, streaming, and real-time data pipelines, Generate accurate training datasets and Serve models and data online with strict SLAs, supporting millisecond latencies and high query volumes. Look no further! In this session, we will unveil a modern technical architecture that simplifies the process of managing real-time ML models and features. Using MLFlow and Tecton, we’ll show you how to build a robust MLOps platform on Databricks that can easily handle the unique challenges of real-time data processing. Join us to discover how to streamline the lifecycle of ML models and features, implement data pipelines with ease, and generate accurate training datasets with minimal effort. See how to serve models and data online with mission-critical speed and reliability, supporting millisecond latencies and high query volumes. Take a firsthand look at how FanDuel uses this solution to power their real-time ML applications, from responsible gaming to content recommendations and marketing optimization. See for yourself how this system can be used to define features, train models, process streaming data, and serve both models and features online for real-time inference with a live demo. Join us to learn how to build a modern MLOps platform for your real-time ML use cases."
        ],
        [
         "Building a Real-Time Model Monitoring Pipeline on Databricks",
         "Model deployment is almost never the final step in any ML lifecycle. ML models can degrade over time due to a variety of influencing factors. In this technical deep dive, we will build a real-time ML model monitoring pipeline on Databricks. We need to own and monitor our models for drifts like feature drift, concept drift, distribution drift, and so on. We must constantly monitor the models and issue alerts or trigger retraining when necessary. With so many open source tools and frameworks available, it can be difficult to figure out how to make everything work. In this tutorial, we will create a high-quality Model Monitoring Pipeline. Everything will be built from the ground up using Spark on Databricks. In this session we will introduce a use case in which we set up a model serving pipeline and log the predictions to a stream in real time. We will then configure a model metric monitoring pipeline to consume from the stream and aggregate over specific time windows. Then, to see these metrics live on dashboards, we will integrate a model monitoring visualizing pipeline.",
         "Title: Building a Real-Time Model Monitoring Pipeline on Databricks\n                Abstract:  Model deployment is almost never the final step in any ML lifecycle. ML models can degrade over time due to a variety of influencing factors. In this technical deep dive, we will build a real-time ML model monitoring pipeline on Databricks. We need to own and monitor our models for drifts like feature drift, concept drift, distribution drift, and so on. We must constantly monitor the models and issue alerts or trigger retraining when necessary. With so many open source tools and frameworks available, it can be difficult to figure out how to make everything work. In this tutorial, we will create a high-quality Model Monitoring Pipeline. Everything will be built from the ground up using Spark on Databricks. In this session we will introduce a use case in which we set up a model serving pipeline and log the predictions to a stream in real time. We will then configure a model metric monitoring pipeline to consume from the stream and aggregate over specific time windows. Then, to see these metrics live on dashboards, we will integrate a model monitoring visualizing pipeline."
        ],
        [
         "Using NLP to evaluate 100 Million global webpages daily to contextually target consumers",
         "This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\n\nTTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\n\nThe Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\n",
         "Title: Using NLP to evaluate 100 Million global webpages daily to contextually target consumers\n                Abstract:  This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\n\nTTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\n\nThe Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers."
        ],
        [
         "How Office Leverages Deep Graph Learning to Improve Productivity Products",
         "We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale.",
         "Title: How Office Leverages Deep Graph Learning to Improve Productivity Products\n                Abstract:  We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale."
        ],
        [
         "Stable Diffusion: The Future of Generative AI",
         "Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering.",
         "Title: Stable Diffusion: The Future of Generative AI\n                Abstract:  Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering."
        ],
        [
         "Vector Data Lakes",
         "Vector databases such as ElasticSearch and Pinecone offer fast ingestion and querying on vector embeddings with ANNs. However, they typically do not decouple compute and storage, making them hard to integrate in production data stacks. Because data storage in these databases is expensive and not easily accessible, data teams typically maintain ETL pipelines to offload historical embedding data to blob stores. When that data needs to be queried, they get loaded back into the vector database in another ETL process. This is reminiscent of loading data from OLTP database to cloud storage, then loading said data into an OLAP warehouse for offline analytics. Recently, “lakehouse” offerings allow direct OLAP querying on cloud storage, removing the need for the second ETL step. The same could be done for embedding data. While embedding storage in blob stores cannot satisfy the high TPS requirements in online settings, we argue it’s sufficient for offline analytics use cases like slicing and dicing data based on embedding clusters. Instead of loading the embedding data back into the vector database for offline analytics, we propose direct processing on embeddings stored in Parquet files in Delta Lake. You will see that offline embedding workloads typically touch a large portion of the stored embeddings without the need for random access. As a result, the workload is entirely bound by network throughput instead of latency, making it quite suitable for blob storage backends. On a test 1 billion vector dataset, ETL into cloud storage takes around 1 hour on a dedicated GPU instance, while batched nearest neighbor search can be done in under one minute with four CPU instances. We believe future “lakehouses” will ship with native support for these embedding workloads.",
         "Title: Vector Data Lakes\n                Abstract:  Vector databases such as ElasticSearch and Pinecone offer fast ingestion and querying on vector embeddings with ANNs. However, they typically do not decouple compute and storage, making them hard to integrate in production data stacks. Because data storage in these databases is expensive and not easily accessible, data teams typically maintain ETL pipelines to offload historical embedding data to blob stores. When that data needs to be queried, they get loaded back into the vector database in another ETL process. This is reminiscent of loading data from OLTP database to cloud storage, then loading said data into an OLAP warehouse for offline analytics. Recently, “lakehouse” offerings allow direct OLAP querying on cloud storage, removing the need for the second ETL step. The same could be done for embedding data. While embedding storage in blob stores cannot satisfy the high TPS requirements in online settings, we argue it’s sufficient for offline analytics use cases like slicing and dicing data based on embedding clusters. Instead of loading the embedding data back into the vector database for offline analytics, we propose direct processing on embeddings stored in Parquet files in Delta Lake. You will see that offline embedding workloads typically touch a large portion of the stored embeddings without the need for random access. As a result, the workload is entirely bound by network throughput instead of latency, making it quite suitable for blob storage backends. On a test 1 billion vector dataset, ETL into cloud storage takes around 1 hour on a dedicated GPU instance, while batched nearest neighbor search can be done in under one minute with four CPU instances. We believe future “lakehouses” will ship with native support for these embedding workloads."
        ],
        [
         "JoinBoost: In-DB ML for Tree-Models",
         "Data and Machine Learning (ML) are crucial for enterprise operations. Enterprises store data in databases for management and use ML to gain business insights. However, there is a mismatch between the way ML expects data to be organized (a single table) and the way data is organized in databases (a join graph of multiple tables) and leads to inefficiencies when joining and materializing tables. In this session, you will see how we successfully address this issue. We introduce JoinBoost, a lightweight python library that trains tree models (such as random forests and gradient boosting) for join graphs in databases. JoinBoost acts as a query rewriting layer that is compatible with cloud databases, and eliminates the need for costly join materialization.",
         "Title: JoinBoost: In-DB ML for Tree-Models\n                Abstract:  Data and Machine Learning (ML) are crucial for enterprise operations. Enterprises store data in databases for management and use ML to gain business insights. However, there is a mismatch between the way ML expects data to be organized (a single table) and the way data is organized in databases (a join graph of multiple tables) and leads to inefficiencies when joining and materializing tables. In this session, you will see how we successfully address this issue. We introduce JoinBoost, a lightweight python library that trains tree models (such as random forests and gradient boosting) for join graphs in databases. JoinBoost acts as a query rewriting layer that is compatible with cloud databases, and eliminates the need for costly join materialization."
        ],
        [
         "Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP",
         "In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.",
         "Title: Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP\n                Abstract:  In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively."
        ],
        [
         "Data Caching Strategies for Data Analytics and AI",
         "The increasing popularity of data analytics and artificial intelligence (AI) has led to a dramatic increase in the volume of data being used in these fields, creating a growing need for an enhanced computational capability. Cache plays a crucial role as an accelerator for data and AI computations, but it is important to note that these domains have different data access patterns, requiring different cache strategies. In this session, you will see our observations on data access patterns in the analytical SQL and AI training domains based on practical experience with large-scale systems. We will discuss the evaluation results of various caching strategies for analytical SQL and AI and provide caching recommendations for different use cases. Over the years, we have learned some best practices from big internet companies about the following aspects of our journey: 1. Traffic pattern for analytical SQL and cache strategy recommendation, 2. Traffic pattern for AI training and how we can measure the cache efficiency for different AI training process, 3. Cache capacity planning based on real-time metrics of the working set, and 4. Adaptive caching admission and eviction for uncertain traffic patterns",
         "Title: Data Caching Strategies for Data Analytics and AI\n                Abstract:  The increasing popularity of data analytics and artificial intelligence (AI) has led to a dramatic increase in the volume of data being used in these fields, creating a growing need for an enhanced computational capability. Cache plays a crucial role as an accelerator for data and AI computations, but it is important to note that these domains have different data access patterns, requiring different cache strategies. In this session, you will see our observations on data access patterns in the analytical SQL and AI training domains based on practical experience with large-scale systems. We will discuss the evaluation results of various caching strategies for analytical SQL and AI and provide caching recommendations for different use cases. Over the years, we have learned some best practices from big internet companies about the following aspects of our journey: 1. Traffic pattern for analytical SQL and cache strategy recommendation, 2. Traffic pattern for AI training and how we can measure the cache efficiency for different AI training process, 3. Cache capacity planning based on real-time metrics of the working set, and 4. Adaptive caching admission and eviction for uncertain traffic patterns"
        ],
        [
         "If a Duck Quacks in the Forrest and Everyone Hears, Should you Care?",
         "\"YES! \"\"Duck posting\"\" has become an internet meme for praising DuckDB on twitter. Nearly every quack using DuckDB has done it once or twice. But",
         "Title: If a Duck Quacks in the Forrest and Everyone Hears, Should you Care?\n                Abstract:  \"YES! \"\"Duck posting\"\" has become an internet meme for praising DuckDB on twitter. Nearly every quack using DuckDB has done it once or twice. But"
        ],
        [
         "Python with Spark Connect",
         "PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python.\n \n With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today’s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example.\n \n In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications.\n \n In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what’s next beyond Apache Spark 3.4.",
         "Title: Python with Spark Connect\n                Abstract:  PySpark has accomplished many milestones such as Project Zen, and been increasingly growing. We introduced pandas API on Spark, and hugely improved usability such as error messages, type hints, etc., and PySpark has become almost the very standard of distributed computing in Python.\n \n With this trend, the kind of PySpark use cases became also very complicated especially for modern data applications such as notebooks, IDEs, even devices such as smart home devices leveraging the power of data, that virtually need a lightweight separate client. However, today’s PySpark client is considerably heavy, and does not allow the separation from its scheduler, optimizer and analyzer as an example.\n \n In Apache Spark 3.4, one of the key features we introduced in PySpark is the Python client for Spark Connect that decouples client-server architecture for Apache Spark that allows remote connectivity to Spark clusters using the DataFrame API and unresolved logical plans as the protocol. The separation between client and server allows Apache Spark and its open ecosystem to be leveraged from everywhere. It can be embedded in modern data applications.\n \n In this talk, we will introduce what Spark Connect is, the internals of Spark Connect with Python, how to use Spark Connect with Python in the end-user perspective, and what’s next beyond Apache Spark 3.4."
        ],
        [
         "Databricks Connect powered by Spark Connect: Develop and Debug Spark from any developer tool",
         "Spark developers want to develop and debug their code using their tools of choice and development best practices while ensuring high-production fidelity on the target remote cluster. However, Spark's driver architecture is monolithic, with no built-in capability to directly connect to a remote Spark cluster from languages other than SQL. This makes it hard to enable such interactive developer experiences from a user’s local IDE of choice. Spark Connect’s decoupled client-server architecture introduces remote connectivity to Spark clusters and with that, enables interactive development experience - Spark and its open ecosystem can be leveraged from everywhere! \n \n In this talk, we show how we leverage Spark Connect to build a completely redesigned version of Databricks Connect, a first-class IDE-based developer experience that offers interactive debugging from any IDE. We show how developers can easily ensure consistency between their local and remote environments. We walk the audience through real-live examples of how to locally debug code running on Databrick. We also show how Databricks Connect integrates into the Databricks Visual Studio Code extension for an even better developer experience.",
         "Title: Databricks Connect powered by Spark Connect: Develop and Debug Spark from any developer tool\n                Abstract:  Spark developers want to develop and debug their code using their tools of choice and development best practices while ensuring high-production fidelity on the target remote cluster. However, Spark's driver architecture is monolithic, with no built-in capability to directly connect to a remote Spark cluster from languages other than SQL. This makes it hard to enable such interactive developer experiences from a user’s local IDE of choice. Spark Connect’s decoupled client-server architecture introduces remote connectivity to Spark clusters and with that, enables interactive development experience - Spark and its open ecosystem can be leveraged from everywhere! \n \n In this talk, we show how we leverage Spark Connect to build a completely redesigned version of Databricks Connect, a first-class IDE-based developer experience that offers interactive debugging from any IDE. We show how developers can easily ensure consistency between their local and remote environments. We walk the audience through real-live examples of how to locally debug code running on Databrick. We also show how Databricks Connect integrates into the Databricks Visual Studio Code extension for an even better developer experience."
        ],
        [
         "Ray on Spark",
         "Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale.\n \n This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. \n \n In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. \n We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. \n During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change!\n We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases.",
         "Title: Ray on Spark\n                Abstract:  Ray and its associated native libraries make scaling ML projects effortless. With only minor modification to existing single-machine code, Ray enables converting ML workloads to a distributed computation environment simple, intuitive, and powerful. By leveraging the infrastructure of Spark and the massive scalability of Ray for ML workloads, running Ray on Spark allows you to scale your ML work with the libraries you want without having to switch to a different implementation paradigm or set of libraries to achieve extreme scale.\n \n This is a presentation of a collaboration between Databricks Engineering and AnyScale Engineering. It covers a joint effort in building an officially-supported integration. \n \n In this talk, we'll be presenting the new integration between Spark and Ray, showcasing how you can start a Ray cluster from within Spark and leverage it for many ML use cases. \n We'll dive into how to start the cluster from within a Databricks Notebook, as well as how to start the Ray dashboard and leverage it for inspecting the performance of submitted tasks. \n During this overview, we'll explain how cluster resources are allocated on Spark, as well as the general architecture of running Ray on Spark. We'll also showcase how to convert your existing Ray workloads to run on Spark with a single line of configuration change!\n We'll conclude with a brief overview of using RayTune to perform hyperparameter tuning of a model, showcasing how easy and powerful it is to use the APIs for ML use cases."
        ],
        [
         "Why Delta Lake is the best storage format for pandas analyses",
         "pandas analyses are often limited by file formats like CSV and Parquet. CSV doesn't allow for column pruning, which is an important performance optimization. Parquet doesn't allow for critical features like ACID transactions, time travel, and schema enforcement. This talk discusses why Delta Lake is the fastest file format for pandas users and how it provides users with great features.",
         "Title: Why Delta Lake is the best storage format for pandas analyses\n                Abstract:  pandas analyses are often limited by file formats like CSV and Parquet. CSV doesn't allow for column pruning, which is an important performance optimization. Parquet doesn't allow for critical features like ACID transactions, time travel, and schema enforcement. This talk discusses why Delta Lake is the fastest file format for pandas users and how it provides users with great features."
        ],
        [
         "Fine tuning & scaling Hugging Face with Ray AIR",
         "Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n \n The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\n \n In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models’ training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n \n Key Takeaways: \n * Python developers and machine learning engineers can use Transformers and scale their language models\n * Get exposed to Ray AIR’s Python APIs for end-to-end Hugging Face and ML workflow\n * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads",
         "Title: Fine tuning & scaling Hugging Face with Ray AIR\n                Abstract:  Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n \n The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\n \n In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models’ training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\n \n Key Takeaways: \n * Python developers and machine learning engineers can use Transformers and scale their language models\n * Get exposed to Ray AIR’s Python APIs for end-to-end Hugging Face and ML workflow\n * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads"
        ],
        [
         "Breaking Barriers with Databricks Lakehouse - How Blackberry is Revolutionizing Cybersecurity Services Worldwide. AI-Driven Cybersecurity that Works Smarter, Not Harder.\n",
         "Cybersecurity incidents are costly, and using an Endpoint Detection and Response (EDR) solution enables the detection of cybersecurity incidents as quickly as possible. To effectively detect cybersecurity incidences requires the collection of millions of data points, and the storing/querying of endpoints data presents considerable engineering challenges. This includes quickly moving local data from endpoints to a single table in the cloud and enabling performant querying against it. The need to avoid internal data siloing within BlackBerry was paramount as multiple teams required access to the data to deliver an effective EDR solution for the present and the future. Databricks tooling enabled us to break down our data silos and iteratively improve our EDR pipeline to ingest data faster and reduce querying latency by more than 20% while reducing costs by more than 30%.\nIn this session, BlackBerry Engineers Srinivasa Kanamatha and Justin Lai will share the journey, lessons learned, and the future for collecting, storing, governing, and sharing data from endpoints in Databricks. The result of building EDR using Databricks helped us accelerate the deployment of our data platform.\"\"",
         "Title: Breaking Barriers with Databricks Lakehouse - How Blackberry is Revolutionizing Cybersecurity Services Worldwide. AI-Driven Cybersecurity that Works Smarter, Not Harder.\n\n                Abstract:  Cybersecurity incidents are costly, and using an Endpoint Detection and Response (EDR) solution enables the detection of cybersecurity incidents as quickly as possible. To effectively detect cybersecurity incidences requires the collection of millions of data points, and the storing/querying of endpoints data presents considerable engineering challenges. This includes quickly moving local data from endpoints to a single table in the cloud and enabling performant querying against it. The need to avoid internal data siloing within BlackBerry was paramount as multiple teams required access to the data to deliver an effective EDR solution for the present and the future. Databricks tooling enabled us to break down our data silos and iteratively improve our EDR pipeline to ingest data faster and reduce querying latency by more than 20% while reducing costs by more than 30%.\nIn this session, BlackBerry Engineers Srinivasa Kanamatha and Justin Lai will share the journey, lessons learned, and the future for collecting, storing, governing, and sharing data from endpoints in Databricks. The result of building EDR using Databricks helped us accelerate the deployment of our data platform.\"\""
        ],
        [
         "Scaling Deep Learning using Delta Lake storage format on Databricks",
         "Delta Lake is an open-source storage format that can be ideally used for storing large-scale datasets, which can be used for single-node and distributed training of deep learning models. Delta Lake storage format gives deep learning practitioners unique data management capabilities for working with their datasets. \n The challenge is that, as of now, it’s not possible to use Delta Lake to train PyTorch models directly. PyTorch community has recently introduced a Torchdata library for efficient data loading. This library supports many formats out of the box, but not Delta Lake. This talk will demonstrate using the Delta Lake storage format for single-node and distributed PyTorch training using the torchdata framework and standalone delta-rs Delta Lake implementation.",
         "Title: Scaling Deep Learning using Delta Lake storage format on Databricks\n                Abstract:  Delta Lake is an open-source storage format that can be ideally used for storing large-scale datasets, which can be used for single-node and distributed training of deep learning models. Delta Lake storage format gives deep learning practitioners unique data management capabilities for working with their datasets. \n The challenge is that, as of now, it’s not possible to use Delta Lake to train PyTorch models directly. PyTorch community has recently introduced a Torchdata library for efficient data loading. This library supports many formats out of the box, but not Delta Lake. This talk will demonstrate using the Delta Lake storage format for single-node and distributed PyTorch training using the torchdata framework and standalone delta-rs Delta Lake implementation."
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Title",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Abstract",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "full_text",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dais_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2cd0e0-8fe7-41a4-b238-a403ea61d484",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 1\n",
    "Set up Chroma and create collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2124a6b0-7232-4510-9f14-684662fb4585",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: /dbfs/mnt/dbacademy-users/labuser5400519@vocareum.com/large-language-models/working/database.db\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "chroma_client = chromadb.Client(\n",
    "    Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=DA.paths.user_db,  # this is an optional argument. If you don't supply this, the data will be ephemeral\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9167095-4763-4cb1-8a83-c903ea929b0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "Assign the value of `my_talks` to the `collection_name` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a6f5873-a24f-4acb-a6b9-b72b49034a77",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No embedding_function provided, using default embedding function: SentenceTransformerEmbeddingFunction\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating collection: 'my_talks'\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d55703635ef4834b8747f16f9cbd217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd4817af8144749b467f3dee5db0544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e1acc3b70e4d2687fcab2be4028b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304190db112e40c68edbc98a5758f9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182e830a35694282ab10ec3a373f1d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1d877d4fb444569b05e823a76a4901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f38b5731dc4061b24baf7d1e2d25a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91e50ff3b17f441fa53e28f828834135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2373b5c9c5e242c9be32b2069e989aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3c3f9ed6dd44ed8b41b94c2ff5bbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f11800fb824464828f1084273de2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9044eac6e4034ea6a11afca67bd909c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d914c310bea943ccb2e40f3471e43d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e967bc6697214a9ba212d0c3aec7e172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collection_name = \"my_talks\"\n",
    "\n",
    "# If you have created the collection before, you need to delete the collection first\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "else:\n",
    "    print(f\"Creating collection: '{collection_name}'\")\n",
    "    talks_collection = chroma_client.create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263321a9-c725-4748-a883-0ef0f05bbf1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question1\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_1(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "421d2ff0-bcba-4849-af98-d21125c95f87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "[Add](https://docs.trychroma.com/reference/Collection#add) data to the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5644da-711b-4dd3-9f25-c69a14b0e841",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "talks_collection.add(\n",
    "    documents=texts,\n",
    "    ids=[f\"id{x}\" for x in range(len(texts))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a9ece4a-0273-4881-8f59-3ac7a29fb551",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question2\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_2(talks_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdda0271-8a50-47e0-a21d-5f69365e279a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "[Query](https://docs.trychroma.com/reference/Collection#query) for relevant documents. If you are looking for talks related to language models, your query texts could be `language models`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4550f19-14e1-4c61-8793-5046273dde12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n    \"ids\": [\n        [\n            \"id171\",\n            \"id171\",\n            \"id145\",\n            \"id145\",\n            \"id166\",\n            \"id166\",\n            \"id152\",\n            \"id152\",\n            \"id163\",\n            \"id163\"\n        ]\n    ],\n    \"embeddings\": null,\n    \"documents\": [\n        [\n            \"Title: Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP\\n                Abstract:  In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple \\u201cretrieve-then-read\\u201d pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate\\u2013Search\\u2013Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37\\u2013125%, 8\\u201340%, and 80\\u2013290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.\",\n            \"Title: Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP\\n                Abstract:  In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple \\u201cretrieve-then-read\\u201d pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate\\u2013Search\\u2013Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37\\u2013125%, 8\\u201340%, and 80\\u2013290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.\",\n            \"Title: How You Can Audit A Language Model\\n                Abstract:  Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\\n \\n Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\\n \\n * Studying past incidents using the AI Incident Database and using this information to guide debugging.\\n \\n * Finding & fixing common data quality issues.\\n \\n * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\\n \\n * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\\n \\n * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\\n \\n * Testing performance, sentiment & toxicity across different & less common languages.\\n \\n * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\\n \\n * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\\n \\n * Engaging stakeholders to help find problems system designers & developers cannot see.\\n \\n It's now time to figure out how to live with AI, and that means audits, risk management & regulation.\",\n            \"Title: How You Can Audit A Language Model\\n                Abstract:  Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\\n \\n Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\\n \\n * Studying past incidents using the AI Incident Database and using this information to guide debugging.\\n \\n * Finding & fixing common data quality issues.\\n \\n * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\\n \\n * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\\n \\n * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\\n \\n * Testing performance, sentiment & toxicity across different & less common languages.\\n \\n * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\\n \\n * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\\n \\n * Engaging stakeholders to help find problems system designers & developers cannot see.\\n \\n It's now time to figure out how to live with AI, and that means audits, risk management & regulation.\",\n            \"Title: Using NLP to evaluate 100 Million global webpages daily to contextually target consumers\\n                Abstract:  This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\\n\\nTTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\\n\\nThe Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\",\n            \"Title: Using NLP to evaluate 100 Million global webpages daily to contextually target consumers\\n                Abstract:  This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\\n\\nTTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\\n\\nThe Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\",\n            \"Title: International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale\\n                Abstract:  International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC\\u2019s experts and other investors working in emerging markets. These algorithms are available via IFC\\u2019s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \\nIn this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC\\u2019s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.\",\n            \"Title: International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale\\n                Abstract:  International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC\\u2019s experts and other investors working in emerging markets. These algorithms are available via IFC\\u2019s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \\nIn this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC\\u2019s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.\",\n            \"Title: Testing Generative AI Models - What You Need to Know\\n                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\\n \\n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\\n \\n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We\\u2019ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We\\u2019ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.\",\n            \"Title: Testing Generative AI Models - What You Need to Know\\n                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\\n \\n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\\n \\n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We\\u2019ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We\\u2019ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.\"\n        ]\n    ],\n    \"metadatas\": [\n        [\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null\n        ]\n    ],\n    \"distances\": [\n        [\n            1.124968409538269,\n            1.124968409538269,\n            1.1281087398529053,\n            1.1281087398529053,\n            1.2049212455749512,\n            1.2049212455749512,\n            1.3258779048919678,\n            1.3258779048919678,\n            1.3803809881210327,\n            1.3803809881210327\n        ]\n    ]\n}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "results = talks_collection.query(\n",
    "    query_texts=[\"language models\"],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3dd3eb5-ad06-4979-a543-81347d2f6c8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Title: Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP\\n                Abstract:  In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.',\n",
       " 'Title: Demonstrate-Search-Predict: Composing Retrieval and Language Models for Knowledge-Intensive NLP\\n                Abstract:  In this session, you will learn about how retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple “retrieve-then-read” pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate–Search–Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37–125%, 8–40%, and 80–290% relative gains against vanilla LMs, a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively.',\n",
       " \"Title: How You Can Audit A Language Model\\n                Abstract:  Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\\n \\n Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\\n \\n * Studying past incidents using the AI Incident Database and using this information to guide debugging.\\n \\n * Finding & fixing common data quality issues.\\n \\n * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\\n \\n * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\\n \\n * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\\n \\n * Testing performance, sentiment & toxicity across different & less common languages.\\n \\n * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\\n \\n * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\\n \\n * Engaging stakeholders to help find problems system designers & developers cannot see.\\n \\n It's now time to figure out how to live with AI, and that means audits, risk management & regulation.\",\n",
       " \"Title: How You Can Audit A Language Model\\n                Abstract:  Language models like ChatGPT are incredible research breakthroughs but require auditing & risk management before productization. These systems raise concerns related to toxicity, transparency & reproducibility, intellectual property licensing & ownership, dis- & misinformation, supply chains & significant carbon footprints. How can your org. leverage these new tools without taking on undue or unknown risks?\\n \\n Recent public reference work from In-Q-Tel Labs & BNH.AI details an audit of a named entity recognition (NER) application based on the pre-trained language model RoBERTa. If you have a language model use case in mind & want to understand your risks, this presentation will cover:\\n \\n * Studying past incidents using the AI Incident Database and using this information to guide debugging.\\n \\n * Finding & fixing common data quality issues.\\n \\n * Applying general public tools & benchmarks as appropriate (e.g., checklist, SuperGLUE, HELM).\\n \\n * Binarizing specific tasks & debugging them using traditional model assessment and bias testing.\\n \\n * Constructing adversarial attacks based on a model's most significant risks and analyzing the results in terms of performance, sentiment & toxicity.\\n \\n * Testing performance, sentiment & toxicity across different & less common languages.\\n \\n * Conducting random attacks: random sequences of attacks, prompts or other tests that may evoke unexpected responses.\\n \\n * Don't forget about security: auditing code for backdoors & training data for poisoning, ensuring endpoints are protected with authentication & throttling and analyzing third-party dependencies.\\n \\n * Engaging stakeholders to help find problems system designers & developers cannot see.\\n \\n It's now time to figure out how to live with AI, and that means audits, risk management & regulation.\",\n",
       " \"Title: Using NLP to evaluate 100 Million global webpages daily to contextually target consumers\\n                Abstract:  This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\\n\\nTTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\\n\\nThe Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\",\n",
       " \"Title: Using NLP to evaluate 100 Million global webpages daily to contextually target consumers\\n                Abstract:  This session will cover the challenges and the solution that The Trade Desk went through to scale their ML models for NLP for 100 million web pages per day.\\n\\nTTD's contextual targeting team needs to analyze 100 Million web pages per day. 50% of the webpages are non-English.  Half of the content was not being properly analyzed and targeted intelligently. TTD attempted to build a model using Spark NLP, however the package could not scale and was not cost-effective. GPU utilization was low and the solution was cost prohibitive (over $400K/year). TTD engaged with Databricks in early 2022 to build an NLP model on Databricks. Our teams partnered closely together.  We were able to build a solution using distributed inference (150-200 GPUs running at 80%+ utilization);  Databricks could translate the 50 Million web pages each day for 35+ languages for $6K per month - 200x faster and at a fraction of the cost. This solution enables TTD teams to standardize on English for contextual targeting ML models. TTD can now be a one-stop shop for their customers' global advertising needs.\\n\\nThe Trade Desk is headquartered in Ventura, California. It is the largest independent demand-side platform in the world, competing against Google, Facebook, and others. Unlike traditional marketing, programmatic marketing is operated by real-time, split-second decisions based on user identity, device information, and other data points. It enables highly personalized consumer experiences and improves return-on-investment for companies and advertisers.\",\n",
       " \"Title: International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale\\n                Abstract:  International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \\nIn this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.\",\n",
       " \"Title: International Finance Corporation's MALENA Platform Provides Investors Working in Emerging Markets the Analytical Capacity to Support the Review of ESG Issues at Scale\\n                Abstract:  International Finance Corporation (IFC) is using data and AI to build machine learning solutions that create analytical capacity to support the review of ESG issues at scale. This includes natural language processing and requires entity recognition and other applications to support the work of IFC’s experts and other investors working in emerging markets. These algorithms are available via IFC’s Machine Learning ESG Analyst (MALENA) platform to enable rapid analysis, increase productivity, and build investor confidence. In this manner, IFC, a development finance institution with the mandate to address poverty in emerging markets, is making use of its historical datasets and open-source AI solutions to build custom-AI applications that democratize access to ESG capacity to read and classify text. \\nIn this session, you will learn the unique flexibility of the Spark ecosystem from Databricks and how that has allowed IFC’s MALENA project to connect to scalable Datalake storage, use different natural language processing models and seamlessly adopt MLOps.\",\n",
       " 'Title: Testing Generative AI Models - What You Need to Know\\n                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\\n \\n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\\n \\n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.',\n",
       " 'Title: Testing Generative AI Models - What You Need to Know\\n                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\\n \\n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\\n \\n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6c7981f-9fa0-41b7-be6d-04b159a367f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n    \"ids\": [\n        [\n            \"id163\",\n            \"id168\",\n            \"id178\",\n            \"id148\",\n            \"id154\",\n            \"id151\",\n            \"id6\",\n            \"id139\",\n            \"id146\",\n            \"id167\"\n        ]\n    ],\n    \"embeddings\": null,\n    \"documents\": [\n        [\n            \"Title: Testing Generative AI Models - What You Need to Know\\n                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\\n \\n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\\n \\n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We\\u2019ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We\\u2019ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.\",\n            \"Title: Stable Diffusion: The Future of Generative AI\\n                Abstract:  Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering.\",\n            \"Title: Fine tuning & scaling Hugging Face with Ray AIR\\n                Abstract:  Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\\n \\n The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\\n \\n In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models\\u2019 training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\\n \\n Key Takeaways: \\n * Python developers and machine learning engineers can use Transformers and scale their language models\\n * Get exposed to Ray AIR\\u2019s Python APIs for end-to-end Hugging Face and ML workflow\\n * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads\",\n            \"Title: Scaling AI Applications With Databricks, HuggingFace and Pinecone\\n                Abstract:  The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\\n In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution.\",\n            \"Title: How We Built a Unified Talent Solution Using Databricks Machine Learning\\n                Abstract:  Using Databricks, we built a \\u201cUnified Talent Solution\\u201d, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta\\u2019s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data\",\n            \"Title: Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases\\n                Abstract:  Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases.\",\n            \"Title: Data Quality: Fast and Slow\\n                Abstract:  Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \\n \\n In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\\n \\n This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical.\",\n            \"Title: AI to FI with Databricks\\n                Abstract:  Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\\n 1. Challenges of common pipeline design to support AI-FI \\n 2. Challenges of leveraging data acquired over many acquisitions\\n 3. How is the Lakehouse paradigm lending itself to solving these challenges?\\n 4. Some cool results we can show the world.\\n \\n To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \\n \\n To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying.\",\n            \"Title: JetBlue\\u2019s real-time AI & ML digital twin journey using Databricks\\n                Abstract:  JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \\n \\n The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code.\",\n            \"Title: How Office Leverages Deep Graph Learning to Improve Productivity Products\\n                Abstract:  We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale.\"\n        ]\n    ],\n    \"metadatas\": [\n        [\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null,\n            null\n        ]\n    ],\n    \"distances\": [\n        [\n            0.984288215637207,\n            1.1213231086730957,\n            1.3105521202087402,\n            1.3813146352767944,\n            1.3968392610549927,\n            1.4458246231079102,\n            1.468117356300354,\n            1.507476806640625,\n            1.5172799825668335,\n            1.5209240913391113\n        ]\n    ]\n}\n"
     ]
    }
   ],
   "source": [
    "results2 = talks_collection.query(\n",
    "    query_texts=[\"Generative AI\"],\n",
    "    n_results=10\n",
    ")\n",
    "\n",
    "print(json.dumps(results2, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83aea1d4-8d63-470a-834b-afdd4e143b67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Title: Testing Generative AI Models - What You Need to Know\\n                Abstract:  Generative AI shows incredible promise for enterprise applications. The explosion of generative AI can be attributed to the convergence of several factors. Most significant is that the barrier to entry has dropped for AI application developers through customizable prompts (few-shot learning), enabling laypeople to generate high-quality content. The flexibility of models like ChatGPT and DALLE-2 have sparked curiosity and creativity about new applications that they can support. The number of tools will continue to grow in a manner similar to how AWS fueled app development.\\n \\n But excitement must be tampered by concerns about new risks imposed to business and society. Increased capability and adoption also increase risk exposure. As organizations explore creative boundaries of generative models, measures to reduce risk must be put in place. However, the enormous size of the input space and inherent complexity make this task more challenging than traditional ML models.\\n \\n In this talk, we summarize the new risks introduced by the new class of generative foundation models through several examples, and compare how these risks relate to the risks of mainstream discriminative models. Steps can be taken to reduce the operational risk, bias and fairness issues, and privacy and security of systems that leverage LLM for automation. We’ll explore model hallucinations, output evaluation, output bias, prompt injection, data leakage, stochasticity, and more. We’ll discuss some of the larger issues common to LLMs and show how to test for them. A comprehensive, test-based approach to generative AI development will help instill model integrity by proactively mitigating failure and the associated business risk.',\n",
       " 'Title: Stable Diffusion: The Future of Generative AI\\n                Abstract:  Stability.ai, makers of Stable Diffusion, is the fastest growing open source software in history, gaining 40,000 GitHub stars in the first 3 months of launch. In this session, you will discuss how this team trained the model for Stable Diffusion, how it is improving towards the new release, and current research questions in development that will reduce inference times, as well as ways the open-source developer community can collaborate on research and engineering.',\n",
       " 'Title: Fine tuning & scaling Hugging Face with Ray AIR\\n                Abstract:  Hugging Face Transformers is a popular open-source project with cutting-edge Machine Learning (ML). Still, meeting the computational requirements for advanced models it provides often requires scaling beyond a single machine. This session explores the integration between Hugging Face and Ray AI Runtime (AIR), allowing users to scale their model training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\\n \\n The computational and memory requirements for fine-tuning and training these models can be significant. To deal with this issue, the Ray team has developed a Hugging Face integration for Ray AI Runtime (AIR), allowing models such as Transformers and Diffusion models training to be easily parallelized across multiple CPUs or GPUs in a Ray Cluster, saving time and money, all the while allowing to take advantage of the rich Ray ML ecosystem thanks to standard and common API.\\n \\n In this session, we explore the integration between Hugging Face and Ray AIR, allowing users to scale their NLP and computer vision models’ training and data loading seamlessly. We will dive deep into the implementation and API and explore how we can use Ray AIR to create an end-to-end Hugging Face workflow, from data ingestion through fine-tuning and HPO to inference and serving.\\n \\n Key Takeaways: \\n * Python developers and machine learning engineers can use Transformers and scale their language models\\n * Get exposed to Ray AIR’s Python APIs for end-to-end Hugging Face and ML workflow\\n * Understand how Ray AIR, built atop Ray, can scale your Python-based ML workloads',\n",
       " \"Title: Scaling AI Applications With Databricks, HuggingFace and Pinecone\\n                Abstract:  The production and management of large-scale vector embeddings can be a challenging problem. The integration of Databricks, Hugging Face and Pinecone offers a powerful solution. Vector embeddings have become an essential tool in the development of AI powered applications. Embeddings are representations of data learned by machine models. High quality embeddings are unlocking use cases like semantic search, recommendation engines, and anomaly detection. Databricks' Spark ecosystem together with Hugging Face's Transformers library enable large-scale vector embeddings production using GPU processing, Pinecone's vector database provides ultra-low latency querying and upserting of billions of embeddings, allowing for high-quality embeddings at scale for real-time AI apps.\\n In this session, we will present a concrete use case of this integration in the context of a natural language processing application. We will demonstrate how Pinecone's vector database can be integrated with Databricks and Hugging Face to produce large-scale vector embeddings of text data and how these embeddings can be used to improve the performance of various AI applications. You will see the benefits of this integration in terms of speed, scalability, and cost efficiency. By leveraging the GPU processing capabilities of Databricks and the ultra low-latency querying capabilities of Pinecone, we can significantly improve the performance of NLP tasks while reducing the cost and complexity of managing large-scale vector embeddings. You will learn about the technical details of this integration and how it can be implemented in your own AI projects, and gain insights into the speed, scalability, and cost efficiency benefits of using this solution.\",\n",
       " \"Title: How We Built a Unified Talent Solution Using Databricks Machine Learning\\n                Abstract:  Using Databricks, we built a “Unified Talent Solution”, backed by a robust data and AI engine for analyzing skills of a combined pool of permanent employees, contractors, part-time employees and vendors, inferring skill gaps, future trends and recommended priority areas to bridge talent gaps, which ultimately greatly improved operational efficiency, transparency, commercial model, and talent experience of our client. We leveraged a variety of ML algorithms such as boosting, neural networks and NLP transformers to provide better AI-driven insights. One inevitable part of developing these models within a typical DS workflow is iteration. Databricks' end-to-end ML/DS workflow service, MLFlow, helped streamline this process by organizing them into experiments that tracked the data used for training/testing, model artifacts, lineage and the corresponding results/metrics. For checking the health of our models using drift detection, bias and explainability techniques, MLFlow's deploying, and monitoring services were leveraged extensively. Our solution built on Databricks platform, simplified ML by defining a data-centric workflow that unified best practices from DevOps, DataOps, and ModelOps. Databricks Feature Store allowed us to productionize our models and features jointly. Insights were done with visually appealing charts and graphs using PowerBI, plotly, matplotlib, that answer business questions most relevant to clients. We built our own advanced custom analytics platform on top of delta lake as Delta’s ACID guarantees allows us to build a real-time reporting app that displays consistent and reliable data - React (for front-end), Structured Streaming for ingesting data from Delta table with live query analytics on real time data ML predictions based on analytics data\",\n",
       " 'Title: Rapidly Scaling Applied AI/ML with Foundational Models and Applying Them to Modern AI/ML Use Cases\\n                Abstract:  Today many of us are familiar with Foundational models such as LLM/ChatGPTl. However, there are many more enterprise foundational models that can be rapidly deployed, trained and applied to enterprise use cases. This approach dramatically increases the performance of AI/ML models in production, but also gives AI teams rapid roadmaps for efficiency and delivering value to the business. Databricks provides the ideal toolset to enable this approach. In this session, we will provide a logically overview of Foundational models available today, demonstrate a real-world use case, and provide a business framework for data scientists and business leaders to collaborate to rapidly deploy these use cases.',\n",
       " \"Title: Data Quality: Fast and Slow\\n                Abstract:  Data quality: the topic du jour. Gartner estimates the average business will lose $10M annually due to data quality problems, and every week we hear another cautionary tale of an AI model gone awry. As a result, startups and thought leaders are rushing to solve the visibility problem around data quality. Technology that unifies batch and streaming has massive but overlooked implications for our ability to trust existing data. \\n \\n In this session, I will demonstrate how architectures that can move between batch and incremental processing without changing the storage and API allow us to solve common data trust problems, such as stale data, as well as production ML risks, such as concept drift.\\n \\n This session is for data architects and practitioners. You don't need to be an ML or ETL expert to attend, but an interest in data architecture is critical.\",\n",
       " 'Title: AI to FI with Databricks\\n                Abstract:  Artificial Intelligence, Business Intelligence, Continuous Intelligence, Data Intelligence, Execution Intelligence, and Financial Intelligence are explained.\\n 1. Challenges of common pipeline design to support AI-FI \\n 2. Challenges of leveraging data acquired over many acquisitions\\n 3. How is the Lakehouse paradigm lending itself to solving these challenges?\\n 4. Some cool results we can show the world.\\n \\n To provide the most meaningful data to Profiles by Kantar, the leading provider of high-quality survey panelists, its data science team developed PROMETHEUS. It is a platform that supports real-time, continuously updated models, rigorous business analysis, MIS dashboards, Operational Research based allocation, and financial forecasts based on a single source of truth with maximum scalability. \\n \\n To successfully realize this, a multi-stage pipeline based on a lakehouse structure was incrementally developed and put into production using Databricks.This session will explain how Profiles by Kantar approached the design, onboarded analysts and developers, integrated data pipelines, and provided model outputs. Think building a plane while it is flying.',\n",
       " \"Title: JetBlue’s real-time AI & ML digital twin journey using Databricks\\n                Abstract:  JetBlue has embarked over the past year on an AI & ML transformation. Databricks has been instrumental in this transformation due to the ability to integrate streaming pipelines, ML training using MLFlow, ML API serving using ML registry and more in one cohesive platform. Using real-time streams of weather, aircraft sensors, FAA data feeds, JetBlue operations and more are used for the world's first AI & ML operating system orchestrating a digital-twin, known as BlueSky for efficient & safe operations. JetBlue has over 10 ML products (multiple models each product) in production across multiple verticals including dynamic pricing, customer recommendation engines, supply chain optimization, customer sentiment NLP and several more. \\n \\n The core JetBlue data science & analytics team consists of Operations Data Science, Commercial Data Science, AI & ML engineering and Business Intelligence. To facilitate the rapid growth and faster go-to-market strategy, the team has built an internal Data Catalog + AutoML + AutoDeploy wrapper called BlueML using Databricks features to empower data scientists including advanced analysts with the ability to train & deploy ML models in less than 5 lines of code.\",\n",
       " 'Title: How Office Leverages Deep Graph Learning to Improve Productivity Products\\n                Abstract:  We are the AI platform team from Microsoft 365. In this presentation, we will describe how our platform infuses various ML technologies that brings significant statistically improvement for hundreds of millions Microsoft 365 users into productivity products across Microsoft 365, along with a deep dive on how we leverage graph embeddings trained from DNN (Deep Graph Learning) technologies to improve search, recommendation and ranking systems across Microsoft 365. We will potentially cover the following topics: How to build per-organization knowledge graphs , ML infrastructure to deliver personalized features/embeddings/ML models at scale and embedding based ANN (Approximately Nearest Neighbor) search service. We will also go through the challenges and the solutions to deliver ML at Microsoft 365 scale.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2[\"documents\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa0edc9-200d-4445-9c94-1b7cebdea4e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question3\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_3(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a359a2f-017d-463b-acd5-de6609bc4732",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "Load a language model and create a [pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc11515e-787c-45f2-9c6b-f28b46a83495",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Pick a model from HuggingFace that can generate text\n",
    "# model_id = \"meta-llama/Llama-2-7b\"\n",
    "# model_id = \"huggyllama/llama-7b\"\n",
    "model_id = \"microsoft/DialoGPT-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", model=lm_model, tokenizer=tokenizer, max_new_tokens=20, device_map=\"auto\", handle_long_generation=\"hole\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b692233f-3309-4dd3-acae-fc6a55b703c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaee8167-814e-48d1-832c-a8bfaa339aac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question4\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_4(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eaaecc0-6a64-4b56-ba98-99990b54704e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "Prompt engineering for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8ca05a8-1fd9-4942-a03e-186cac99c547",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Come up with a question that you need the LLM assistant to help you with\n",
    "# A sample question is \"Help me find sessions related to XYZ\" \n",
    "# Note: Your \"XYZ\" should be related to the query you passed in Question 3. \n",
    "question = \"What are language models?\"\n",
    "\n",
    "# Provide all returned similar documents from the cell above below\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "\n",
    "# Feel free to be creative how you construct the prompt. You can use the demo notebook as a jumpstart reference.\n",
    "# You can also provide more requirements in the text how you want the answers to look like.\n",
    "# Example requirement: \"Recommend top-5 relevant sessions for me to attend.\"\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088dbb8f-94f8-4c86-badc-44fd5423558f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question5\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_5(question, context, prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95e0c02a-b67d-4062-90c9-8f8659d8ba7a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 6 \n",
    "\n",
    "Submit query for language model to generate response.\n",
    "\n",
    "Hint: If you run into the error `index out of range in self`, make sure to check out this [documentation page](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline.__call__.handle_long_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0df6ac-e892-449b-a462-e232a73a5a3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3351 > 1024). Running this sequence through the model will result in indexing errors\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-882104734880123>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m lm_response \u001B[38;5;241m=\u001B[39m pipe(prompt_template)\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(lm_response[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
       "\n",
       "\u001B[0;31mIndexError\u001B[0m: list index out of range"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-882104734880123>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m lm_response \u001B[38;5;241m=\u001B[39m pipe(prompt_template)\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(lm_response[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\n\u001B[0;31mIndexError\u001B[0m: list index out of range",
       "errorSummary": "<span class='ansi-red-fg'>IndexError</span>: list index out of range",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[1][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed98e970-4e4e-438a-8220-30df8987e50d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson2, question6\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_6(lm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80c638d1-0650-407d-9554-18c0840822b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice that the output isn't exactly helpful. Head on to using OpenAI to try out GPT-3.5 instead! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8503f6ee-ebf4-4666-95d0-ff765d2c4343",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## OPTIONAL (Non-Graded): Use OpenAI models for Q/A\n",
    "\n",
    "For this section to work, you need to generate an Open AI key. \n",
    "\n",
    "Steps:\n",
    "1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n",
    "2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "Note: OpenAI does not have a free option, but it gives you $5 as credit. Once you have exhausted your $5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). **IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02542ec9-7f85-43fb-9422-256d534a7876",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<FILL IN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f7b81b-d17d-4f23-9d13-e3f35a6dff7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fc3db3d-b0ed-47c2-9c8a-4f1028d9c7ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you would like to estimate how much it would cost to use OpenAI, you can use `tiktoken` library from OpenAI to get the number of tokens from your prompt.\n",
    "\n",
    "\n",
    "We will be using `gpt-3.5-turbo` since it's the most economical option at ($0.002/1k tokens), as of May 2023. GPT-4 charges $0.04/1k tokens. The following code block below is referenced from OpenAI's documentation on [\"Managing tokens\"](https://platform.openai.com/docs/guides/chat/managing-tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e6c87bf-f41d-45e4-b075-1d57f97524f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "price_token = 0.002\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "cost_to_run = len(encoder.encode(prompt_template)) / 1000 * price_token\n",
    "print(f\"It would take roughly ${round(cost_to_run, 5)} to run this prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "714545d6-9871-4eee-98aa-c65b506386d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We won't have to create a new vector database again. We can just send our `context` from above to OpenAI. We will use their chat completion API to interact with `GPT-3.5-turbo`. You can refer to their [documentation here](https://platform.openai.com/docs/guides/chat).\n",
    "\n",
    "Something interesting is that OpenAI models use the system message to help their assistant to be more accurate. From OpenAI's [docs](https://platform.openai.com/docs/guides/chat/introduction):\n",
    "\n",
    "> Future models will be trained to pay stronger attention to system messages. The system message helps set the behavior of the assistant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f29e524-0392-43aa-acb6-c1cb1f9eb8ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "gpt35_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": <FILL_IN>},\n",
    "    ],\n",
    "    temperature=0, # 0 makes outputs deterministic; The closer the value is to 1, the more random the outputs are for each time you re-run.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c663139e-fcbc-4e8f-9e55-846c84de616b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(gpt35_response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5506adab-5269-47b8-a83f-a04c33a652f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(gpt35_response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86e88c8a-8400-4b18-bb15-818339b112fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also check how many tokens OpenAI has used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab30bb87-6f27-423e-a2b6-2af8bdd3b03e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gpt35_response[\"usage\"][\"total_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f632a31b-e736-423e-95bb-634e19397487",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The results are noticeably much better compared to when using Hugging Face's GPT-2! It didn't get stuck in the text generation, but the sessions recommended are not all relevant to pandas either. You can further do more prompt engineering to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a922fea7-634a-4760-9e99-04ccdcb344e4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Submit your Results (edX Verified Only)\n",
    "\n",
    "To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b5306c-54bf-4dbd-8563-be7b65e1ff35",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02L - Embeddings, Vector Databases, and Search",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
