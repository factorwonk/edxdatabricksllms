{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1d18b2c-2101-4275-ae56-30cc12ae0ad9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcc7d34a-ed49-4382-baa8-18dabc8f5bfc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Mixture-of-Experts - Achieve Massively Scaled, but Efficient, LLM Performance \n",
    "In this lab we will explore how to build our own, simplified version of a mixture-of-experts (MoE) LLM system. While this method often involves a complex training and transformer configuration, we can see some of the benefits of this approach in a pseudo-MoE that we will build with some open source LLMs. \n",
    "\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Create your own MoE system using open source LLMs\n",
    "1. Build different gating mechanisms to direct different prompts to appropriate \"expert models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02bc32d7-6431-4847-8ca5-d31544380eab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e252a38-c05b-49b2-9309-5e8eca244021",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the learning environment:\n| enumerating serving endpoints...found 5...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/labuser5381118@vocareum.com/llm-foundation-models\"...(1 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/llm-foundation-models/v01-raw\"\n\nValidating the locally installed datasets:\n| listing local files...(3 seconds)\n| validation completed...(3 seconds total)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing lab testing framework.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nUsing the \"default\" schema.\n\nPredefined paths variables:\n| DA.paths.working_dir: /dbfs/mnt/dbacademy-users/labuser5381118@vocareum.com/llm-foundation-models\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/labuser5381118@vocareum.com/llm-foundation-models/database.db\n| DA.paths.datasets:    /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw\n\nSetup completed (8 seconds)\n\nThe models developed or used in this course are for demonstration and learning purposes only.\nModels may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n"
     ]
    }
   ],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ebb734-a277-4472-bfb6-ebecf8897819",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting textblob==0.17.1\n  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 636.8/636.8 kB 6.0 MB/s eta 0:00:00\nRequirement already satisfied: nltk>=3.1 in /databricks/python3/lib/python3.10/site-packages (from textblob==0.17.1) (3.7)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.1->textblob==0.17.1) (1.2.0)\nRequirement already satisfied: click in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.1->textblob==0.17.1) (8.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.1->textblob==0.17.1) (2022.7.9)\nRequirement already satisfied: tqdm in /databricks/python3/lib/python3.10/site-packages (from nltk>=3.1->textblob==0.17.1) (4.64.1)\nInstalling collected packages: textblob\nSuccessfully installed textblob-0.17.1\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install textblob==0.17.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f75ed69a-6c6b-4edd-b570-689c9262f034",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e44d7bb-164c-4d0f-94b9-80a2a824b7a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 1: An Overview of Mixture-of-Experts (MoE)\n",
    "Mixture-of-Experts (MoE) is a machine learning architecture that incorporates the idea of \"divide and conquer\" to solve complex problems. In this approach, the model is composed of multiple individual models, referred to as \"experts\", each of which specializes in some aspect of the data. The model also includes a \"gating\" function that determines which expert or combination of experts to consult for a given input.\n",
    "\n",
    "The key feature of MoE models is that they can handle a diverse range of data patterns through the different areas of expertise of their component models. Each expert has its own set of parameters and is typically a simpler model than would be necessary to model the entire data set effectively. The gating mechanism then learns to recognize which expert is most likely to provide the best output for a particular input, thereby effectively dividing the problem space among the different experts.\n",
    "\n",
    "In a true MoE model, the experts and the gating function are trained together in an end-to-end manner. This joint training allows the experts to specialize on different parts of the input space and the gating function to learn how to best utilize the experts based on the input. It's a kind of \"cooperative competition\" among the experts, where they compete to contribute to the final output but cooperate in the sense that their combined expertise leads to a better overall model.\n",
    "\n",
    "An illustrative diagram would show the input being fed into the gating function, which then weights the contribution of each expert model to produce the final output. The expert models themselves would be shown as individual networks, each receiving the same input and producing its own output.\n",
    "\n",
    "The main advantage of MoE models is their efficiency in modeling complex functions with fewer parameters than a single large model. Since different experts can share parameters, this reduces the total number of parameters needed. This feature makes MoE models particularly useful in settings where data is diverse and complex, and a single model may struggle to capture all the different patterns present in the data.\n",
    "\n",
    "In this notebook, we will be creating a \"pseudo\" MoE model. This is not a true MoE model because we are not training the experts and the gating function together in an end-to-end manner. Instead, we will be using pre-trained models as our experts and defining our own simple gating function. While this approach does not fully capture the power of a true MoE model, it provides a useful introduction to the concept and allows us to explore how different experts and gating functions can affect the performance of the model. It also provides a foundation for understanding how a true MoE model might be implemented and trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a08c646-7a27-4f5a-8027-51de163e1be6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 2: The Pseudo MoE Model\n",
    "In this section, we'll implement a simplified version of an MoE model. Instead of training the experts and gating function together, we'll use pre-trained transformer models as our experts and a simple rule-based function as our gating function.\n",
    "\n",
    "We'll also look at different types of gating mechanisms - hard gating, soft gating, and top-k gating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e01927e8-4f2e-4478-8443-bde585072a18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/huggingface_hub/file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw/models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b641338f7ac94b11a2e396cd99ad46b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b69718370b7416e844094cd2c8dd052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11aeb3cd5d640939c537933c773f0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/huggingface_hub/file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /dbfs/mnt/dbacademy-datasets/llm-foundation-models/v01-raw/models/models. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n  warnings.warn(message)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81598713dcf144fa86b37667cde08d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "# transformers is a state-of-the-art library for Natural Language Processing tasks, providing a wide range of pre-trained models\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertForSequenceClassification, BertTokenizer, T5ForConditionalGeneration, T5Tokenizer\n",
    "# torch.nn.functional provides functions that don't have any parameters, such as activation functions, loss functions etc.\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the GPT2 model and tokenizer\n",
    "# GPT2 is an autoregressive language model that uses transformer blocks and byte-pair encoding\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "# The tokenizer is responsible for turning input data into the format that the model expects\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-XL\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "# BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based machine learning technique for natural language processing pre-training\n",
    "bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "# The tokenizer is responsible for turning input data into the format that the model expects\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "# T5 (Text-to-Text Transfer Transformer) is a transformer model which treats every NLP problem as a text generation task\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\"t5-base\", cache_dir=DA.paths.datasets+\"/models\")\n",
    "# The tokenizer is responsible for turning input data into the format that the model expects\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\", cache_dir=DA.paths.datasets+\"/models\"+\"/models\")\n",
    "\n",
    "# Define the \"hard gating\" function\n",
    "# This function decides which model to use based on the length of the input\n",
    "def hard_gating_function(input):\n",
    "    if len(input) < 10:\n",
    "        # For inputs less than 10 characters long, use the GPT2 model\n",
    "        return \"gpt2\", gpt2, gpt2_tokenizer\n",
    "    elif len(input) < 100:\n",
    "        # For inputs less than 100 characters long but greater than 10 characters, use the T5 model\n",
    "        return \"t5\" , t5, t5_tokenizer\n",
    "    else:\n",
    "        # For inputs greater than 100 characters, use the BERT model\n",
    "        return \"bert\", bert, bert_tokenizer\n",
    "\n",
    "# Define the \"soft gating\" function\n",
    "# This function assigns a weight to each model based on the length of the input, and all models are used to a certain extent to generate the output\n",
    "def soft_gating_function(input):\n",
    "    # The weights for each model are calculated using the softmax function, which outputs a probability distribution\n",
    "    weights = F.softmax(torch.tensor([len(input), 100 - len(input), len(input)], dtype=torch.float), dim=0)\n",
    "    # The weights for each model are returned along with the models and their tokenizers\n",
    "    return {\"gpt2\": (gpt2, gpt2_tokenizer, weights[0]),\n",
    "            \"bert\": (bert, bert_tokenizer, weights[1]),\n",
    "            \"t5\": (t5, t5_tokenizer, weights[2])}\n",
    "\n",
    "# Define the pseudo MoE model\n",
    "# This function uses the gating function to decide which model(s) to use for a given input\n",
    "def pseudo_moe_model(input, gating_function, hard_fn=hard_gating_function, soft_fn=soft_gating_function):\n",
    "    if gating_function == \"hard\":\n",
    "        # If the hard gating function is used, only one model is used for a given input\n",
    "        model_name, model, tokenizer = hard_fn(input)\n",
    "        inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "        if model_name == \"t5\":\n",
    "            # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
    "            decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "            outputs = model(**inputs, decoder_input_ids=decoder_inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        # The output of the model is decoded into a string\n",
    "        decoded_output = tokenizer.decode(outputs.logits[0].argmax(-1).tolist())\n",
    "        # The name of the model used and the decoded output are returned\n",
    "        return model_name, decoded_output\n",
    "    else:  # soft gating\n",
    "        # If the soft gating function is used, all models are used to a certain extent to generate the output\n",
    "        models = soft_fn(input)\n",
    "        outputs = []\n",
    "        for model_name, (model, tokenizer, weight) in models.items():\n",
    "            inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "            if model_name == \"t5\":\n",
    "                # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
    "                decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "                output = model(**inputs, decoder_input_ids=decoder_inputs)\n",
    "            else:\n",
    "                output = model(**inputs)\n",
    "            # The output of each model is multiplied by its weight\n",
    "            outputs.append((model_name, output.logits * weight))\n",
    "        # The outputs of all models are added together to generate the final output\n",
    "        decoded_outputs = [(model_name, tokenizer.decode(output[0].argmax(-1).tolist())) for model_name, output in outputs]\n",
    "        # The decoded outputs are returned\n",
    "        return decoded_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "430f842d-4a7d-4e26-8f1e-f732ea01b514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard gating output: ('t5', '<extra_id_0>This')\nSoft gating output: [('gpt2', '—ation says reported After și readingatoration anyone each of given dem off usinguk After E valuable of ou Aftereaza of of și'), ('bert', '<pad>'), ('t5', '<extra_id_0>We')]\n"
     ]
    }
   ],
   "source": [
    "# Test the hard gating function\n",
    "example_1 = \"Translate to german: This is a short input.\"\n",
    "output = pseudo_moe_model(example_1, gating_function=\"hard\")\n",
    "print(\"Hard gating output:\", output)\n",
    "\n",
    "# Test the soft gating function\n",
    "example_2 = \"This is a longer input. We're adding more text here to make sure it's longer than 50 characters but shorter than 100 characters.\"\n",
    "output = pseudo_moe_model(example_2, gating_function=\"soft\")\n",
    "print(\"Soft gating output:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ba9e561-ee67-41ac-9908-bdb75dd88285",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Section 3: Your Turn\n",
    "Now it's your turn to experiment with the pseudo MoE model. Here are some exercises you can try:\n",
    "\n",
    "Implement a new gating function: Instead of just using the length of the input, can you use a basic sentiment analysis to determine which model to use? You can use the textblob library for the sentiment analysis.\n",
    "\n",
    "Add a new expert: Can you add a new expert to the pseudo MoE model? Try using the distilbert model, which is a smaller, faster, cheaper version of BERT.\n",
    "\n",
    "Test the updated pseudo MoE model: Once you've made your updates, test the pseudo MoE model with example inputs of different sentiment. What do you notice about the performance of the different experts (models) for different input text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c402fbc2-6bba-4552-b8d5-e4fa46487cb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8608409581024043a6245dca8ec8d1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0b6f3e6d7347bba35e5299a4fb49e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30323991cdaa46faa91c8fbe0535f78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2111c4238d7f4bd982474c8f51f0dd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-506823110941305>:82\u001B[0m\n",
       "\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# 3. Test the updated pseudo MoE model\u001B[39;00m\n",
       "\u001B[1;32m     81\u001B[0m test_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI like cookies more than ice cream\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m---> 82\u001B[0m \u001B[38;5;28mprint\u001B[39m(pseudo_moe_model(test_input, gating_function\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhard\u001B[39m\u001B[38;5;124m'\u001B[39m, hard_fn\u001B[38;5;241m=\u001B[39mupdated_gating_function))\n",
       "\n",
       "File \u001B[0;32m<command-506823110941302>:53\u001B[0m, in \u001B[0;36mpseudo_moe_model\u001B[0;34m(input, gating_function, hard_fn, soft_fn)\u001B[0m\n",
       "\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpseudo_moe_model\u001B[39m(\u001B[38;5;28minput\u001B[39m, gating_function, hard_fn\u001B[38;5;241m=\u001B[39mhard_gating_function, soft_fn\u001B[38;5;241m=\u001B[39msoft_gating_function):\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gating_function \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhard\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m     52\u001B[0m         \u001B[38;5;66;03m# If the hard gating function is used, only one model is used for a given input\u001B[39;00m\n",
       "\u001B[0;32m---> 53\u001B[0m         model_name, model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mhard_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     54\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m tokenizer(\u001B[38;5;28minput\u001B[39m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     55\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m model_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt5\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m     56\u001B[0m             \u001B[38;5;66;03m# For T5, create a decoder input sequence consisting of only the <BOS> token\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: updated_gating_function() missing 1 required positional argument: 'gating_function'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-506823110941305>:82\u001B[0m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;66;03m# 3. Test the updated pseudo MoE model\u001B[39;00m\n\u001B[1;32m     81\u001B[0m test_input \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI like cookies more than ice cream\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 82\u001B[0m \u001B[38;5;28mprint\u001B[39m(pseudo_moe_model(test_input, gating_function\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhard\u001B[39m\u001B[38;5;124m'\u001B[39m, hard_fn\u001B[38;5;241m=\u001B[39mupdated_gating_function))\n\nFile \u001B[0;32m<command-506823110941302>:53\u001B[0m, in \u001B[0;36mpseudo_moe_model\u001B[0;34m(input, gating_function, hard_fn, soft_fn)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpseudo_moe_model\u001B[39m(\u001B[38;5;28minput\u001B[39m, gating_function, hard_fn\u001B[38;5;241m=\u001B[39mhard_gating_function, soft_fn\u001B[38;5;241m=\u001B[39msoft_gating_function):\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gating_function \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhard\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     52\u001B[0m         \u001B[38;5;66;03m# If the hard gating function is used, only one model is used for a given input\u001B[39;00m\n\u001B[0;32m---> 53\u001B[0m         model_name, model, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mhard_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m tokenizer(\u001B[38;5;28minput\u001B[39m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m model_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mt5\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     56\u001B[0m             \u001B[38;5;66;03m# For T5, create a decoder input sequence consisting of only the <BOS> token\u001B[39;00m\n\n\u001B[0;31mTypeError\u001B[0m: updated_gating_function() missing 1 required positional argument: 'gating_function'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: updated_gating_function() missing 1 required positional argument: 'gating_function'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO\n",
    "# 1. Implement a new gating function\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_based_gating_function(input):\n",
    "    blob = TextBlob(input)\n",
    "    if blob.sentiment.polarity <= 0:\n",
    "        return \"gpt2\", gpt2, gpt2_tokenizer\n",
    "    elif blob.sentiment.polarity >= 0 and blob.sentiment.polarity < 0.3:\n",
    "        return \"t5\" , t5, t5_tokenizer\n",
    "    elif blob.sentiment.polarity >= 0.3 and blob.sentiment.polarity < 0.6:\n",
    "        return \"bert\", bert, bert_tokenizer\n",
    "    else:\n",
    "        return \"distilbert\", distilbert, distilbert_tokenizer\n",
    "\n",
    "# test_input = \"I am so happy today!\"\n",
    "# print(pseudo_moe_model(test_input, gating_function=\"hard\", hard_fn=sentiment_based_gating_function))\n",
    "\n",
    "# 2. Add a new expert\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "distilbert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Update the gating function to include the new expert\n",
    "\n",
    "def updated_gating_function(input, gating_function):\n",
    "    if gating_function == \"hard\":\n",
    "        # If the hard gating function is used, only one model is used for a given input\n",
    "        model_name, model, tokenizer = hard_gating_function(input)\n",
    "        inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "        if model_name == \"t5\":\n",
    "            # <BOS> - Beginning of Sentence\n",
    "            # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
    "            decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "            outputs = model(**inputs, decoder_input_ids=decoder_inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        # The output of the model is decoded into a string\n",
    "        decoded_output = tokenizer.decode(outputs.logits[0].argmax(-1).tolist())\n",
    "        # The name of the model used and the decoded output are returned\n",
    "        return model_name, decoded_output\n",
    "    elif gating_function == \"soft\":  # soft gating\n",
    "        # If the soft gating function is used, all models are used to a certain extent to generate the output\n",
    "        models = soft_gating_function(input)\n",
    "        outputs = []\n",
    "        for model_name, (model, tokenizer, weight) in models.items():\n",
    "            inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "            if model_name == \"t5\":\n",
    "                # <BOS> - Beginning of Sentence\n",
    "                # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
    "                decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "                output = model(**inputs, decoder_input_ids=decoder_inputs)\n",
    "            else:\n",
    "                output = model(**inputs)\n",
    "            # The output of each model is multiplied by its weight\n",
    "            outputs.append((model_name, output.logits * weight))\n",
    "        # The outputs of all models are added together to generate the final output\n",
    "        decoded_outputs = [(model_name, tokenizer.decode(output[0].argmax(-1).tolist())) for model_name, output in outputs]\n",
    "        # The decoded outputs are returned\n",
    "        return decoded_outputs\n",
    "    elif gating_function == \"sentiment\":\n",
    "        model_name, model, tokenizer = sentiment_based_gating_function(input)\n",
    "        inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "        if model_name == \"t5\":\n",
    "            # <BOS> - Beginning of Sentence\n",
    "            # For T5, create a decoder input sequence consisting of only the <BOS> token\n",
    "            decoder_inputs = tokenizer([\"<pad>\"], return_tensors=\"pt\")[\"input_ids\"]\n",
    "            outputs = model(**inputs, decoder_input_ids=decoder_inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        # The output of the model is decoded into a string\n",
    "        decoded_output = tokenizer.decode(outputs.logits[0].argmax(-1).tolist())\n",
    "        # The name of the model used and the decoded output are returned\n",
    "        return model_name, decoded_output\n",
    "\n",
    "# 3. Test the updated pseudo MoE model\n",
    "\n",
    "test_input = \"I like cookies more than ice cream\"\n",
    "print(pseudo_moe_model(test_input, gating_function='hard', hard_fn=updated_gating_function))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8121487b-ee10-4bef-b033-00c2e4b68429",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32mPASSED\u001B[0m: All tests passed for lesson3, question1\n\u001B[32mRESULTS RECORDED\u001B[0m: Click `Submit` when all questions are completed to log the results.\n"
     ]
    }
   ],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion3_1(distilbert, distilbert_tokenizer, test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82dd0b59-891e-4148-8b75-b9a3603f7c4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 03L - Deployment of LLMs Lab",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
